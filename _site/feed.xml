<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-11-14T00:32:59-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Deep Learning</title><subtitle>Mohsen Dehghani</subtitle><entry><title type="html">Understanding Dimensionality Reduction for High-Dimensional Data Visualization</title><link href="http://localhost:4000/update/2024/11/12/code.html" rel="alternate" type="text/html" title="Understanding Dimensionality Reduction for High-Dimensional Data Visualization" /><published>2024-11-12T19:31:29-05:00</published><updated>2024-11-12T19:31:29-05:00</updated><id>http://localhost:4000/update/2024/11/12/code</id><content type="html" xml:base="http://localhost:4000/update/2024/11/12/code.html"><![CDATA[<h3 id="understanding-dimensionality-reduction-for-high-dimensional-data-visualization">Understanding Dimensionality Reduction for High-Dimensional Data Visualization</h3>

<p>In this section, we will cover:</p>

<ol>
  <li><strong>The Importance of Dimensionality Reduction</strong>:</li>
</ol>

<p>Discuss why dimensionality reduction is essential for visualizing and analyzing complex, high-dimensional data.</p>

<ol>
  <li><strong>Techniques Overview</strong>:</li>
</ol>

<p>Provide a brief explanation of PCA, LDA, t-SNE, and UMAP, highlighting their strengths and best-use cases.</p>

<ol>
  <li><strong>Choosing the Right Technique</strong>:</li>
</ol>

<p>Guide users on selecting the best method depending on the dataset and objectives, perhaps with visual examples.</p>

<ol>
  <li><strong>Applications and Examples</strong>:</li>
</ol>

<p>Show specific scenarios (like image data, text, or clustering) where these techniques are applied effectively.</p>

<ol>
  <li><strong>Limitations and Trade-Offs</strong>:</li>
</ol>

<p>Discuss common challenges, such as interpretability, parameter tuning, and computational cost, to help users understand when and how to apply these methods effectively.</p>

<p>This would give readers both an informative and practical understanding of dimensionality reduction in data science.
Analyze and visualize the statistical properties and distributions of a dataset</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">KNNImputer</span>

<span class="k">class</span> <span class="nc">AbstractPreprocessor</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_path</span><span class="p">,</span> <span class="n">file_type</span><span class="o">=</span><span class="s">'csv'</span><span class="p">,</span> <span class="n">irrelevant_columns</span><span class="o">=</span><span class="p">[]):</span>
        <span class="s">"""
        Initializes the preprocessor with data from a specified file.
        :param data_path: Path to the dataset (csv or json).
        :param file_type: Format of the dataset, either 'csv' or 'json'.
        :param irrelevant_columns: List of column names to drop.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data_path</span> <span class="o">=</span> <span class="n">data_path</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">file_type</span> <span class="o">=</span> <span class="n">file_type</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">irrelevant_columns</span> <span class="o">=</span> <span class="n">irrelevant_columns</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""
        Loads the dataset based on the file type (csv or json).
        """</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">file_type</span> <span class="o">==</span> <span class="s">'csv'</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data_path</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">file_type</span> <span class="o">==</span> <span class="s">'json'</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_json</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data_path</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Unsupported file type. Please use 'csv' or 'json'."</span><span class="p">)</span>

    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">visualize_before</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">visualize_after</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">remove_duplicates</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">duplicates</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">duplicated</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Removing </span><span class="si">{</span><span class="n">duplicates</span><span class="si">}</span><span class="s"> duplicate rows."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">drop_duplicates</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">remove_irrelevant</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Removing irrelevant columns: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">irrelevant_columns</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">irrelevant_columns</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">identify_null_values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Identifying null values and other missing indicators..."</span><span class="p">)</span>
        <span class="n">null_counts</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">isnull</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span>
        <span class="n">blank_counts</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">==</span> <span class="s">""</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Null values per column:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">null_counts</span><span class="p">[</span><span class="n">null_counts</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">])</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Blank values per column:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">blank_counts</span><span class="p">[</span><span class="n">blank_counts</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">identify_extreme_values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Identifying columns with extreme values and unique values..."</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">number</span><span class="p">]):</span>
            <span class="n">min_value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="nb">min</span><span class="p">()</span>
            <span class="n">max_value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="nb">max</span><span class="p">()</span>
            <span class="n">zero_count</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
            <span class="n">unique_count</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">nunique</span><span class="p">()</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Column '</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s">': min=</span><span class="si">{</span><span class="n">min_value</span><span class="si">}</span><span class="s">, max=</span><span class="si">{</span><span class="n">max_value</span><span class="si">}</span><span class="s">, zero_count=</span><span class="si">{</span><span class="n">zero_count</span><span class="si">}</span><span class="s">, unique_count=</span><span class="si">{</span><span class="n">unique_count</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">calculate_statistics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column</span><span class="p">):</span>
        <span class="s">"""
        Calculates and prints key statistics for a given column.
        """</span>
        <span class="n">stats</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">'mean'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">mean</span><span class="p">(),</span>
            <span class="s">'median'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">median</span><span class="p">(),</span>
            <span class="s">'std'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">std</span><span class="p">(),</span>
            <span class="s">'min'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="nb">min</span><span class="p">(),</span>
            <span class="s">'max'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="nb">max</span><span class="p">(),</span>
            <span class="s">'25th_percentile'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">),</span>
            <span class="s">'50th_percentile'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.50</span><span class="p">),</span>
            <span class="s">'75th_percentile'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">),</span>
            <span class="s">'skew'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">skew</span><span class="p">()</span>
        <span class="p">}</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Statistics for '</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s">':"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">stat</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">stats</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">stat</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">stats</span>

    <span class="k">def</span> <span class="nf">analyze_distributions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">target_column</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="s">"""
        Analyzes distributions of specified columns and visualizes them using boxplots, density plots, and histograms.
        :param columns: List of columns to analyze. If None, analyzes all numeric columns.
        :param target_column: Optional target column for class-based visualization.
        """</span>
        <span class="k">if</span> <span class="n">columns</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">number</span><span class="p">]).</span><span class="n">columns</span>
        
        <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Analyzing distribution for '</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s">':"</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">calculate_statistics</span><span class="p">(</span><span class="n">column</span><span class="p">)</span>

            <span class="c1"># Visualization
</span>            <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">target_column</span> <span class="ow">and</span> <span class="n">target_column</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
                <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">target_column</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">column</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">column</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="n">target_column</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">sns</span><span class="p">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">column</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="n">target_column</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">sns</span><span class="p">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

            <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s">"Boxplot of </span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s">"Density Plot of </span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s">"Histogram of </span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outlier_columns</span><span class="o">=</span><span class="p">[],</span> <span class="n">missing_strategy</span><span class="o">=</span><span class="s">'mean'</span><span class="p">,</span> <span class="n">outlier_strategy</span><span class="o">=</span><span class="s">'cap'</span><span class="p">):</span>
        <span class="s">"""
        Runs the entire preprocessing pipeline: duplicates, irrelevant columns, null handling, extreme values, outliers, and missing values.
        :param outlier_columns: List of columns to check for outliers.
        :param missing_strategy: Strategy to handle missing values.
        :param outlier_strategy: Strategy to handle outliers.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">remove_duplicates</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">remove_irrelevant</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">identify_null_values</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">identify_extreme_values</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">outlier_columns</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">handle_outliers</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="n">outlier_strategy</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">handle_missing_values</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="n">missing_strategy</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">statistical_analysis</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">check_correlations</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">plot_distributions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">columns</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'Distribution of Features'</span><span class="p">):</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
            <span class="n">sns</span><span class="p">.</span><span class="n">histplot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Example Concrete Implementation
</span><span class="k">class</span> <span class="nc">MyPreprocessor</span><span class="p">(</span><span class="n">AbstractPreprocessor</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">visualize_before</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Visualizing data before preprocessing..."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">plot_distributions</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'Before Preprocessing'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">visualize_after</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Visualizing data after preprocessing..."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">plot_distributions</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'After Preprocessing'</span><span class="p">)</span>

<span class="c1"># Usage Example
# data_path = "your_dataset.csv"
# irrelevant_columns = ['irrelevant_feature']
# preprocessor = MyPreprocessor(data_path, file_type='csv', irrelevant_columns=irrelevant_columns)
# preprocessor.visualize_before()
# preprocessor.analyze_distributions(columns=['your_numeric_column'], target_column='target_class')
# preprocessor.visualize_after()
</span></code></pre></div></div>

<h3 id="explanation-of-analyze_distributions">Explanation of <code class="highlighter-rouge">analyze_distributions</code></h3>

<ol>
  <li><strong>Statistics Calculation</strong>:
    <ul>
      <li>The <code class="highlighter-rouge">calculate_statistics</code> method computes key statistics for each specified column: mean, median, standard deviation, minimum, maximum, percentiles, and skewness.</li>
      <li>These statistics help identify the central tendency, spread, and skewness, which guide the choice of transformations (e.g., log or square root) and scaling methods.</li>
    </ul>
  </li>
  <li><strong>Visualization</strong>:
    <ul>
      <li>For each column, the method produces three plots:
        <ul>
          <li><strong>Boxplot</strong>: Highlights outliers, median, and interquartile range (IQR), ideal for spotting distribution spread and skewness.</li>
          <li><strong>Density Plot</strong>: Shows the continuous shape of the distribution, useful for visualizing skewness.</li>
          <li><strong>Histogram</strong>: Provides a bar representation of value frequencies, ideal for spotting skewness and data range.</li>
        </ul>
      </li>
      <li>If a target column is provided (for multi-class analysis), the visualizations show feature distributions across classes, which helps understand feature-target relationships.</li>
    </ul>
  </li>
  <li><strong>Optional Target-Based Plotting</strong>:
    <ul>
      <li>When <code class="highlighter-rouge">target_column</code> is specified, the method visualizes each feature’s distribution per class. This helps identify which features are most distinct or predictive for different classes.</li>
    </ul>
  </li>
</ol>

<p>This updated <code class="highlighter-rouge">analyze_distributions</code> method will give you a comprehensive view of each feature’s distribution, helping you make informed decisions about scaling, transformation, or outlier handling. Let me know if you’d like further customization!</p>]]></content><author><name></name></author><category term="Update" /><summary type="html"><![CDATA[Understanding Dimensionality Reduction for High-Dimensional Data Visualization]]></summary></entry><entry><title type="html">High-dimensional Spaces and The Concept of Angles Between Features</title><link href="http://localhost:4000/update/2024/11/12/fetures.html" rel="alternate" type="text/html" title="High-dimensional Spaces and The Concept of Angles Between Features" /><published>2024-11-12T19:31:29-05:00</published><updated>2024-11-12T19:31:29-05:00</updated><id>http://localhost:4000/update/2024/11/12/fetures</id><content type="html" xml:base="http://localhost:4000/update/2024/11/12/fetures.html"><![CDATA[<p>In feature selection, the angle between features can be a useful tool for understanding and managing redundancy and correlation in the data, especially in high-dimensional spaces. Here’s how the angle between features impacts feature selection and the techniques that can leverage this information:</p>

<h3 id="1-understanding-redundancy-with-angles">1. <strong>Understanding Redundancy with Angles</strong></h3>
<ul>
  <li>Features that have a small angle between them are highly correlated, meaning they contain similar information. Including both in a model might not add much value and could introduce redundancy.</li>
  <li>By selecting features with larger angles between them (closer to orthogonal), you’re choosing features that contribute more unique information, potentially improving the model’s robustness and interpretability.</li>
</ul>

<h3 id="2-dimensionality-reduction-techniques">2. <strong>Dimensionality Reduction Techniques</strong></h3>
<ul>
  <li><strong>Principal Component Analysis (PCA)</strong>: PCA transforms the feature space into a new set of orthogonal components, which are linear combinations of the original features. By choosing the components that capture the most variance, you’re effectively selecting directions in feature space that maximize information content while minimizing redundancy.</li>
  <li><strong>Independent Component Analysis (ICA)</strong>: While PCA focuses on uncorrelated features, ICA aims for statistical independence, which often corresponds to large angles between features in transformed space. ICA can help separate features that have meaningful independent contributions.</li>
</ul>

<h3 id="3-correlation-based-feature-selection">3. <strong>Correlation-Based Feature Selection</strong></h3>
<ul>
  <li>By calculating the correlation (or cosine similarity) between pairs of features, you can identify features that have small angles between them, indicating high correlation.</li>
  <li><strong>Threshold-Based Selection</strong>: A common approach is to set a correlation threshold (e.g., features with correlations above 0.9) and remove one of the correlated features. This is particularly useful when features are highly correlated, as you can remove redundant features to streamline the model without losing much information.</li>
</ul>

<h3 id="4-regularization-techniques-in-high-dimensions">4. <strong>Regularization Techniques in High Dimensions</strong></h3>
<ul>
  <li><strong>Lasso Regression (L1 Regularization)</strong>: Lasso regression tends to select a subset of features by driving coefficients of less important (or redundant) features to zero. By penalizing model complexity, Lasso helps in selecting features that contribute unique information, thus indirectly accounting for the “angle” between features.</li>
  <li><strong>Elastic Net</strong>: This combines L1 and L2 regularization, balancing between feature selection and managing multicollinearity. Elastic Net is effective in high-dimensional spaces where groups of correlated features (small angles) exist. It often selects one feature from each correlated group, effectively reducing redundancy.</li>
</ul>

<h3 id="5-variance-inflation-factor-vif">5. <strong>Variance Inflation Factor (VIF)</strong></h3>
<ul>
  <li><strong>VIF</strong> quantifies how much the variance of a regression coefficient is inflated due to multicollinearity with other features. High VIF values indicate a small angle (high correlation) with other features, suggesting redundancy.</li>
  <li>By removing features with high VIF values, you retain only those features that contribute unique information, reducing the chance of multicollinearity affecting model performance.</li>
</ul>

<h3 id="6-mutual-information-and-feature-selection">6. <strong>Mutual Information and Feature Selection</strong></h3>
<ul>
  <li><strong>Mutual Information (MI)</strong> measures the dependency between features and can be seen as a non-linear analog to cosine similarity for more complex relationships. Small MI values indicate independence (similar to orthogonal vectors), suggesting that features contribute unique information.</li>
  <li>Selecting features with low MI relative to others ensures that each feature adds unique value, similar to selecting features with large angles between them.</li>
</ul>

<h3 id="practical-approach-for-feature-selection-using-angles">Practical Approach for Feature Selection Using Angles</h3>
<p>If you want to use angles explicitly for feature selection:</p>
<ol>
  <li><strong>Calculate Cosine Similarity Matrix</strong>: Compute the cosine similarity (or Pearson correlation) between each pair of features.</li>
  <li><strong>Set a Threshold</strong>: Decide on a similarity threshold, such as 0.9. For pairs of features with similarity above this threshold (i.e., angle close to 0°), retain only one feature in each pair.</li>
  <li><strong>Select Independent Features</strong>: Keep features with lower cosine similarity (or correlation), effectively selecting features that provide more unique information.</li>
</ol>

<p>These steps can help ensure that your selected features are diverse in their contributions, enhancing model accuracy and stability. Let me know if you’d like assistance with code or examples for any of these techniques!</p>]]></content><author><name></name></author><category term="Update" /><summary type="html"><![CDATA[In feature selection, the angle between features can be a useful tool for understanding and managing redundancy and correlation in the data, especially in high-dimensional spaces. Here’s how the angle between features impacts feature selection and the techniques that can leverage this information:]]></summary></entry><entry><title type="html">Combining Gradient-Boosted Tree Ensembles with Deep Learning</title><link href="http://localhost:4000/update/2024/11/12/preprossesing.html" rel="alternate" type="text/html" title="Combining Gradient-Boosted Tree Ensembles with Deep Learning" /><published>2024-11-12T19:31:29-05:00</published><updated>2024-11-12T19:31:29-05:00</updated><id>http://localhost:4000/update/2024/11/12/preprossesing</id><content type="html" xml:base="http://localhost:4000/update/2024/11/12/preprossesing.html"><![CDATA[<p><strong>“Combining Gradient-Boosted Tree Ensembles with Deep Learning: Implementations and Code Examples of Hybrid Models”</strong></p>

<p>Alternatively, if you’re looking for a more concise title, here are some options:</p>

<ol>
  <li><strong>“Hybrid Models: Integrating Gradient Boosting and Deep Learning with Python Examples”</strong></li>
  <li><strong>“From Trees to Neural Networks: Gradient Boosting-Inspired Deep Learning Models Explained”</strong></li>
  <li><strong>“Deep Learning Meets Gradient Boosting: Python Implementations of Hybrid Algorithms”</strong></li>
</ol>

<p>Each of these titles captures the essence of using deep learning methods inspired by gradient-boosted trees and provides clarity on the focus of the explanation and code examples.</p>

<p>Here’s an abstract Python class that preprocesses data by addressing duplicates, irrelevant information, structural errors, outliers, and missing values, as per your requirements. It also includes functions to visualize data before and after preprocessing. This class uses common libraries like <code class="highlighter-rouge">pandas</code>, <code class="highlighter-rouge">matplotlib</code>, and <code class="highlighter-rouge">seaborn</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>

<span class="k">class</span> <span class="nc">AbstractPreprocessor</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_path</span><span class="p">,</span> <span class="n">file_type</span><span class="o">=</span><span class="s">'csv'</span><span class="p">,</span> <span class="n">irrelevant_columns</span><span class="o">=</span><span class="p">[]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data_path</span> <span class="o">=</span> <span class="n">data_path</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">file_type</span> <span class="o">=</span> <span class="n">file_type</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">irrelevant_columns</span> <span class="o">=</span> <span class="n">irrelevant_columns</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">file_type</span> <span class="o">==</span> <span class="s">'csv'</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data_path</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">file_type</span> <span class="o">==</span> <span class="s">'json'</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_json</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data_path</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Unsupported file type. Please use 'csv' or 'json'."</span><span class="p">)</span>
    
    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">visualize_before</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">visualize_after</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">remove_duplicates</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">duplicates</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">duplicated</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Removing </span><span class="si">{</span><span class="n">duplicates</span><span class="si">}</span><span class="s"> duplicate rows."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">drop_duplicates</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">remove_irrelevant</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Removing irrelevant columns: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">irrelevant_columns</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">irrelevant_columns</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">fix_structural_errors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column</span><span class="p">,</span> <span class="n">correction_dict</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Fixing structural errors in column '</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s">' using provided mapping."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="n">correction_dict</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">handle_outliers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column</span><span class="p">):</span>
        <span class="c1"># Define a simple method to handle outliers using IQR
</span>        <span class="n">Q1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>
        <span class="n">Q3</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span>
        <span class="n">IQR</span> <span class="o">=</span> <span class="n">Q3</span> <span class="o">-</span> <span class="n">Q1</span>
        <span class="n">lower_bound</span> <span class="o">=</span> <span class="n">Q1</span> <span class="o">-</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">IQR</span>
        <span class="n">upper_bound</span> <span class="o">=</span> <span class="n">Q3</span> <span class="o">+</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">IQR</span>
        <span class="n">outliers</span> <span class="o">=</span> <span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">lower_bound</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">upper_bound</span><span class="p">)).</span><span class="nb">sum</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Handling </span><span class="si">{</span><span class="n">outliers</span><span class="si">}</span><span class="s"> outliers in column '</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s">'."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">lower_bound</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">upper_bound</span><span class="p">),</span>
                                   <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">handle_missing_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'mean'</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s">'mean'</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Filling missing values with column means."</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">fillna</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">mean</span><span class="p">())</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s">'median'</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Filling missing values with column medians."</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">fillna</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">median</span><span class="p">())</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s">'mode'</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Filling missing values with column modes."</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">fillna</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">mode</span><span class="p">().</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Method not supported. Use 'mean', 'median', or 'mode'."</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outlier_columns</span><span class="o">=</span><span class="p">[]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">remove_duplicates</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">remove_irrelevant</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">outlier_columns</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">handle_outliers</span><span class="p">(</span><span class="n">column</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">handle_missing_values</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">plot_distributions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">columns</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'Distribution of Features'</span><span class="p">):</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
            <span class="n">sns</span><span class="p">.</span><span class="n">histplot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Example Concrete Implementation
</span><span class="k">class</span> <span class="nc">MyPreprocessor</span><span class="p">(</span><span class="n">AbstractPreprocessor</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">visualize_before</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Visualizing data before preprocessing..."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">plot_distributions</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'Before Preprocessing'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">visualize_after</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Visualizing data after preprocessing..."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">plot_distributions</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'After Preprocessing'</span><span class="p">)</span>

<span class="c1"># Usage Example
</span><span class="n">data_path</span> <span class="o">=</span> <span class="s">"your_dataset.csv"</span>
<span class="n">irrelevant_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'irrelevant_feature'</span><span class="p">]</span>
<span class="n">preprocessor</span> <span class="o">=</span> <span class="n">MyPreprocessor</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">file_type</span><span class="o">=</span><span class="s">'csv'</span><span class="p">,</span> <span class="n">irrelevant_columns</span><span class="o">=</span><span class="n">irrelevant_columns</span><span class="p">)</span>

<span class="c1"># Visualize before processing
</span><span class="n">preprocessor</span><span class="p">.</span><span class="n">visualize_before</span><span class="p">()</span>

<span class="c1"># Preprocess the data
</span><span class="n">preprocessor</span><span class="p">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">outlier_columns</span><span class="o">=</span><span class="p">[</span><span class="s">'feature_with_outliers'</span><span class="p">])</span>

<span class="c1"># Visualize after processing
</span><span class="n">preprocessor</span><span class="p">.</span><span class="n">visualize_after</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="key-functionalities">Key Functionalities:</h3>
<ol>
  <li><strong>Duplicates</strong>: Detects and removes duplicates using <code class="highlighter-rouge">pandas</code>’ <code class="highlighter-rouge">.duplicated()</code> and <code class="highlighter-rouge">.drop_duplicates()</code> methods.</li>
  <li><strong>Irrelevant Columns</strong>: Drops irrelevant columns passed to the class during initialization.</li>
  <li><strong>Structural Errors</strong>: Fixes structural errors in specific columns using a correction dictionary (<code class="highlighter-rouge">correction_dict</code>) that standardizes values.</li>
  <li><strong>Outliers</strong>: Handles outliers using the IQR (Interquartile Range) method, but this can be extended depending on the dataset needs.</li>
  <li><strong>Missing Values</strong>: Fills missing values using mean, median, or mode.</li>
</ol>

<h3 id="visualization">Visualization:</h3>
<p>Before and after distributions are plotted using Seaborn’s <code class="highlighter-rouge">histplot</code> for each feature, allowing you to see the effect of preprocessing.</p>

<p>We can customize the preprocessing steps by creating new methods in the <code class="highlighter-rouge">AbstractPreprocessor</code> class or extending the existing ones in your concrete class (<code class="highlighter-rouge">MyPreprocessor</code> in this case).</p>

<p>In addition to the preprocessing steps already mentioned (duplicates, irrelevant information, structural errors, outliers, and missing values), there are several other important preprocessing techniques that can be applied depending on the dataset and the model you plan to use. Here are some additional preprocessing techniques you can consider:</p>

<h3 id="1-data-type-conversion">1. <strong>Data Type Conversion</strong></h3>
<ul>
  <li><strong>Why?</strong>: Ensures that the data types are correct for each feature. Sometimes numeric columns are read as strings or categorical columns are interpreted as numerical.</li>
  <li><strong>How?</strong>: Convert columns to the appropriate types (e.g., converting strings to categories or integers to floats).
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'column'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'column'</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="s">'category'</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="2-feature-scaling--normalization">2. <strong>Feature Scaling / Normalization</strong></h3>
<ul>
  <li><strong>Why?</strong>: Many machine learning models (like SVM, KNN, or neural networks) perform better when the data is scaled or normalized, as features may be on different scales (e.g., age, income, etc.).</li>
  <li><strong>How?</strong>: Use Min-Max scaling, Z-score normalization, or more advanced methods such as RobustScaler (good for handling outliers).
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
   <span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
   <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[[</span><span class="s">'feature1'</span><span class="p">,</span> <span class="s">'feature2'</span><span class="p">]]</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[[</span><span class="s">'feature1'</span><span class="p">,</span> <span class="s">'feature2'</span><span class="p">]])</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="3-categorical-encoding">3. <strong>Categorical Encoding</strong></h3>
<ul>
  <li><strong>Why?</strong>: Many machine learning algorithms cannot handle categorical data directly and require numerical encoding.</li>
  <li><strong>How?</strong>:
    <ul>
      <li><strong>One-Hot Encoding</strong>: Converts categorical columns into binary columns.</li>
      <li><strong>Label Encoding</strong>: Assigns a unique integer to each category (for tree-based models like Random Forest, XGBoost).
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">LabelEncoder</span>
 <span class="n">encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
 <span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
 <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'category_column'</span><span class="p">])</span>
 <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'label_column'</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'label_column'</span><span class="p">])</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h3 id="4-feature-engineering">4. <strong>Feature Engineering</strong></h3>
<ul>
  <li><strong>Why?</strong>: Creates new meaningful features from the existing ones, which can provide more insights to the model.</li>
  <li><strong>How?</strong>: You can create new columns such as:
    <ul>
      <li><strong>Interaction features</strong>: Multiplying two or more columns together.</li>
      <li><strong>Date/Time features</strong>: Extracting parts of a date like day, month, hour, or even calculating time differences.
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'new_feature'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'feature1'</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'feature2'</span><span class="p">]</span>
 <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'month'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'date'</span><span class="p">]).</span><span class="n">dt</span><span class="p">.</span><span class="n">month</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h3 id="5-dimensionality-reduction">5. <strong>Dimensionality Reduction</strong></h3>
<ul>
  <li><strong>Why?</strong>: High-dimensional data (many features) can cause overfitting or increase computation time. Reducing dimensions can help eliminate redundant information.</li>
  <li><strong>How?</strong>: Techniques like PCA (Principal Component Analysis) or feature selection methods such as removing low-variance features.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
   <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
   <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="6-text-preprocessing">6. <strong>Text Preprocessing</strong></h3>
<ul>
  <li><strong>Why?</strong>: Text data must be cleaned and transformed into a suitable format for NLP models.</li>
  <li><strong>How?</strong>:
    <ul>
      <li><strong>Tokenization</strong>: Splitting text into words or tokens.</li>
      <li><strong>Removing Stopwords</strong>: Eliminating common words that do not carry much information (e.g., “the”, “and”).</li>
      <li><strong>Stemming/Lemmatization</strong>: Reducing words to their base or root form.</li>
      <li><strong>TF-IDF or Bag-of-Words</strong>: Converting text into a numerical representation.
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
 <span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s">'english'</span><span class="p">)</span>
 <span class="n">text_features</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'text_column'</span><span class="p">])</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h3 id="7-handling-imbalanced-datasets">7. <strong>Handling Imbalanced Datasets</strong></h3>
<ul>
  <li><strong>Why?</strong>: If one class is significantly more frequent than others in classification problems, it can bias the model.</li>
  <li><strong>How?</strong>: Use techniques like oversampling (SMOTE), undersampling, or generating synthetic samples.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">SMOTE</span>
   <span class="n">smote</span> <span class="o">=</span> <span class="n">SMOTE</span><span class="p">()</span>
   <span class="n">X_res</span><span class="p">,</span> <span class="n">y_res</span> <span class="o">=</span> <span class="n">smote</span><span class="p">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="8-binningdiscretization">8. <strong>Binning/Discretization</strong></h3>
<ul>
  <li><strong>Why?</strong>: Converts continuous variables into categorical bins, which can help with noisy data or certain models like decision trees.</li>
  <li><strong>How?</strong>: Use <code class="highlighter-rouge">pandas.cut()</code> or <code class="highlighter-rouge">pandas.qcut()</code> to bin numerical values into fixed-width bins or quantile-based bins.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'binned_feature'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">cut</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'feature'</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s">"Low"</span><span class="p">,</span> <span class="s">"Medium"</span><span class="p">,</span> <span class="s">"High"</span><span class="p">])</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="9-time-series-processing">9. <strong>Time Series Processing</strong></h3>
<ul>
  <li><strong>Why?</strong>: Time series data requires special handling, especially if data has a temporal relationship.</li>
  <li><strong>How?</strong>: Check for stationarity, remove trends or seasonality, and create lag features.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'lag_1'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'time_series_column'</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="10-handling-multicollinearity">10. <strong>Handling Multicollinearity</strong></h3>
<ul>
  <li><strong>Why?</strong>: If two or more features are highly correlated, they may not provide much additional value and can confuse models like linear regression.</li>
  <li><strong>How?</strong>: You can calculate the correlation matrix and drop features that have high correlations with others.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="n">corr_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">corr</span><span class="p">().</span><span class="nb">abs</span><span class="p">()</span>
   <span class="n">upper</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">triu</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">bool</span><span class="p">))</span>
   <span class="n">to_drop</span> <span class="o">=</span> <span class="p">[</span><span class="n">column</span> <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">upper</span><span class="p">.</span><span class="n">columns</span> <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">upper</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.9</span><span class="p">)]</span>
   <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">to_drop</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="11-feature-selection">11. <strong>Feature Selection</strong></h3>
<ul>
  <li><strong>Why?</strong>: Choosing the right features can reduce overfitting, improve model performance, and reduce computational time.</li>
  <li><strong>How?</strong>:
    <ul>
      <li><strong>Variance Threshold</strong>: Remove features with very low variance.</li>
      <li><strong>Recursive Feature Elimination (RFE)</strong>: Systematically remove features based on model importance.
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">VarianceThreshold</span>
 <span class="n">selector</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
 <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">selector</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h3 id="12-log-transformation">12. <strong>Log Transformation</strong></h3>
<ul>
  <li><strong>Why?</strong>: Skewed data distributions can be transformed to more normal-like distributions using log transformations.</li>
  <li><strong>How?</strong>: Apply <code class="highlighter-rouge">np.log1p()</code> for features with positive skew to normalize their distribution.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'log_transformed'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log1p</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="s">'positive_skew_feature'</span><span class="p">])</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="13-data-imputation-advanced">13. <strong>Data Imputation (Advanced)</strong></h3>
<ul>
  <li><strong>Why?</strong>: For missing values, simple mean/median imputation might not capture patterns in the data. Advanced imputation can consider relationships between features.</li>
  <li><strong>How?</strong>: Techniques like K-Nearest Neighbors (KNN) imputation or iterative imputation methods (e.g., MICE).
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">KNNImputer</span>
   <span class="n">imputer</span> <span class="o">=</span> <span class="n">KNNImputer</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
   <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">imputer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="14-creating-polynomial-features">14. <strong>Creating Polynomial Features</strong></h3>
<ul>
  <li><strong>Why?</strong>: Some non-linear relationships between features can be captured by creating polynomial features.</li>
  <li><strong>How?</strong>: Use polynomial transformations for selected features.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
   <span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
   <span class="bp">self</span><span class="p">.</span><span class="n">df_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[[</span><span class="s">'feature1'</span><span class="p">,</span> <span class="s">'feature2'</span><span class="p">]])</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="15-data-augmentation-for-images-text-etc">15. <strong>Data Augmentation (for images, text, etc.)</strong></h3>
<ul>
  <li><strong>Why?</strong>: In domains like image processing and NLP, augmenting data helps to artificially increase the dataset size, improving model generalization.</li>
  <li><strong>How?</strong>: Techniques such as flipping, rotation for images or synonym replacement for text.</li>
</ul>

<hr />

<h3 id="conclusion">Conclusion:</h3>
<p>The preprocessing techniques you choose will depend on your dataset and model. Combining several of these methods in an efficient and appropriate manner can significantly improve the performance of machine learning models.</p>

<p>Here’s an updated version of the abstract preprocessing class with all the preprocessing methods included, each accompanied by a brief docstring explaining its purpose.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">LabelEncoder</span><span class="p">,</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">VarianceThreshold</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">KNNImputer</span>
<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">SMOTE</span>

<span class="k">class</span> <span class="nc">AbstractPreprocessor</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_path</span><span class="p">,</span> <span class="n">file_type</span><span class="o">=</span><span class="s">'csv'</span><span class="p">,</span> <span class="n">irrelevant_columns</span><span class="o">=</span><span class="p">[]):</span>
        <span class="s">"""
        Initializes the preprocessor with data from a specified file.
        :param data_path: Path to the dataset (csv or json).
        :param file_type: Format of the dataset, either 'csv' or 'json'.
        :param irrelevant_columns: List of column names to drop.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data_path</span> <span class="o">=</span> <span class="n">data_path</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">file_type</span> <span class="o">=</span> <span class="n">file_type</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">irrelevant_columns</span> <span class="o">=</span> <span class="n">irrelevant_columns</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""
        Loads the dataset based on the file type (csv or json).
        """</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">file_type</span> <span class="o">==</span> <span class="s">'csv'</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data_path</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">file_type</span> <span class="o">==</span> <span class="s">'json'</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_json</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data_path</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Unsupported file type. Please use 'csv' or 'json'."</span><span class="p">)</span>

    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">visualize_before</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""
        Abstract method to visualize the dataset before preprocessing.
        Must be implemented in a subclass.
        """</span>
        <span class="k">pass</span>

    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">visualize_after</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""
        Abstract method to visualize the dataset after preprocessing.
        Must be implemented in a subclass.
        """</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">remove_duplicates</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""
        Removes duplicate rows from the dataset.
        """</span>
        <span class="n">duplicates</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">duplicated</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Removing </span><span class="si">{</span><span class="n">duplicates</span><span class="si">}</span><span class="s"> duplicate rows."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">drop_duplicates</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">remove_irrelevant</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""
        Removes irrelevant columns from the dataset.
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Removing irrelevant columns: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">irrelevant_columns</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">irrelevant_columns</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fix_structural_errors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column</span><span class="p">,</span> <span class="n">correction_dict</span><span class="p">):</span>
        <span class="s">"""
        Fixes structural errors in a column by standardizing values using a correction dictionary.
        :param column: Column name where structural errors exist.
        :param correction_dict: A dictionary mapping incorrect values to correct ones.
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Fixing structural errors in column '</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s">' using provided mapping."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="n">correction_dict</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">handle_outliers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column</span><span class="p">):</span>
        <span class="s">"""
        Detects and handles outliers in the specified column using the IQR method.
        :param column: Column name to check for outliers.
        """</span>
        <span class="n">Q1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>
        <span class="n">Q3</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span>
        <span class="n">IQR</span> <span class="o">=</span> <span class="n">Q3</span> <span class="o">-</span> <span class="n">Q1</span>
        <span class="n">lower_bound</span> <span class="o">=</span> <span class="n">Q1</span> <span class="o">-</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">IQR</span>
        <span class="n">upper_bound</span> <span class="o">=</span> <span class="n">Q3</span> <span class="o">+</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">IQR</span>
        <span class="n">outliers</span> <span class="o">=</span> <span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">lower_bound</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">upper_bound</span><span class="p">)).</span><span class="nb">sum</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Handling </span><span class="si">{</span><span class="n">outliers</span><span class="si">}</span><span class="s"> outliers in column '</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s">'."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">lower_bound</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">upper_bound</span><span class="p">),</span>
                                   <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">handle_missing_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'mean'</span><span class="p">):</span>
        <span class="s">"""
        Handles missing values by filling them with the specified method (mean, median, or mode).
        :param method: Method to fill missing values (mean, median, or mode).
        """</span>
        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s">'mean'</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Filling missing values with column means."</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">fillna</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">mean</span><span class="p">())</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s">'median'</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Filling missing values with column medians."</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">fillna</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">median</span><span class="p">())</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s">'mode'</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Filling missing values with column modes."</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">fillna</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">mode</span><span class="p">().</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Method not supported. Use 'mean', 'median', or 'mode'."</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">convert_data_types</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="s">"""
        Converts the data type of the specified column.
        :param column: Column to convert.
        :param dtype: Target data type (e.g., 'category', 'float', etc.).
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Converting column '</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s">' to </span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s">."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">feature_scaling</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">columns</span><span class="p">):</span>
        <span class="s">"""
        Scales specified columns using Min-Max scaling.
        :param columns: List of columns to scale.
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Scaling columns: </span><span class="si">{</span><span class="n">columns</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">columns</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">columns</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">encode_categorical</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">columns</span><span class="p">,</span> <span class="n">encoding_type</span><span class="o">=</span><span class="s">'onehot'</span><span class="p">):</span>
        <span class="s">"""
        Encodes categorical variables using One-Hot or Label encoding.
        :param columns: List of categorical columns to encode.
        :param encoding_type: 'onehot' for One-Hot Encoding or 'label' for Label Encoding.
        """</span>
        <span class="k">if</span> <span class="n">encoding_type</span> <span class="o">==</span> <span class="s">'onehot'</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Applying One-Hot Encoding on columns: </span><span class="si">{</span><span class="n">columns</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">encoding_type</span> <span class="o">==</span> <span class="s">'label'</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Applying Label Encoding on columns: </span><span class="si">{</span><span class="n">columns</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Encoding type not supported. Use 'onehot' or 'label'."</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">feature_engineering</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_column</span><span class="p">,</span> <span class="n">formula</span><span class="p">):</span>
        <span class="s">"""
        Creates a new feature based on a formula combining existing features.
        :param new_column: Name of the new feature.
        :param formula: A lambda function that defines how the new feature is calculated.
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Creating new feature '</span><span class="si">{</span><span class="n">new_column</span><span class="si">}</span><span class="s">'."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">new_column</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reduce_dimensionality</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="s">"""
        Reduces the dimensionality of the dataset using PCA.
        :param n_components: Number of principal components to keep.
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Applying PCA to reduce dataset to </span><span class="si">{</span><span class="n">n_components</span><span class="si">}</span><span class="s"> dimensions."</span><span class="p">)</span>
        <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">bin_numerical</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="s">"""
        Discretizes a numerical column into specified bins.
        :param column: Column to discretize.
        :param bins: Number of bins or custom bin edges.
        :param labels: Labels for the bins.
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Binning column '</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s">' into </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">bins</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s"> categories."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">cut</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">handle_imbalanced_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="s">"""
        Balances imbalanced data using SMOTE (Synthetic Minority Over-sampling Technique).
        :param X: Feature matrix.
        :param y: Target vector.
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Handling imbalanced dataset using SMOTE."</span><span class="p">)</span>
        <span class="n">smote</span> <span class="o">=</span> <span class="n">SMOTE</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">smote</span><span class="p">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">polynomial_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">columns</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="s">"""
        Generates polynomial features for specified columns.
        :param columns: List of columns to apply polynomial expansion.
        :param degree: The degree of polynomial features to generate.
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Generating polynomial features of degree </span><span class="si">{</span><span class="n">degree</span><span class="si">}</span><span class="s"> for columns: </span><span class="si">{</span><span class="n">columns</span><span class="si">}</span><span class="s">."</span><span class="p">)</span>
        <span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">columns</span><span class="p">]</span> <span class="o">=</span> <span class="n">poly</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">columns</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">remove_low_variance_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="s">"""
        Removes features with variance below a given threshold.
        :param threshold: Variance threshold below which features will be removed.
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Removing features with variance lower than </span><span class="si">{</span><span class="n">threshold</span><span class="si">}</span><span class="s">."</span><span class="p">)</span>
        <span class="n">selector</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">selector</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">log_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column</span><span class="p">):</span>
        <span class="s">"""
        Applies a log transformation to the specified column to reduce skewness.
        :param column: Column to transform.
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Applying log transformation to column '</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s">'."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log1p</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">impute_missing_values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""
        Imputes missing values using KNN imputation.
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Imputing missing values using KNN Imputer."</span><span class="p">)</span>
        <span class="n">imputer</span> <span class="o">=</span> <span class="n">KNNImputer</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">imputer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outlier_columns</span><span class="o">=</span><span class="p">[]):</span>
        <span class="s">"""
        Runs the entire preprocessing pipeline: duplicates, irrelevant columns, outliers, and missing values.
        :param outlier_columns: List of columns to check for outliers.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">remove_duplicates</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">remove_irrelevant</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">outlier_columns</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">handle_outliers</span><span class="p">(</span><span class="n">column</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">handle_missing_values</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">plot_distributions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">columns</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'Distribution of Features'</span><span class="p">):</span>
        <span class="s">"""
        Plots distributions of specified columns before and after preprocessing.
        :param columns: List of columns to plot.


        :param title: Plot title.
        """</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
            <span class="n">sns</span><span class="p">.</span><span class="n">histplot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Example Concrete Implementation
</span><span class="k">class</span> <span class="nc">MyPreprocessor</span><span class="p">(</span><span class="n">AbstractPreprocessor</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">visualize_before</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""
        Visualizes the data before preprocessing.
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Visualizing data before preprocessing..."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">plot_distributions</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'Before Preprocessing'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">visualize_after</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""
        Visualizes the data after preprocessing.
        """</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Visualizing data after preprocessing..."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">plot_distributions</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'After Preprocessing'</span><span class="p">)</span>

<span class="c1"># Usage Example
</span><span class="n">data_path</span> <span class="o">=</span> <span class="s">"your_dataset.csv"</span>
<span class="n">irrelevant_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'irrelevant_feature'</span><span class="p">]</span>
<span class="n">preprocessor</span> <span class="o">=</span> <span class="n">MyPreprocessor</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">file_type</span><span class="o">=</span><span class="s">'csv'</span><span class="p">,</span> <span class="n">irrelevant_columns</span><span class="o">=</span><span class="n">irrelevant_columns</span><span class="p">)</span>

<span class="c1"># Visualize before processing
</span><span class="n">preprocessor</span><span class="p">.</span><span class="n">visualize_before</span><span class="p">()</span>

<span class="c1"># Preprocess the data
</span><span class="n">preprocessor</span><span class="p">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">outlier_columns</span><span class="o">=</span><span class="p">[</span><span class="s">'feature_with_outliers'</span><span class="p">])</span>

<span class="c1"># Visualize after processing
</span><span class="n">preprocessor</span><span class="p">.</span><span class="n">visualize_after</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="summary-of-added-methods">Summary of Added Methods:</h3>

<ol>
  <li><strong><code class="highlighter-rouge">convert_data_types()</code></strong>: Converts data types for columns to ensure correct interpretation.</li>
  <li><strong><code class="highlighter-rouge">feature_scaling()</code></strong>: Scales columns to a specific range using Min-Max scaling.</li>
  <li><strong><code class="highlighter-rouge">encode_categorical()</code></strong>: Handles categorical encoding (one-hot or label encoding).</li>
  <li><strong><code class="highlighter-rouge">feature_engineering()</code></strong>: Adds new features derived from existing ones using a formula.</li>
  <li><strong><code class="highlighter-rouge">reduce_dimensionality()</code></strong>: Applies PCA for dimensionality reduction.</li>
  <li><strong><code class="highlighter-rouge">bin_numerical()</code></strong>: Discretizes continuous numerical data into bins.</li>
  <li><strong><code class="highlighter-rouge">handle_imbalanced_data()</code></strong>: Uses SMOTE to address class imbalance.</li>
  <li><strong><code class="highlighter-rouge">polynomial_features()</code></strong>: Generates polynomial features to model non-linear relationships.</li>
  <li><strong><code class="highlighter-rouge">remove_low_variance_features()</code></strong>: Removes features with low variance.</li>
  <li><strong><code class="highlighter-rouge">log_transform()</code></strong>: Applies log transformation to skewed data.</li>
  <li><strong><code class="highlighter-rouge">impute_missing_values()</code></strong>: Uses KNN imputation to fill missing values.</li>
</ol>

<p>This abstract class provides a robust preprocessing pipeline, addressing both basic and advanced preprocessing tasks. The example concrete class <code class="highlighter-rouge">MyPreprocessor</code> implements the visualization methods.</p>

<p>The best algorithm for gradient-boosted tree ensembles depends on the specific task, data, and computational resources available. However, the following are some of the most popular and highly regarded algorithms used for gradient boosting, each with its strengths and unique features:</p>

<h3 id="1-xgboost-extreme-gradient-boosting">1. <strong>XGBoost (Extreme Gradient Boosting)</strong></h3>
<ul>
  <li><strong>Strengths</strong>:
    <ul>
      <li>Extremely popular for structured/tabular data and consistently performs well in machine learning competitions (e.g., Kaggle).</li>
      <li>Implements regularization (L1 and L2), which helps prevent overfitting.</li>
      <li>Features include column sampling, advanced tree pruning, efficient handling of sparse data, and fast training speed.</li>
      <li>Parallelized computation makes it faster than other algorithms.</li>
      <li>Supports handling of missing values naturally during training.</li>
    </ul>
  </li>
  <li><strong>Use Cases</strong>:
    <ul>
      <li>Works well with both regression and classification tasks, time series forecasting, and ranking problems.</li>
    </ul>
  </li>
  <li><strong>Limitations</strong>:
    <ul>
      <li>Can be memory-intensive for very large datasets.</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   pip <span class="nb">install </span>xgboost
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="n">xgb</span>
   <span class="n">model</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="n">XGBClassifier</span><span class="p">()</span>  <span class="c1"># or XGBRegressor() for regression
</span>   <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="2-lightgbm-light-gradient-boosting-machine">2. <strong>LightGBM (Light Gradient Boosting Machine)</strong></h3>
<ul>
  <li><strong>Strengths</strong>:
    <ul>
      <li>Known for its speed and efficiency, especially with large datasets.</li>
      <li>Uses a technique called “leaf-wise” growth (instead of the traditional level-wise approach), which results in deeper trees and higher efficiency.</li>
      <li>Scales to very large datasets and provides excellent performance on high-dimensional data.</li>
      <li>Works well with categorical features, using native support for categorical features without needing one-hot encoding.</li>
      <li>Memory-efficient, and faster compared to XGBoost for many tasks.</li>
    </ul>
  </li>
  <li><strong>Use Cases</strong>:
    <ul>
      <li>Well-suited for large datasets, high-dimensional data, and tasks that need fast training times.</li>
    </ul>
  </li>
  <li><strong>Limitations</strong>:
    <ul>
      <li>Sometimes more prone to overfitting due to the aggressive tree growth.</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   pip <span class="nb">install </span>lightgbm
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="kn">import</span> <span class="nn">lightgbm</span> <span class="k">as</span> <span class="n">lgb</span>
   <span class="n">model</span> <span class="o">=</span> <span class="n">lgb</span><span class="p">.</span><span class="n">LGBMClassifier</span><span class="p">()</span>  <span class="c1"># or LGBMRegressor() for regression
</span>   <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="3-catboost-categorical-boosting">3. <strong>CatBoost (Categorical Boosting)</strong></h3>
<ul>
  <li><strong>Strengths</strong>:
    <ul>
      <li>Specifically designed to handle categorical features natively without preprocessing or encoding (no need for one-hot encoding or label encoding).</li>
      <li>Provides good performance on datasets with a mix of categorical and numerical features.</li>
      <li>Has automatic handling of missing values.</li>
      <li>Easy to use, with strong default hyperparameters that work well in many cases.</li>
      <li>Provides fast inference, making it suitable for production deployment.</li>
    </ul>
  </li>
  <li><strong>Use Cases</strong>:
    <ul>
      <li>Works well for datasets with categorical features and tabular data where encoding would be a bottleneck.</li>
    </ul>
  </li>
  <li><strong>Limitations</strong>:
    <ul>
      <li>Slower than LightGBM on very large datasets, though faster than XGBoost in many scenarios.</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   pip <span class="nb">install </span>catboost
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="kn">from</span> <span class="nn">catboost</span> <span class="kn">import</span> <span class="n">CatBoostClassifier</span>
   <span class="n">model</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">()</span>
   <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cat_features</span><span class="o">=</span><span class="n">categorical_feature_indices</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="4-histgradientboosting-from-scikit-learn">4. <strong>HistGradientBoosting (from scikit-learn)</strong></h3>
<ul>
  <li><strong>Strengths</strong>:
    <ul>
      <li>Part of <code class="highlighter-rouge">scikit-learn</code>, it offers a histogram-based implementation of gradient boosting, similar to LightGBM and XGBoost.</li>
      <li>Can handle missing values natively.</li>
      <li>Offers categorical feature support through <code class="highlighter-rouge">CategoricalSplitter</code>.</li>
      <li>Very easy to use if you’re already familiar with <code class="highlighter-rouge">scikit-learn</code>.</li>
      <li>Good default hyperparameters and performance that is often competitive with XGBoost and LightGBM.</li>
    </ul>
  </li>
  <li><strong>Use Cases</strong>:
    <ul>
      <li>Good for medium to large datasets where you want a fast and straightforward implementation within the <code class="highlighter-rouge">scikit-learn</code> ecosystem.</li>
    </ul>
  </li>
  <li><strong>Limitations</strong>:
    <ul>
      <li>Not as fast or memory efficient as LightGBM or XGBoost for very large datasets.</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   pip <span class="nb">install</span> <span class="nt">-U</span> scikit-learn
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingClassifier</span>
   <span class="n">model</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">()</span>
   <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="5-ngboost-natural-gradient-boosting">5. <strong>NGBoost (Natural Gradient Boosting)</strong></h3>
<ul>
  <li><strong>Strengths</strong>:
    <ul>
      <li>Focuses on probabilistic predictions, providing full predictive distributions rather than point estimates.</li>
      <li>Unique among the boosting algorithms for its ability to model uncertainty and provide interpretable confidence intervals.</li>
    </ul>
  </li>
  <li><strong>Use Cases</strong>:
    <ul>
      <li>Best suited for applications where uncertainty in predictions is crucial, such as healthcare, risk modeling, or finance.</li>
    </ul>
  </li>
  <li><strong>Limitations</strong>:
    <ul>
      <li>Slower than XGBoost, LightGBM, and CatBoost on larger datasets.</li>
    </ul>
  </li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   pip <span class="nb">install </span>ngboost
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="kn">from</span> <span class="nn">ngboost</span> <span class="kn">import</span> <span class="n">NGBClassifier</span>
   <span class="n">model</span> <span class="o">=</span> <span class="n">NGBClassifier</span><span class="p">()</span>
   <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="6-gradientboosting-from-scikit-learn">6. <strong>GradientBoosting (from scikit-learn)</strong></h3>
<ul>
  <li><strong>Strengths</strong>:
    <ul>
      <li>Classic implementation of gradient boosting in <code class="highlighter-rouge">scikit-learn</code>, very simple and easy to use.</li>
      <li>Suitable for small to medium-sized datasets.</li>
      <li>Part of the robust and reliable <code class="highlighter-rouge">scikit-learn</code> framework, making it easy to integrate into standard workflows.</li>
    </ul>
  </li>
  <li><strong>Use Cases</strong>:
    <ul>
      <li>Simple tasks where you don’t need the advanced features provided by XGBoost, LightGBM, or CatBoost.</li>
    </ul>
  </li>
  <li><strong>Limitations</strong>:
    <ul>
      <li>Slower and less efficient compared to newer gradient boosting implementations.</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
   <span class="n">model</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">()</span>
   <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h3 id="comparison-of-gradient-boosting-algorithms">Comparison of Gradient Boosting Algorithms:</h3>

<table>
  <thead>
    <tr>
      <th>Algorithm</th>
      <th>Speed</th>
      <th>Memory Efficiency</th>
      <th>Handling Large Datasets</th>
      <th>Missing Values</th>
      <th>Categorical Data Handling</th>
      <th>Performance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>XGBoost</strong></td>
      <td>Fast</td>
      <td>Moderate</td>
      <td>Good</td>
      <td>Yes</td>
      <td>No (requires encoding)</td>
      <td>High</td>
    </tr>
    <tr>
      <td><strong>LightGBM</strong></td>
      <td>Very Fast</td>
      <td>High</td>
      <td>Excellent</td>
      <td>Yes</td>
      <td>Native support</td>
      <td>Very High</td>
    </tr>
    <tr>
      <td><strong>CatBoost</strong></td>
      <td>Moderate</td>
      <td>Moderate</td>
      <td>Good</td>
      <td>Yes</td>
      <td>Native support</td>
      <td>High</td>
    </tr>
    <tr>
      <td><strong>HistGB (sklearn)</strong></td>
      <td>Fast</td>
      <td>Moderate</td>
      <td>Good</td>
      <td>Yes</td>
      <td>Yes (via splitter)</td>
      <td>High</td>
    </tr>
    <tr>
      <td><strong>NGBoost</strong></td>
      <td>Moderate</td>
      <td>Moderate</td>
      <td>Moderate</td>
      <td>No</td>
      <td>No (requires encoding)</td>
      <td>Special (uncertainty)</td>
    </tr>
    <tr>
      <td><strong>GradientBoosting (sklearn)</strong></td>
      <td>Moderate</td>
      <td>Moderate</td>
      <td>Moderate</td>
      <td>No</td>
      <td>No (requires encoding) Moderate</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h3 id="conclusion-1">Conclusion:</h3>
<ul>
  <li><strong>LightGBM</strong>: Best for very large datasets due to its speed and efficiency.</li>
  <li><strong>XGBoost</strong>: Offers great performance, especially with structured data, and provides advanced control over regularization.</li>
  <li><strong>CatBoost</strong>: Ideal for datasets with categorical features, where it outperforms other algorithms without needing extra encoding.</li>
  <li><strong>HistGradientBoosting</strong>: A solid and easy-to-use choice for those already working with <code class="highlighter-rouge">scikit-learn</code>.</li>
  <li><strong>NGBoost</strong>: Best if you need uncertainty modeling and probabilistic outputs.</li>
</ul>

<p>For most general use cases, <strong>LightGBM</strong> and <strong>XGBoost</strong> are often the go-to algorithms for gradient-boosted tree ensembles. If your dataset has a lot of categorical features, <strong>CatBoost</strong> may be the best choice.</p>

<p>There are deep learning algorithms inspired by the principles of gradient-boosted tree ensembles. These algorithms aim to combine the strengths of gradient boosting (e.g., sequential training, handling complex patterns, and high accuracy in tabular data) with the power of deep learning models. While gradient-boosted tree ensembles are powerful in structured/tabular data, deep learning models, especially neural networks, excel in unstructured data (images, text, audio). Some algorithms blend both worlds to tackle structured data more effectively.</p>

<p>Here are a few notable deep learning algorithms inspired by gradient-boosted tree ensembles:</p>

<h3 id="1-deepgbm">1. <strong>DeepGBM</strong></h3>
<ul>
  <li><strong>Description</strong>: DeepGBM integrates gradient boosting decision trees (GBDT) with deep learning models to improve performance on tabular datasets. The key idea is to leverage the GBDT’s feature extraction capabilities to enhance the inputs to a neural network.</li>
  <li><strong>How it works</strong>:
    <ul>
      <li>GBDT models are used to generate feature representations (leaf indices or intermediate values).</li>
      <li>These features are then passed as inputs into a deep learning model (typically a fully connected neural network).</li>
      <li>This approach combines the interpretability and strength of GBDT with the learning capacity of deep learning.</li>
    </ul>
  </li>
  <li><strong>Use Cases</strong>: Effective for tabular data where both feature interactions and deep learning’s representation learning capabilities can be leveraged.</li>
</ul>

<p><strong>Reference</strong>: <a href="https://arxiv.org/abs/1910.03622">DeepGBM: A Deep Learning Framework Distilled by GBDT for Online Prediction Tasks</a></p>

<h3 id="2-deep-neural-decision-forests">2. <strong>Deep Neural Decision Forests</strong></h3>
<ul>
  <li><strong>Description</strong>: Neural Decision Forests combine the hierarchical structure of decision trees with the representational power of deep learning. The algorithm models the decision-making process of trees as a probabilistic combination of decisions, where deep learning helps guide the feature transformation.</li>
  <li><strong>How it works</strong>:
    <ul>
      <li>A neural network learns feature representations from data.</li>
      <li>These representations are then passed to a decision forest, where each tree uses the features for further decision making.</li>
      <li>The decision tree structure is modeled with soft decisions (using probability distributions), making the entire process differentiable and trainable using backpropagation.</li>
    </ul>
  </li>
  <li><strong>Use Cases</strong>: Suitable for tasks that require hierarchical decision-making like classification and regression tasks.</li>
</ul>

<p><strong>Reference</strong>: <a href="https://arxiv.org/abs/1503.05678">Neural Decision Forests</a></p>

<h3 id="3-boosted-neural-networks-boostnn">3. <strong>Boosted Neural Networks (BoostNN)</strong></h3>
<ul>
  <li><strong>Description</strong>: BoostNN is an algorithm that marries the sequential learning approach of boosting with the representation power of deep neural networks. In BoostNN, neural networks are trained in sequence, with each subsequent network trying to correct the errors made by the previous one (similar to gradient boosting with trees).</li>
  <li><strong>How it works</strong>:
    <ul>
      <li>A sequence of neural networks is trained where each subsequent network focuses on the residual errors from the previous network.</li>
      <li>The networks can be shallow or deep depending on the complexity of the task.</li>
      <li>This approach creates an ensemble of neural networks, similar to how gradient boosting creates an ensemble of decision trees.</li>
    </ul>
  </li>
  <li><strong>Use Cases</strong>: Works well for complex tasks where the errors of one network can be iteratively corrected by subsequent networks.</li>
</ul>

<p><strong>Reference</strong>: <a href="https://arxiv.org/abs/1511.01692">Boosting Neural Networks</a></p>

<h3 id="4-ngboost-for-neural-networks-natural-gradient-boosting">4. <strong>NGBoost for Neural Networks (Natural Gradient Boosting)</strong></h3>
<ul>
  <li><strong>Description</strong>: NGBoost, originally a probabilistic boosting algorithm, has extensions where deep neural networks (DNNs) are used as the base learners instead of traditional decision trees. NGBoost improves neural networks’ capacity to model uncertainty in predictions by applying the natural gradient descent algorithm.</li>
  <li><strong>How it works</strong>:
    <ul>
      <li>Neural networks serve as the base learner for each iteration of boosting.</li>
      <li>Instead of using regular gradient descent, NGBoost applies natural gradients to improve training stability and predictive performance.</li>
      <li>The output of the model includes not just predictions but also the distribution of possible outcomes, allowing for better uncertainty modeling.</li>
    </ul>
  </li>
  <li><strong>Use Cases</strong>: Effective in situations where understanding uncertainty is crucial, such as in medical diagnosis, financial risk analysis, etc.</li>
</ul>

<p><strong>Reference</strong>: <a href="https://arxiv.org/abs/1910.03225">Natural Gradient Boosting</a></p>

<h3 id="5-neural-additive-models-nam">5. <strong>Neural Additive Models (NAM)</strong></h3>
<ul>
  <li><strong>Description</strong>: Neural Additive Models (NAMs) are deep learning models that maintain the interpretability of generalized additive models (GAMs) while leveraging the flexibility of deep neural networks to capture non-linear relationships between features.</li>
  <li><strong>How it works</strong>:
    <ul>
      <li>NAMs model the data as the sum of multiple sub-models (one per feature), similar to how gradient boosting models sum the output of trees.</li>
      <li>Each sub-model is a neural network trained to learn the effect of a single feature, which ensures the model is additive and easy to interpret.</li>
      <li>Unlike traditional neural networks, NAMs provide transparency into feature contributions while maintaining the representational capacity of deep learning.</li>
    </ul>
  </li>
  <li><strong>Use Cases</strong>: Excellent for tabular data, especially in fields like healthcare, finance, or domains requiring model interpretability.</li>
</ul>

<p><strong>Reference</strong>: <a href="https://arxiv.org/abs/2004.13912">NAM: Neural Additive Models</a></p>

<h3 id="6-dart-dropouts-meet-multiple-additive-regression-trees">6. <strong>DART (Dropouts meet Multiple Additive Regression Trees)</strong></h3>
<ul>
  <li><strong>Description</strong>: DART extends gradient-boosted decision trees by introducing dropout, a popular regularization technique in deep learning, to avoid overfitting. It applies dropout to trees in the ensemble rather than neural network units, making it a hybrid between tree ensembles and dropout-based deep learning regularization.</li>
  <li><strong>How it works</strong>:
    <ul>
      <li>During training, some trees are randomly dropped, and only the remaining trees are used to fit the residual errors, similar to how dropout works in neural networks.</li>
      <li>This introduces randomness and helps prevent overfitting in gradient-boosted trees.</li>
    </ul>
  </li>
  <li><strong>Use Cases</strong>: Effective for tasks where traditional gradient boosting might overfit, particularly in noisy datasets.</li>
</ul>

<p><strong>Reference</strong>: <a href="https://arxiv.org/abs/1505.01866">DART: Dropout meets Multiple Additive Regression Trees</a></p>

<h3 id="7-gluonts-deepar">7. <strong>GluonTS (DeepAR)</strong></h3>
<ul>
  <li><strong>Description</strong>: In time series forecasting, models like DeepAR combine the power of autoregressive models with recurrent neural networks (RNNs) to predict future values. While not a direct application of gradient boosting, it borrows the idea of sequential corrections (like gradient boosting does) to refine time series predictions.</li>
  <li><strong>How it works</strong>:
    <ul>
      <li>The model predicts the distribution of future time steps by learning from past patterns. It refines these predictions iteratively in a similar way that gradient boosting refines residuals.</li>
      <li>DeepAR is based on RNNs and can capture long-term dependencies, making it useful for sequential data.</li>
    </ul>
  </li>
  <li><strong>Use Cases</strong>: Time series forecasting tasks, particularly with univariate and multivariate time series data.</li>
</ul>

<p><strong>Reference</strong>: <a href="https://arxiv.org/abs/1704.04110">DeepAR: Probabilistic forecasting with autoregressive recurrent networks</a></p>

<h3 id="8-tabnet">8. <strong>TabNet</strong></h3>
<ul>
  <li><strong>Description</strong>: TabNet is a deep learning model specifically designed for tabular data, directly inspired by tree-based models. It aims to capture the interpretability and sequential decision-making of tree ensembles while utilizing neural attention mechanisms.</li>
  <li><strong>How it works</strong>:
    <ul>
      <li>TabNet uses a combination of sequential attention and feature selection techniques to decide which features to process at each step, mimicking the hierarchical decision-making process of decision trees.</li>
      <li>It also trains in a differentiable, end-to-end manner, leveraging deep learning’s flexibility and power.</li>
    </ul>
  </li>
  <li><strong>Use Cases</strong>: Structured/tabular datasets, where both interpretability and feature selection are crucial.</li>
</ul>

<p><strong>Reference</strong>: <a href="https://arxiv.org/abs/1908.07442">TabNet: Attentive Interpretable Tabular Learning</a></p>

<hr />

<h3 id="conclusion-2">Conclusion:</h3>
<p>Deep learning algorithms like <strong>DeepGBM</strong>, <strong>Neural Decision Forests</strong>, and <strong>BoostNN</strong> draw inspiration from gradient-boosted trees by combining their strengths (e.g., sequential training, feature importance) with the representational power of deep neural networks. Other models like <strong>NAMs</strong> and <strong>TabNet</strong> focus on interpretability, which is a key advantage of gradient-boosted trees.</p>

<p>If you want to combine the advantages of deep learning with the performance and interpretability of gradient boosting, <strong>DeepGBM</strong>, <strong>Neural Decision Forests</strong>, and <strong>TabNet</strong> are excellent places to start.</p>

<p>The algorithms and techniques mentioned, such as <strong>DeepGBM</strong>, <strong>Neural Decision Forests</strong>, <strong>BoostNN</strong>, and <strong>TabNet</strong>, are research-driven models or frameworks that have been proposed and developed by the machine learning community to combine the strengths of gradient-boosted tree ensembles with deep learning methods. Here’s where you can find them and how you can use them:</p>

<h3 id="1-deepgbm-1">1. <strong>DeepGBM</strong>:</h3>
<ul>
  <li><strong>What</strong>: A framework that integrates gradient-boosted decision trees (GBDT) with deep neural networks to handle structured/tabular data more effectively.</li>
  <li><strong>Where</strong>: DeepGBM is a research proposal, and while official implementations may not always be available, similar frameworks or ideas can be implemented manually using libraries like <code class="highlighter-rouge">XGBoost</code> or <code class="highlighter-rouge">LightGBM</code> to extract features and then feed them into a neural network.</li>
  <li><strong>Implementation</strong>: You might need to implement it by combining tree-based models (like LightGBM or XGBoost) with deep learning frameworks (such as PyTorch or TensorFlow) by using GBDT to generate features and passing them into a neural network.</li>
</ul>

<p><strong>Reference</strong>: <a href="https://arxiv.org/abs/1910.03622">DeepGBM: A Deep Learning Framework Distilled by GBDT for Online Prediction Tasks</a></p>

<hr />

<h3 id="2-neural-decision-forests">2. <strong>Neural Decision Forests</strong>:</h3>
<ul>
  <li><strong>What</strong>: An algorithm that merges decision trees with deep neural networks, where the decision-making process of trees is modeled as a probabilistic process, allowing backpropagation to be used for training.</li>
  <li><strong>Where</strong>: You can find implementations or research code for this model in various research papers or open-source repositories. Frameworks like TensorFlow and PyTorch are typically used to implement Neural Decision Forests from scratch.</li>
  <li><strong>Implementation</strong>: You can implement the concept using custom neural network layers that simulate decision trees’ behavior and soft decision boundaries. Some libraries may have preliminary implementations, but you might need to develop it based on the ideas from research papers.</li>
</ul>

<p><strong>Reference</strong>: <a href="https://arxiv.org/abs/1503.05678">Neural Decision Forests</a></p>

<hr />

<h3 id="3-boosted-neural-networks-boostnn-1">3. <strong>Boosted Neural Networks (BoostNN)</strong>:</h3>
<ul>
  <li><strong>What</strong>: A neural network-based ensemble model that applies boosting principles to train multiple networks in sequence, correcting errors iteratively like in gradient boosting.</li>
  <li><strong>Where</strong>: This is mainly a research concept, and you may find open-source implementations based on the paper. However, like DeepGBM, implementing this from scratch is possible using deep learning frameworks like TensorFlow or PyTorch.</li>
  <li><strong>Implementation</strong>: You can implement BoostNN by training a series of neural networks, where each model focuses on the residuals of the previous models in a boosting-like manner.</li>
</ul>

<p><strong>Reference</strong>: <a href="https://arxiv.org/abs/1511.01692">Boosting Neural Networks</a></p>

<hr />

<h3 id="4-ngboost-for-neural-networks">4. <strong>NGBoost for Neural Networks</strong>:</h3>
<ul>
  <li><strong>What</strong>: A probabilistic gradient-boosting framework, NGBoost can be extended to work with deep neural networks, allowing for uncertainty modeling while combining the principles of gradient boosting and neural networks.</li>
  <li><strong>Where</strong>: The official NGBoost library is available on GitHub and through pip, though its default implementation typically uses trees. To extend NGBoost to neural networks, you’d have to modify the framework or build a custom solution.</li>
  <li><strong>Implementation</strong>: You can modify NGBoost or adapt it to work with deep learning models by changing the base learner from trees to neural networks.</li>
</ul>

<p><strong>Library</strong>: <a href="https://github.com/stanfordmlgroup/ngboost">NGBoost GitHub</a></p>

<hr />

<h3 id="5-neural-additive-models-nam-1">5. <strong>Neural Additive Models (NAM)</strong>:</h3>
<ul>
  <li><strong>What</strong>: NAMs extend generalized additive models (GAMs) with neural networks to learn interpretable models while maintaining flexibility in capturing non-linear patterns in the data.</li>
  <li><strong>Where</strong>: Official implementations of NAMs are available on GitHub, making it easy to integrate into your projects using frameworks like TensorFlow or PyTorch.</li>
  <li><strong>Implementation</strong>: You can use the existing NAM library or implement a similar idea using neural networks that train each feature independently and sum their contributions, mimicking the structure of GAMs.</li>
</ul>

<p><strong>Library</strong>: <a href="https://github.com/AMLab-Amsterdam/Neural-Additive-Models">NAM GitHub</a></p>

<hr />

<h3 id="6-dart-dropouts-meet-additive-regression-trees">6. <strong>DART (Dropouts meet Additive Regression Trees)</strong>:</h3>
<ul>
  <li><strong>What</strong>: DART applies dropout, a popular deep learning regularization technique, to gradient-boosted decision trees, making it a hybrid approach.</li>
  <li><strong>Where</strong>: DART is integrated into popular gradient-boosting frameworks like <code class="highlighter-rouge">XGBoost</code>. You can enable DART by specifying it as a boosting method in these libraries.</li>
  <li><strong>Implementation</strong>: Use <code class="highlighter-rouge">XGBoost</code> or <code class="highlighter-rouge">LightGBM</code> and set the booster type to <code class="highlighter-rouge">dart</code> to implement DART in your models.</li>
</ul>

<p><strong>Library</strong>: <a href="https://xgboost.readthedocs.io/en/latest/parameter.html#booster">XGBoost Documentation</a></p>

<hr />

<h3 id="7-gluonts-deepar-1">7. <strong>GluonTS (DeepAR)</strong>:</h3>
<ul>
  <li><strong>What</strong>: DeepAR is a time-series forecasting algorithm that combines autoregressive models with deep learning, particularly RNNs. While it’s not a direct boosting model, it uses sequential correction principles similar to boosting.</li>
  <li><strong>Where</strong>: DeepAR is part of Amazon’s <code class="highlighter-rouge">GluonTS</code> library, which focuses on time series models. It’s easy to use for probabilistic forecasting tasks in time series data.</li>
  <li><strong>Implementation</strong>: Install the GluonTS library and use the built-in <code class="highlighter-rouge">DeepAR</code> model for time series forecasting.</li>
</ul>

<p><strong>Library</strong>: <a href="https://github.com/awslabs/gluon-ts">GluonTS GitHub</a></p>

<hr />

<h3 id="8-tabnet-1">8. <strong>TabNet</strong>:</h3>
<ul>
  <li><strong>What</strong>: A deep learning model designed specifically for tabular data, combining attention mechanisms and tree-like feature selection principles. TabNet allows for interpretability while maintaining the representational power of deep learning.</li>
  <li><strong>Where</strong>: TabNet is available as part of the PyTorch ecosystem, and you can easily install and use it for tabular datasets.</li>
  <li><strong>Implementation</strong>: Use <code class="highlighter-rouge">PyTorch TabNet</code> to train interpretable deep learning models for structured/tabular data.</li>
</ul>

<p><strong>Library</strong>: <a href="https://github.com/dreamquark-ai/tabnet">PyTorch TabNet GitHub</a></p>

<hr />

<h3 id="conclusion-3">Conclusion:</h3>
<p>These algorithms are either research-driven or have open-source implementations. Some, like <strong>TabNet</strong>, are fully available and integrated with frameworks like PyTorch, while others, like <strong>Neural Decision Forests</strong> and <strong>DeepGBM</strong>, might require more custom implementations based on research papers.</p>

<p>For practical usage, <strong>TabNet</strong>, <strong>NGBoost</strong>, and <strong>GluonTS</strong> with <strong>DeepAR</strong> are the most readily available and user-friendly. Others, like <strong>DeepGBM</strong> and <strong>Neural Decision Forests</strong>, may require you to build custom solutions based on research or use ideas from papers to implement them.</p>

<p>Below are basic Python code examples for each of the algorithms or models inspired by gradient-boosted tree ensembles, based on their respective libraries or concepts. For some algorithms that require custom implementation, I provide a conceptual implementation or reference code from available resources.</p>

<h3 id="1-deepgbm-conceptual-example">1. <strong>DeepGBM (Conceptual Example)</strong></h3>

<p>DeepGBM involves extracting features using a gradient-boosting model (e.g., XGBoost or LightGBM) and passing these features into a neural network. Here’s a conceptual implementation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="n">xgb</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="c1"># Step 1: Train GBDT model (XGBoost) to extract features
</span><span class="n">xgb_model</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="n">XGBRegressor</span><span class="p">()</span>
<span class="n">xgb_model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Extract leaf indices from XGBoost model (as feature transformation)
</span><span class="n">leaf_indices</span> <span class="o">=</span> <span class="n">xgb_model</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># Step 2: Create a neural network model
</span><span class="k">class</span> <span class="nc">SimpleNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Prepare the transformed features for input to the neural network
</span><span class="n">X_train_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">leaf_indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_train_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Step 3: Train the neural network on the leaf index features
</span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">X_train_torch</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1"># Training loop
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_train_torch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y_train_torch</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Training complete."</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="2-neural-decision-forests-conceptual-example">2. <strong>Neural Decision Forests (Conceptual Example)</strong></h3>

<p>Neural Decision Forests can be implemented by combining a neural network with probabilistic decision trees. This is a simplified example, as the full implementation is more complex.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">NeuralDecisionForest</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">num_trees</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NeuralDecisionForest</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">decision_trees</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">_build_tree</span><span class="p">(</span><span class="n">depth</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_trees</span><span class="p">)])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_trees</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_build_tree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">):</span>
            <span class="n">layers</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">input_layer</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">tree_outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">decision_trees</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">tree</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
            <span class="n">tree_outputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">tree_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">tree_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">output_layer</span><span class="p">(</span><span class="n">tree_outputs</span><span class="p">)</span>

<span class="c1"># Example usage:
</span><span class="n">model</span> <span class="o">=</span> <span class="n">NeuralDecisionForest</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_trees</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">X_train_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># Example data
</span><span class="n">y_train_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1"># Training loop
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_train_torch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y_train_torch</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Training complete."</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="3-boosted-neural-networks-boostnn-2">3. <strong>Boosted Neural Networks (BoostNN)</strong></h3>

<p>BoostNN can be implemented by training neural networks sequentially, where each new network corrects the errors of the previous ones. Here’s a simple conceptual example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">SimpleNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Training loop with boosting
</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">X_train_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_train_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">X_train_torch</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_train_torch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y_train_torch</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Add the trained model to the ensemble
</span>    <span class="n">models</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="c1"># Adjust target values based on residuals
</span>    <span class="n">y_train_torch</span> <span class="o">=</span> <span class="n">y_train_torch</span> <span class="o">-</span> <span class="n">outputs</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Boosted Neural Networks training complete."</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="4-ngboost-with-neural-networks">4. <strong>NGBoost (with Neural Networks)</strong></h3>

<p>NGBoost is an open-source probabilistic boosting framework, which you can modify to use neural networks as the base learners. Here’s a basic example:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>ngboost
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">ngboost</span> <span class="kn">import</span> <span class="n">NGBRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Example dataset
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># NGBoost with default trees
</span><span class="n">model</span> <span class="o">=</span> <span class="n">NGBRegressor</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"NGBoost training complete."</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="5-neural-additive-models-nams">5. <strong>Neural Additive Models (NAMs)</strong></h3>

<p>NAMs are available as an open-source project, which you can easily install and use:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>nam
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">nam</span> <span class="kn">import</span> <span class="n">NAMClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Example dataset
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># NAM Model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">NAMClassifier</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"NAM training complete."</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="6-dart-dropouts-meet-additive-regression-trees-in-xgboost">6. <strong>DART (Dropouts meet Additive Regression Trees) in XGBoost</strong></h3>

<p>DART is implemented in XGBoost, and you can activate it by setting the <code class="highlighter-rouge">booster</code> parameter to <code class="highlighter-rouge">dart</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>xgboost
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="n">xgb</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Example dataset
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># DART in XGBoost
</span><span class="n">model</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">booster</span><span class="o">=</span><span class="s">'dart'</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"DART training complete."</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="7-deepar-from-gluonts">7. <strong>DeepAR (from GluonTS)</strong></h3>

<p>DeepAR is part of the GluonTS library, designed for time series forecasting:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>gluonts mxnet
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">gluonts.dataset.common</span> <span class="kn">import</span> <span class="n">ListDataset</span>
<span class="kn">from</span> <span class="nn">gluonts.model.deepar</span> <span class="kn">import</span> <span class="n">DeepAREstimator</span>
<span class="kn">from</span> <span class="nn">gluonts.mx.trainer</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Example time series data
</span><span class="n">train_data</span> <span class="o">=</span> <span class="n">ListDataset</span><span class="p">([{</span><span class="s">'target'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="s">'start'</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">Timestamp</span><span class="p">(</span><span class="s">"2020-01-01"</span><span class="p">)}],</span> <span class="n">freq</span><span class="o">=</span><span class="s">'1D'</span><span class="p">)</span>

<span class="c1"># DeepAR Estimator
</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">DeepAREstimator</span><span class="p">(</span><span class="n">freq</span><span class="o">=</span><span class="s">"1D"</span><span class="p">,</span> <span class="n">prediction_length</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">trainer</span><span class="o">=</span><span class="n">Trainer</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
<span class="n">predictor</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">training_data</span><span class="o">=</span><span class="n">train_data</span><span class="p">)</span>

<span class="c1"># Generate predictions
</span><span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"DeepAR training complete."</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="8-tabnet-2">8. <strong>TabNet</strong></h3>

<p>TabNet is available via PyTorch, and here’s how to use it for a classification task:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>pytorch-tabnet
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pytorch_tabnet.tab_model</span> <span class="kn">import</span> <span class="n">TabNetClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Example dataset
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># TabNet model
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">TabNetClassifier</span><span class="p">()</span>
<span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"TabNet training complete."</span><span class="p">)</span>


</code></pre></div></div>

<h3 id="conclusion-4">Conclusion:</h3>
<p>Each of these algorithms represents a unique combination of deep learning and gradient-boosted tree-inspired approaches. You can experiment with them in your specific applications by using the code provided, depending on your problem domain and the dataset you are working with.</p>]]></content><author><name></name></author><category term="Update" /><summary type="html"><![CDATA[“Combining Gradient-Boosted Tree Ensembles with Deep Learning: Implementations and Code Examples of Hybrid Models”]]></summary></entry><entry><title type="html">Downloading and Running a Jekyll Site Locally on Windows Using VSCode</title><link href="http://localhost:4000/update/2024/11/10/Installation-jeykll.html" rel="alternate" type="text/html" title="Downloading and Running a Jekyll Site Locally on Windows Using VSCode" /><published>2024-11-10T19:31:29-05:00</published><updated>2024-11-10T19:31:29-05:00</updated><id>http://localhost:4000/update/2024/11/10/Installation-jeykll</id><content type="html" xml:base="http://localhost:4000/update/2024/11/10/Installation-jeykll.html"><![CDATA[<h3 id="downloading-and-running-a-jekyll-site-locally-on-windows-using-vscode">Downloading and Running a Jekyll Site Locally on Windows Using VSCode</h3>
<p>We will cover in this section:</p>
<ol>
  <li><strong>Setting Up Prerequisites</strong>: Installing Ruby, Jekyll, Git, and VSCode on Windows.</li>
  <li><strong>Cloning the GitHub Repository</strong>: Using Git to download the Jekyll site code from GitHub.</li>
  <li><strong>Installing Dependencies</strong>: Using <code class="highlighter-rouge">bundle install</code> to set up necessary gems for Jekyll.</li>
  <li><strong>Serving the Site Locally</strong>: Running <code class="highlighter-rouge">bundle exec jekyll serve</code> to start the local Jekyll server and preview the site.</li>
</ol>

<p>To download your GitHub repository and run Jekyll on Visual Studio Code (VSCode) on Windows, follow these steps:</p>

<h3 id="step-1-install-prerequisites">Step 1: Install Prerequisites</h3>
<ol>
  <li><strong>Install Ruby and Jekyll</strong>:
    <ul>
      <li>Download and install Ruby for Windows from the <a href="https://rubyinstaller.org/">RubyInstaller</a>.</li>
      <li>After installation, open a new terminal (Command Prompt or PowerShell) and install Jekyll and Bundler by running:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gem <span class="nb">install </span>jekyll bundler
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li><strong>Install Git</strong>:
    <ul>
      <li>Download and install Git for Windows from <a href="https://git-scm.com/">git-scm.com</a>. This will allow you to clone your repository.</li>
    </ul>
  </li>
  <li><strong>Install VSCode</strong>:
    <ul>
      <li>Download and install <a href="https://code.visualstudio.com/">Visual Studio Code</a> if you haven’t already.</li>
    </ul>
  </li>
</ol>

<h3 id="step-2-clone-your-github-repository">Step 2: Clone Your GitHub Repository</h3>
<ol>
  <li><strong>Open Git Bash or Command Prompt</strong>:
    <ul>
      <li>In Windows, open Git Bash, Command Prompt, or PowerShell.</li>
    </ul>
  </li>
  <li><strong>Navigate to Your Desired Directory</strong>:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>path<span class="se">\t</span>o<span class="se">\y</span>our<span class="se">\d</span>esired<span class="se">\d</span>irectory
</code></pre></div>    </div>
  </li>
  <li><strong>Clone the Repository</strong>:
    <ul>
      <li>Replace <code class="highlighter-rouge">your-username</code> and <code class="highlighter-rouge">your-repository</code> with your GitHub username and repository name:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/your-username/your-repository.git
</code></pre></div>        </div>
      </li>
      <li>This will download the repository to your local directory.</li>
    </ul>
  </li>
</ol>

<h3 id="step-3-open-the-project-in-vscode">Step 3: Open the Project in VSCode</h3>
<ol>
  <li>Open Visual Studio Code and go to <code class="highlighter-rouge">File</code> &gt; <code class="highlighter-rouge">Open Folder...</code>.</li>
  <li>Select the folder where you cloned your GitHub repository.</li>
</ol>

<h3 id="step-4-install-jekyll-dependencies">Step 4: Install Jekyll Dependencies</h3>
<ol>
  <li><strong>Open the Terminal in VSCode</strong>:
    <ul>
      <li>Go to <code class="highlighter-rouge">View</code> &gt; <code class="highlighter-rouge">Terminal</code> to open the integrated terminal in VSCode.</li>
    </ul>
  </li>
  <li><strong>Navigate to Your Jekyll Project Directory</strong>:
    <ul>
      <li>Ensure you’re in the correct directory where your Jekyll project is located.</li>
    </ul>
  </li>
  <li><strong>Install Dependencies with Bundler</strong>:
    <ul>
      <li>Run the following command to install the necessary dependencies:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundle <span class="nb">install</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ol>

<h3 id="step-5-serve-the-jekyll-site-locally">Step 5: Serve the Jekyll Site Locally</h3>
<ol>
  <li><strong>Run the Jekyll Server</strong>:
    <ul>
      <li>Start the Jekyll server by running:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundle <span class="nb">exec </span>jekyll serve
</code></pre></div>        </div>
      </li>
      <li>This will start a local server for your Jekyll site. You can access it at <code class="highlighter-rouge">http://127.0.0.1:4000</code> in your browser.</li>
    </ul>
  </li>
  <li><strong>View Live Changes</strong>:
    <ul>
      <li>As you make updates in VSCode, Jekyll will automatically regenerate the site, allowing you to see changes immediately in your browser.</li>
    </ul>
  </li>
</ol>

<h3 id="additional-tips">Additional Tips</h3>
<ul>
  <li><strong>Incremental Builds</strong>: For faster builds during development, you can add the <code class="highlighter-rouge">--incremental</code> flag:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundle <span class="nb">exec </span>jekyll serve <span class="nt">--incremental</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Error Debugging</strong>: If you encounter any errors, try using <code class="highlighter-rouge">--trace</code> for detailed output:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundle <span class="nb">exec </span>jekyll serve <span class="nt">--trace</span>
</code></pre></div>    </div>
  </li>
</ul>]]></content><author><name></name></author><category term="Update" /><summary type="html"><![CDATA[Downloading and Running a Jekyll Site Locally on Windows Using VSCode We will cover in this section: Setting Up Prerequisites: Installing Ruby, Jekyll, Git, and VSCode on Windows. Cloning the GitHub Repository: Using Git to download the Jekyll site code from GitHub. Installing Dependencies: Using bundle install to set up necessary gems for Jekyll. Serving the Site Locally: Running bundle exec jekyll serve to start the local Jekyll server and preview the site.]]></summary></entry><entry><title type="html">Python Packages Specialize for Feature Engineering Techniques</title><link href="http://localhost:4000/update/2024/10/18/Feature_-Engineering.html" rel="alternate" type="text/html" title="Python Packages Specialize for Feature Engineering Techniques" /><published>2024-10-18T20:31:29-04:00</published><updated>2024-10-18T20:31:29-04:00</updated><id>http://localhost:4000/update/2024/10/18/Feature_%20Engineering</id><content type="html" xml:base="http://localhost:4000/update/2024/10/18/Feature_-Engineering.html"><![CDATA[<h3 id="python-packages-specialize-for-feature-engineering-techniques">Python Packages Specialize for Feature Engineering Techniques</h3>
<p>Several Python packages specialize in feature engineering techniques, which can help automate tasks like encoding, scaling, generating interaction features, or extracting domain-specific features. Here are some popular feature engineering packages and examples of how to use them:</p>

<h3 id="1-feature-engine">1. <strong>Feature-engine</strong></h3>

<p>Feature-engine provides a variety of feature engineering techniques, including encoding, variable transformation, feature creation, and feature selection. It integrates well with scikit-learn and allows you to create pipelines with ease.</p>

<h4 id="example-with-feature-engine">Example with Feature-engine</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">feature_engine.encoding</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">feature_engine.imputation</span> <span class="kn">import</span> <span class="n">MeanMedianImputer</span>
<span class="kn">from</span> <span class="nn">feature_engine.transformation</span> <span class="kn">import</span> <span class="n">PowerTransformer</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Load your dataset
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"your_dataset.csv"</span><span class="p">)</span>

<span class="c1"># Example pipeline for feature engineering
# 1. Fill missing values in numeric columns
</span><span class="n">imputer</span> <span class="o">=</span> <span class="n">MeanMedianImputer</span><span class="p">(</span><span class="n">imputation_method</span><span class="o">=</span><span class="s">"mean"</span><span class="p">,</span> <span class="n">variables</span><span class="o">=</span><span class="p">[</span><span class="s">"numerical_column"</span><span class="p">])</span>
<span class="n">df_imputed</span> <span class="o">=</span> <span class="n">imputer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="c1"># 2. Apply one-hot encoding to categorical columns
</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">variables</span><span class="o">=</span><span class="p">[</span><span class="s">"categorical_column"</span><span class="p">])</span>
<span class="n">df_encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_imputed</span><span class="p">)</span>

<span class="c1"># 3. Apply power transformation to normalize skewed distributions
</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">PowerTransformer</span><span class="p">(</span><span class="n">variables</span><span class="o">=</span><span class="p">[</span><span class="s">"numerical_column"</span><span class="p">])</span>
<span class="n">df_transformed</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_encoded</span><span class="p">)</span>

<span class="n">df_transformed</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="2-featuretools">2. <strong>Featuretools</strong></h3>

<p>Featuretools is a library for automated feature engineering, especially useful for relational datasets (i.e., datasets with multiple tables). It uses a technique called “Deep Feature Synthesis” to automatically create new features based on relationships and aggregations.</p>

<h4 id="example-with-featuretools">Example with Featuretools</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">featuretools</span> <span class="k">as</span> <span class="n">ft</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Load data
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"your_dataset.csv"</span><span class="p">)</span>

<span class="c1"># Create an EntitySet and add the dataframe
</span><span class="n">es</span> <span class="o">=</span> <span class="n">ft</span><span class="p">.</span><span class="n">EntitySet</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s">"dataset"</span><span class="p">)</span>
<span class="n">es</span> <span class="o">=</span> <span class="n">es</span><span class="p">.</span><span class="n">entity_from_dataframe</span><span class="p">(</span><span class="n">entity_id</span><span class="o">=</span><span class="s">"data"</span><span class="p">,</span> <span class="n">dataframe</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="s">"index_column"</span><span class="p">)</span>

<span class="c1"># Perform deep feature synthesis to automatically create new features
</span><span class="n">features</span><span class="p">,</span> <span class="n">feature_defs</span> <span class="o">=</span> <span class="n">ft</span><span class="p">.</span><span class="n">dfs</span><span class="p">(</span><span class="n">entityset</span><span class="o">=</span><span class="n">es</span><span class="p">,</span> <span class="n">target_entity</span><span class="o">=</span><span class="s">"data"</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">features</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="3-tsfresh-for-time-series-data">3. <strong>tsfresh</strong> (for Time Series Data)</h3>

<p><code class="highlighter-rouge">tsfresh</code> is a great package for extracting features from time series data. It provides a comprehensive set of feature extraction functions tailored for time series, like statistical metrics and frequency domain transformations.</p>

<h4 id="example-with-tsfresh">Example with tsfresh</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tsfresh</span> <span class="kn">import</span> <span class="n">extract_features</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Assuming your dataset is in a long format with columns "id", "time", and "value"
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s">"id"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="s">"time"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="s">"value"</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>
<span class="p">})</span>

<span class="c1"># Extract features
</span><span class="n">extracted_features</span> <span class="o">=</span> <span class="n">extract_features</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">column_id</span><span class="o">=</span><span class="s">"id"</span><span class="p">,</span> <span class="n">column_sort</span><span class="o">=</span><span class="s">"time"</span><span class="p">)</span>

<span class="n">extracted_features</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="4-scikit-learn-pipelines-with-featureunion">4. <strong>Scikit-learn Pipelines with FeatureUnion</strong></h3>

<p>For basic feature engineering in scikit-learn, the <code class="highlighter-rouge">FeatureUnion</code> module allows you to combine multiple feature engineering steps, like scaling, polynomial features, and encoding, into a single pipeline.</p>

<h4 id="example-with-scikit-learn-pipelines">Example with Scikit-learn Pipelines</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span><span class="p">,</span> <span class="n">FeatureUnion</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Load your dataset
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"your_dataset.csv"</span><span class="p">)</span>

<span class="c1"># Example feature engineering pipeline
</span><span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s">"features"</span><span class="p">,</span> <span class="n">FeatureUnion</span><span class="p">([</span>
        <span class="p">(</span><span class="s">"poly_features"</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">"scaler"</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">())</span>
    <span class="p">]))</span>
<span class="p">])</span>

<span class="c1"># Fit and transform the dataset
</span><span class="n">X_transformed</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s">"numerical_column"</span><span class="p">]])</span>

<span class="n">X_transformed</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="5-kats-for-time-series">5. <strong>Kats</strong> (for Time Series)</h3>

<p>Kats, developed by Facebook, offers advanced feature engineering for time series data, including trend detection, seasonal decomposition, anomaly detection, and feature extraction.</p>

<h4 id="example-with-kats">Example with Kats</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">kats.tsfeatures.tsfeatures</span> <span class="kn">import</span> <span class="n">TsFeatures</span>
<span class="kn">from</span> <span class="nn">kats.consts</span> <span class="kn">import</span> <span class="n">TimeSeriesData</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Example time series data
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s">"time"</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">date_range</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="s">"2020-01-01"</span><span class="p">,</span> <span class="n">periods</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s">"D"</span><span class="p">),</span>
    <span class="s">"value"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="p">})</span>

<span class="c1"># Convert to Kats TimeSeriesData format
</span><span class="n">ts_data</span> <span class="o">=</span> <span class="n">TimeSeriesData</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="c1"># Initialize Kats features and calculate
</span><span class="n">ts_features</span> <span class="o">=</span> <span class="n">TsFeatures</span><span class="p">()</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">ts_features</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">ts_data</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
</code></pre></div></div>

<p>These packages offer a wide range of feature engineering capabilities, from automated feature extraction to custom transformations for specific data types. You can choose the best one depending on your dataset and the type of features you want to generate.</p>]]></content><author><name></name></author><category term="Update" /><summary type="html"><![CDATA[Python Packages Specialize for Feature Engineering Techniques Several Python packages specialize in feature engineering techniques, which can help automate tasks like encoding, scaling, generating interaction features, or extracting domain-specific features. Here are some popular feature engineering packages and examples of how to use them:]]></summary></entry><entry><title type="html">Python Packages Help Automate Dataset Preprocessing</title><link href="http://localhost:4000/update/2024/09/02/Preprossesing.html" rel="alternate" type="text/html" title="Python Packages Help Automate Dataset Preprocessing" /><published>2024-09-02T20:31:29-04:00</published><updated>2024-09-02T20:31:29-04:00</updated><id>http://localhost:4000/update/2024/09/02/Preprossesing</id><content type="html" xml:base="http://localhost:4000/update/2024/09/02/Preprossesing.html"><![CDATA[<h3 id="python-packages-help-automate-dataset-preprocessing">Python Packages Help Automate Dataset Preprocessing</h3>
<p>There are several Python packages that can help automate dataset preprocessing, provide insights, and suggest improvements. Here are some popular ones:</p>

<ol>
  <li><strong>Pandas Profiling</strong>:
    <ul>
      <li>Generates a detailed report with summaries and suggestions on a dataset, including missing values, correlations, outliers, and data type distributions.</li>
      <li>Install it with <code class="highlighter-rouge">pip install pandas-profiling</code>.</li>
      <li>Usage:
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">pandas_profiling</span> <span class="kn">import</span> <span class="n">ProfileReport</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"your_dataset.csv"</span><span class="p">)</span>
<span class="n">profile</span> <span class="o">=</span> <span class="n">ProfileReport</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">"Dataset Report"</span><span class="p">)</span>
<span class="n">profile</span><span class="p">.</span><span class="n">to_notebook_iframe</span><span class="p">()</span>  <span class="c1"># Or save it with profile.to_file("report.html")
</span></code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li><strong>Sweetviz</strong>:
    <ul>
      <li>Similar to Pandas Profiling but focuses more on visualizations and comparisons, especially useful for comparing train and test datasets.</li>
      <li>Install it with <code class="highlighter-rouge">pip install sweetviz</code>.</li>
      <li>Usage:
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">sweetviz</span> <span class="k">as</span> <span class="n">sv</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"your_dataset.csv"</span><span class="p">)</span>
<span class="n">report</span> <span class="o">=</span> <span class="n">sv</span><span class="p">.</span><span class="n">analyze</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">report</span><span class="p">.</span><span class="n">show_html</span><span class="p">(</span><span class="s">"sweetviz_report.html"</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li><strong>AutoML Libraries with Preprocessing Capabilities</strong>:
    <ul>
      <li><strong>Auto-Sklearn</strong>, <strong>TPOT</strong>, and <strong>H2O.ai AutoML</strong> can handle not only preprocessing but also feature selection and model selection. They automate the entire ML pipeline, including data cleaning, feature engineering, and hyperparameter tuning.</li>
      <li>Install with <code class="highlighter-rouge">pip install auto-sklearn</code>, <code class="highlighter-rouge">pip install tpot</code>, or <code class="highlighter-rouge">pip install h2o</code>.</li>
      <li>Usage varies based on the library, but each has comprehensive documentation.</li>
    </ul>
  </li>
  <li><strong>DataPrep</strong>:
    <ul>
      <li>Provides automated data cleaning and preprocessing, plus exploratory data analysis.</li>
      <li>Install it with <code class="highlighter-rouge">pip install dataprep</code>.</li>
      <li>Usage:
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">dataprep.eda</span> <span class="kn">import</span> <span class="n">create_report</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"your_dataset.csv"</span><span class="p">)</span>
<span class="n">create_report</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li><strong>PyCaret</strong>:
    <ul>
      <li>A low-code machine learning library that also offers data preprocessing, feature engineering, and model selection. It even has modules for data imputation, transformation, scaling, and encoding.</li>
      <li>Install it with <code class="highlighter-rouge">pip install pycaret</code>.</li>
      <li>Usage:
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pycaret.classification</span> <span class="kn">import</span> <span class="n">setup</span>

<span class="n">setup</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s">"target_column"</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ol>

<p>Each of these tools provides a variety of automated insights and summaries, so you can choose the one that best fits your needs!</p>

<p>Of the packages mentioned, <strong>PyCaret</strong> and <strong>TPOT</strong> are designed to return a preprocessed dataset as part of their pipeline. Here’s how you can use each of them to get the preprocessed dataset:</p>

<h3 id="1-pycaret">1. <strong>PyCaret</strong></h3>

<p>PyCaret is a low-code machine learning library that not only preprocesses data but also prepares it for training. After setting up, it returns the preprocessed dataset and can show you what transformations were applied.</p>

<h4 id="example-with-pycaret">Example with PyCaret</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">pycaret.classification</span> <span class="kn">import</span> <span class="n">setup</span><span class="p">,</span> <span class="n">compare_models</span><span class="p">,</span> <span class="n">get_config</span>

<span class="c1"># Load your dataset
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"your_dataset.csv"</span><span class="p">)</span>

<span class="c1"># Setup environment for classification (change to 'pycaret.regression' for regression tasks)
</span><span class="n">s</span> <span class="o">=</span> <span class="n">setup</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s">"target_column"</span><span class="p">,</span> <span class="n">silent</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">session_id</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># Get the transformed training dataset
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">get_config</span><span class="p">(</span><span class="s">'X_train'</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">get_config</span><span class="p">(</span><span class="s">'y_train'</span><span class="p">)</span>

<span class="c1"># Check the transformations applied (optional)
</span><span class="n">X_train</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<p>Here, <code class="highlighter-rouge">get_config</code> gives access to the preprocessed training set <code class="highlighter-rouge">X_train</code> and the target labels <code class="highlighter-rouge">y_train</code>. PyCaret also performs automatic feature encoding, scaling, and outlier handling based on the setup.</p>

<h3 id="2-tpot">2. <strong>TPOT</strong></h3>

<p>TPOT is an automated machine learning library that can optimize preprocessing steps and model selection. TPOT doesn’t explicitly return a preprocessed dataset, but it creates a preprocessing pipeline that you can apply to your data.</p>

<h4 id="example-with-tpot">Example with TPOT</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tpot</span> <span class="kn">import</span> <span class="n">TPOTClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Load your dataset
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"your_dataset.csv"</span><span class="p">)</span>

<span class="c1"># Split into features and target
</span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">"target_column"</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">"target_column"</span><span class="p">]</span>

<span class="c1"># Train/test split
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="c1"># Set up TPOT and fit to data
</span><span class="n">tpot</span> <span class="o">=</span> <span class="n">TPOTClassifier</span><span class="p">(</span><span class="n">generations</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">population_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbosity</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tpot</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Export the best pipeline
</span><span class="n">tpot</span><span class="p">.</span><span class="n">export</span><span class="p">(</span><span class="s">"best_pipeline.py"</span><span class="p">)</span>

<span class="c1"># Get the preprocessed data (optional)
</span><span class="n">pipeline</span> <span class="o">=</span> <span class="n">tpot</span><span class="p">.</span><span class="n">fitted_pipeline_</span>
<span class="n">X_train_transformed</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_transformed</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<p>In this example, <code class="highlighter-rouge">pipeline.transform(X_train)</code> returns the transformed dataset using TPOT’s chosen preprocessing pipeline. The exported Python file (<code class="highlighter-rouge">best_pipeline.py</code>) contains the code for the preprocessing and modeling steps, so you can see exactly what TPOT did to transform the data.</p>

<h3 id="3-dataprep-eda-module">3. <strong>DataPrep (EDA module)</strong></h3>

<p>While DataPrep primarily focuses on creating reports and visualizing data, you can use its <strong>cleaning</strong> functionality from <code class="highlighter-rouge">DataPrep.Clean</code> to preprocess the dataset manually.</p>

<h4 id="example-with-dataprep">Example with DataPrep</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">dataprep.clean</span> <span class="kn">import</span> <span class="n">clean_dates</span><span class="p">,</span> <span class="n">clean_text</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Load your dataset
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"your_dataset.csv"</span><span class="p">)</span>

<span class="c1"># Example of specific cleaning functions (text and date)
</span><span class="n">df_cleaned_dates</span> <span class="o">=</span> <span class="n">clean_dates</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">"date_column"</span><span class="p">)</span>  <span class="c1"># Cleans date column
</span><span class="n">df_cleaned_text</span> <span class="o">=</span> <span class="n">clean_text</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">"text_column"</span><span class="p">)</span>    <span class="c1"># Cleans text column
</span>
<span class="c1"># To get the cleaned dataframe
</span><span class="n">df_preprocessed</span> <span class="o">=</span> <span class="n">df_cleaned_dates</span>  <span class="c1"># or combine them as needed
</span></code></pre></div></div>

<p>DataPrep won’t automatically preprocess the dataset but can be used for selective cleaning tasks on text, dates, or duplicates.</p>

<p>These examples demonstrate the most practical approaches in each package for retrieving preprocessed datasets!</p>]]></content><author><name></name></author><category term="Update" /><summary type="html"><![CDATA[Python Packages Help Automate Dataset Preprocessing There are several Python packages that can help automate dataset preprocessing, provide insights, and suggest improvements. Here are some popular ones:]]></summary></entry><entry><title type="html">Code From Scratch Versus Using a Tool Like GitHub Copilot</title><link href="http://localhost:4000/update/2024/09/02/code-vers_copilot.html" rel="alternate" type="text/html" title="Code From Scratch Versus Using a Tool Like GitHub Copilot" /><published>2024-09-02T20:31:29-04:00</published><updated>2024-09-02T20:31:29-04:00</updated><id>http://localhost:4000/update/2024/09/02/code-vers_copilot</id><content type="html" xml:base="http://localhost:4000/update/2024/09/02/code-vers_copilot.html"><![CDATA[<h3 id="code-from-scratch-versus-using-a-tool-like-github-copilot">Code From Scratch Versus Using a Tool Like GitHub Copilot</h3>
<p>The main differences between writing code <strong>from scratch</strong> and using a tool like <strong>GitHub Copilot</strong> or another code completion assistant are related to <strong>efficiency</strong>, <strong>guidance</strong>, and <strong>creativity</strong>. Let’s explore the differences in a few key areas:</p>

<h3 id="1-efficiency">1. <strong>Efficiency</strong></h3>
<ul>
  <li><strong>From Scratch</strong>: Writing from scratch means starting with a blank slate. You need to conceptualize every step and write each line manually, which can be time-consuming. It requires thinking about the entire problem structure, syntax, and functionality yourself.</li>
  <li><strong>With Copilot</strong>: Copilot can quickly generate large sections of code based on comments, prompts, or partially written code. It helps speed up repetitive or boilerplate tasks, like setting up configurations, writing standard functions, or implementing common patterns, making it faster to get started and progress.</li>
</ul>

<h3 id="2-guidance-and-suggestions">2. <strong>Guidance and Suggestions</strong></h3>
<ul>
  <li><strong>From Scratch</strong>: When writing from scratch, you rely solely on your own knowledge or documentation for guidance, which can sometimes lead to slower problem-solving, especially for new or complex concepts.</li>
  <li><strong>With Copilot</strong>: Copilot provides real-time suggestions that can guide your coding direction. It suggests code that fits with what you’re already writing and can sometimes even introduce ideas or approaches you may not have considered. This can be especially helpful when you’re exploring unfamiliar libraries or frameworks.</li>
</ul>

<h3 id="3-learning-and-skill-development">3. <strong>Learning and Skill Development</strong></h3>
<ul>
  <li><strong>From Scratch</strong>: Writing code manually strengthens your understanding of programming concepts, syntax, and problem-solving. You are actively engaging in each step, which can reinforce your skills and help you learn the intricacies of the codebase.</li>
  <li><strong>With Copilot</strong>: While Copilot can accelerate coding, it might limit deep learning if you rely on it too heavily without understanding the underlying logic. It’s easy to accept suggestions without fully processing them, so reviewing and studying what it generates is essential to maintain learning.</li>
</ul>

<h3 id="4-creativity-and-customization">4. <strong>Creativity and Customization</strong></h3>
<ul>
  <li><strong>From Scratch</strong>: You have complete control over the code structure and can build custom solutions tailored to your exact needs. This approach allows for more creativity, as you can experiment freely without relying on predefined patterns or suggestions.</li>
  <li><strong>With Copilot</strong>: Copilot may default to common patterns or solutions, which can sometimes restrict creativity or lead to generic code if you aren’t intentional about customizing it. However, Copilot can still spark new ideas by suggesting alternative ways to solve a problem.</li>
</ul>

<h3 id="5-error-prone-vs-error-corrective">5. <strong>Error-Prone vs. Error-Corrective</strong></h3>
<ul>
  <li><strong>From Scratch</strong>: Writing code manually means you might make more syntax or logical errors, especially when working on complex code. Debugging can take longer since you’ve created all parts yourself.</li>
  <li><strong>With Copilot</strong>: Copilot reduces the chances of syntax errors by suggesting code that is likely syntactically correct, although logical errors are still possible. It can help avoid common mistakes by completing or auto-correcting code structures, reducing some errors but not eliminating them entirely.</li>
</ul>

<h3 id="6-dependency-on-code-assistance">6. <strong>Dependency on Code Assistance</strong></h3>
<ul>
  <li><strong>From Scratch</strong>: When you write code independently, you rely more on your skills and understanding, which builds confidence in tackling projects without assistance.</li>
  <li><strong>With Copilot</strong>: There’s a risk of developing a dependency on Copilot or similar tools if used constantly. It’s important to strike a balance to avoid becoming overly reliant, which might impact your ability to write complex code independently.</li>
</ul>

<h3 id="example-comparison">Example Comparison</h3>

<p>Let’s say you’re writing a Python function to train a machine learning model:</p>

<ul>
  <li>
    <p><strong>From Scratch</strong>: You’ll start by manually importing the necessary libraries, setting up the dataset loading functions, defining the model, training loop, and evaluation code. You’ll carefully consider each line’s purpose and ensure all components work together.</p>
  </li>
  <li>
    <p><strong>With Copilot</strong>: You might start by typing a comment like <code class="highlighter-rouge"># Train a CNN model on the dataset</code> and Copilot might auto-suggest code for setting up the model, loading data, and writing the training loop. It might even add steps like setting a learning rate scheduler or saving checkpoints based on common patterns, helping you move faster. However, it’s essential to understand each line to ensure it’s appropriate for your specific case.</p>
  </li>
</ul>

<p>Using both approaches effectively can help you code efficiently while still deepening your knowledge and skills.</p>

<p>The complexity of code written from scratch versus code generated by tools like GitHub Copilot can vary significantly, as each approach has its unique impact on how complex or streamlined the final code may be. Here’s a breakdown:</p>

<h3 id="1-control-over-complexity">1. <strong>Control Over Complexity</strong></h3>
<ul>
  <li><strong>From Scratch</strong>: Writing code from scratch gives you complete control over the complexity of your solution. You can tailor each part to be as simple or as intricate as needed, and you’re mindful of how each piece fits together. For instance, if you’re building a custom algorithm, you can optimize it for performance and readability as you go, which helps manage complexity.</li>
  <li><strong>With Copilot</strong>: Copilot generates code based on common patterns and popular solutions found in public repositories. It can sometimes introduce complex solutions that, while effective, might not be the simplest approach for your specific case. This complexity may include extra configurations, unnecessary layers, or less-readable code that could complicate debugging or scaling.</li>
</ul>

<h3 id="2-code-readability-and-maintainability">2. <strong>Code Readability and Maintainability</strong></h3>
<ul>
  <li><strong>From Scratch</strong>: When you write from scratch, you have more freedom to prioritize readability and maintainability. Since you’re crafting each line, you’re more likely to include comments, logical organization, and formatting choices that make sense for future maintenance.</li>
  <li><strong>With Copilot</strong>: Copilot may generate code that works, but not necessarily in a style that’s easy to understand or maintain, especially if it’s incorporating complex methods or patterns you didn’t intend to use. Generated code may lack the logical flow or documentation that makes it immediately understandable, so it’s crucial to review, refactor, and document suggestions to keep things manageable.</li>
</ul>

<h3 id="3-scalability-of-complexity">3. <strong>Scalability of Complexity</strong></h3>
<ul>
  <li><strong>From Scratch</strong>: Starting from scratch allows you to design for scalability from the beginning. For example, you can make modular choices—such as separating functions and classes into reusable components—according to your specific requirements.</li>
  <li><strong>With Copilot</strong>: While Copilot can suggest scalable patterns, it might not always do so in the way you’d like or with your project’s structure in mind. If you’re not careful, Copilot may introduce complexity that doesn’t scale well, such as overly nested functions, redundant code, or tightly coupled components that make scaling challenging.</li>
</ul>

<h3 id="4-algorithmic-complexity">4. <strong>Algorithmic Complexity</strong></h3>
<ul>
  <li><strong>From Scratch</strong>: When you write code manually, you have full awareness of algorithmic complexity, such as time and space complexity. You can choose algorithms and data structures best suited to your requirements, which often results in optimized code.</li>
  <li><strong>With Copilot</strong>: Copilot typically suggests solutions that are syntactically correct and commonly used, but it may not always consider algorithmic efficiency. For example, Copilot may suggest a straightforward solution that is more computationally intensive than necessary, especially if it’s based on patterns in open-source code where performance wasn’t the primary focus.</li>
</ul>

<h3 id="5-error-handling-and-edge-cases">5. <strong>Error Handling and Edge Cases</strong></h3>
<ul>
  <li><strong>From Scratch</strong>: When writing from scratch, you’re often more attentive to error handling, validation, and edge cases since you’re deeply involved in each function and step.</li>
  <li><strong>With Copilot</strong>: Copilot might not include specific error handling or edge-case coverage unless you prompt it to do so. It can sometimes miss subtle complexities in error scenarios or exceptions, potentially leading to more brittle code. You may need to add custom checks and handling to ensure robust code.</li>
</ul>

<h3 id="example-comparison-of-complexity">Example Comparison of Complexity</h3>

<p>Imagine you’re writing a function to process data for a machine learning pipeline:</p>

<ul>
  <li>
    <p><strong>From Scratch</strong>: You might manually set up the data loading, validation checks, and processing logic, such as normalization and encoding. This gives you control over each step and ensures that it’s as efficient and straightforward as possible for your specific dataset and use case.</p>
  </li>
  <li>
    <p><strong>With Copilot</strong>: Copilot might suggest a data processing function that includes several extra steps, such as data augmentation techniques or additional encoding, based on patterns it has seen. While helpful, these additional steps can introduce complexity, making the function harder to understand or debug if they aren’t necessary for your case. You may need to simplify or refactor Copilot’s output to align it with your intended approach.</p>
  </li>
</ul>

<h3 id="summary">Summary</h3>

<p>In short:</p>
<ul>
  <li><strong>From Scratch</strong>: Allows for controlled, intentional complexity where you decide exactly how intricate the code should be. It can take longer but typically results in more optimized and tailored code.</li>
  <li><strong>With Copilot</strong>: Can introduce unintended complexity through suggestions that are correct but not necessarily optimized for your situation. It speeds up development but requires careful review to ensure it aligns with your design goals and complexity requirements.</li>
</ul>

<p>Ultimately, balancing both approaches can help you manage complexity effectively, using Copilot to generate ideas or handle repetitive tasks, while refining and optimizing the code yourself for the best balance of efficiency and readability.</p>]]></content><author><name></name></author><category term="Update" /><summary type="html"><![CDATA[Code From Scratch Versus Using a Tool Like GitHub Copilot The main differences between writing code from scratch and using a tool like GitHub Copilot or another code completion assistant are related to efficiency, guidance, and creativity. Let’s explore the differences in a few key areas:]]></summary></entry><entry><title type="html">Recurrent Neural Network Models</title><link href="http://localhost:4000/update/2024/01/23/Forecasting-Alghorithems.html" rel="alternate" type="text/html" title="Recurrent Neural Network Models" /><published>2024-01-23T19:31:29-05:00</published><updated>2024-01-23T19:31:29-05:00</updated><id>http://localhost:4000/update/2024/01/23/Forecasting-Alghorithems</id><content type="html" xml:base="http://localhost:4000/update/2024/01/23/Forecasting-Alghorithems.html"><![CDATA[<h3 id="recurrent-neural-network-models-for-forecasting">Recurrent Neural Network Models For Forecasting</h3>

<p>LSTM achieves this by learning the weights for internal gates that control the recurrent connections within each node. Although developed for sequence data, LSTMs have not proven effective on time series forecasting problems where the output is a function of recent observations, e.g. an autoregressive type forecasting problem, such as the car sales dataset.</p>

<p>In this section, we will explore three variations on the LSTM model for univariate time series forecasting:</p>
<ul>
  <li>Vanilla LSTM: The LSTM network as-is.</li>
  <li>CNN-LSTM: A CNN network that learns input features and an LSTM that interprets them.</li>
  <li>ConvLSTM: A combination of CNNs and LSTMs where the LSTM units read input data using the convolutional process of a CNN.</li>
</ul>]]></content><author><name></name></author><category term="Update" /><summary type="html"><![CDATA[Recurrent Neural Network Models For Forecasting]]></summary></entry><entry><title type="html">Maximum Likelihood Estimator</title><link href="http://localhost:4000/update/2023/02/03/Maximum-Likelihood-Estimator.html" rel="alternate" type="text/html" title="Maximum Likelihood Estimator" /><published>2023-02-03T05:31:29-05:00</published><updated>2023-02-03T05:31:29-05:00</updated><id>http://localhost:4000/update/2023/02/03/Maximum-Likelihood-Estimator</id><content type="html" xml:base="http://localhost:4000/update/2023/02/03/Maximum-Likelihood-Estimator.html"><![CDATA[<p>Suppose that we have access to some data set as follow:
\(x^{(1)},x^{(2)},x^{(3)},\cdots x^{(m)}\).
Our main intresting question is:
can we find a random variable \(X\) and distribution \(P\) such that \(p(X)\) can  well-modeled our data by this distribution?. In general, this is a hard problem. If we consider the class of all possible distributions, there is no way to best fit to the data.</p>

<p>Instead, the common strategy is to choose our distribution from a certain parameterized family of distribution \(p(X;θ)\), parameterized by \(θ\), and have our goal be to find the parameters \(θ\) that fit the data best. Even here there are multiple different approaches that are possible, but at the very least this gives us a more concrete problem that lets us better attack the underlying problem.</p>

<p>In this set of notes, we’ll first answer this estimation problem by appeal to the maximum likelihood estimation (MLE) procedure.</p>
<h3 id="maximum-likelihood-estimation">Maximum likelihood estimation</h3>

<p>Given some parameterized distribution \(p(X;θ)\), and an collection of (independent) samples \(x^{(1)},x^{(2)},x^{(3)},\cdots x^{(m)}.\)
We can compute the probability of observing this set of samples under the distribution, which is simply given by</p>

<p>\(p(x^{(1)},x^{(2)},x^{(3)},\cdots x^{(m)};θ)=∏_{i=1}^mp(x^{(i)};θ)\)
We suppose here samples are all assumed to be independent.The basic idea of maximum likelihood estimation, is that we want to pick parameters \(θ\)that maximize the probaiblity of the observed data; in other words, we want to choose \(θ\) to solve the optimization problem</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <munder>
    <mo lspace="0" rspace="0" movablelimits="true">maximize</mo>
    <mi>&#x03B8;<!-- θ --></mi>
  </munder>
  <mspace width="thickmathspace" />
  <munderover>
    <mo>&#x220F;<!-- ∏ --></mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>m</mi>
  </munderover>
  <mi>p</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mi>x</mi>
    <mrow class="MJX-TeXAtom-ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>;</mo>
  <mi>&#x03B8;<!-- θ --></mi>
  <mo stretchy="false">)</mo>
</math>

<p>or equivalently (because maximizing a function is equivalent to maximizing the log of that function, and we can scale this function arbitrarily).</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <munder>
    <mo lspace="0" rspace="0" movablelimits="true">maximize</mo>
    <mi>&#x03B8;<!-- θ --></mi>
  </munder>
  <mspace width="thickmathspace" />
  <mfrac>
    <mn>1</mn>
    <mi>m</mi>
  </mfrac>
  <munderover>
    <mo>&#x2211;<!-- ∑ --></mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>m</mi>
  </munderover>
  <mi>log</mi>
  <mo>&#x2061;<!-- ⁡ --></mo>
  <mi>p</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mi>x</mi>
    <mrow class="MJX-TeXAtom-ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>;</mo>
  <mi>&#x03B8;<!-- θ --></mi>
  <mo stretchy="false">)</mo>
</math>
<p>The term we are maximizing above is so common that it usually has it’s own name: the log-likelihood of the data, written as</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>&#x2113;<!-- ℓ --></mi>
  <mo stretchy="false">(</mo>
  <mi>&#x03B8;<!-- θ --></mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mfrac>
    <mn>1</mn>
    <mi>m</mi>
  </mfrac>
  <munderover>
    <mo>&#x2211;<!-- ∑ --></mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>m</mi>
  </munderover>
  <mi>log</mi>
  <mo>&#x2061;<!-- ⁡ --></mo>
  <mi>p</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mi>x</mi>
    <mrow class="MJX-TeXAtom-ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>;</mo>
  <mi>&#x03B8;<!-- θ --></mi>
  <mo stretchy="false">)</mo>
</math>
<p>where we explicitly write \(ℓ\) as a function of \(θ\) because we want to emphasize the fact this likelihood depends on the parameters.</p>

<p>This procedure may seem “obvious” when stated like this (of course we want to find the parameters that make the data as likely as possible), but there are actually a number of other estimators that are equally valid or reasonable in many situations. We’ll consider some of these when we discuss hypothesis testing and then later probabilistic modeling, but for now, maximum likelihood estimation will serve as a nice principle for how we fit parameters of distributions to data.</p>

<h3 id="example-bernoulli-distribution">Example: Bernoulli distribution</h3>
<p>Let’s take a simple example as an illustration of this point for the Bernoulli distribution. Recall that a Bernoulli distribution, 
\(p(X;ϕ)\) is a simple binary distribution over random variables taking values in \({0,1}\), parameterized by \(ϕ\), which is just the probability of the random variable being equal to one. Now suppose we have some data \(x^{(1)},x^{(2)},x^{(3)},\cdots x^{(m)}\) with 
\(x^{(i)}∈({0,1})\); what would be a good estimate of the Bernoulli parameter \(ϕ\)? 
For example, maybe we flipped a coin 100 times and 30 of these times it came up heads; what would be a good estimate for the probability that this coin comes up heads?</p>

<p>The “obvious” answer here is that we just estimate \(ϕ\) to be the proportion of 1’s in the data</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>&#x03D5;<!-- ϕ --></mi>
  <mo>=</mo>
  <mfrac>
    <mstyle displaystyle="false" scriptlevel="0">
      <mtext># 1's</mtext>
    </mstyle>
    <mstyle displaystyle="false" scriptlevel="0">
      <mtext># Total</mtext>
    </mstyle>
  </mfrac>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <munderover>
        <mo>&#x2211;<!-- ∑ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi>i</mi>
          <mo>=</mo>
          <mn>1</mn>
        </mrow>
        <mi>m</mi>
      </munderover>
      <msup>
        <mi>x</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">(</mo>
          <mi>i</mi>
          <mo stretchy="false">)</mo>
        </mrow>
      </msup>
    </mrow>
    <mi>m</mi>
  </mfrac>
  <mo>.</mo>
</math>
<p>But why is this the case? If we flip the coin just once, for example, would we expect that we should estimate \(ϕ\) to be either zero or one? Maybe some other estimators exist that can better handle our expectation that the coin “should” be unbiased, i.e., have \(ϕ=1/2\).</p>

<p>While this is certainly true, in fact that maximum likelihood estimate of \(ϕ\) is just the equation above, the number of ones divided by the total number. So this gives some rationale that at least under the principles of maximum likelihood esimation, we should believe that this is a good estimate. However, showing that this is in fact the maximum likelihood estimator is a little more involved that you might expect. Let’s go through the derivation to see how this work.</p>

<p>First, recall that our objective is to choose \(ϕ\) maximize the likelihood, or equivalently the log likelihood of the data, of the observed data \(x^{(1)},x^{(2)},x^{(3)},\cdots x^{(m)}\). This can be written as the optimization problem.</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <munder>
    <mo lspace="0" rspace="0" movablelimits="true">maximize</mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mi>&#x03D5;<!-- ϕ --></mi>
    </mrow>
  </munder>
  <mo>&#x2061;<!-- ⁡ --></mo>
  <munderover>
    <mo>&#x2211;<!-- ∑ --></mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>m</mi>
  </munderover>
  <mi>log</mi>
  <mo>&#x2061;<!-- ⁡ --></mo>
  <mi>p</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mi>x</mi>
    <mrow class="MJX-TeXAtom-ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>;</mo>
  <mi>&#x03D5;<!-- ϕ --></mi>
  <mo stretchy="false">)</mo>
  <mo>.</mo>
</math>

<p>Recall that the probability under a Bernoulli distribution is just \(p(X=1;ϕ)=ϕ\), and \(p(X=0;ϕ)1−ϕ\), , which we can write compactly as</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>p</mi>
  <mo stretchy="false">(</mo>
  <mi>X</mi>
  <mo>=</mo>
  <mi>x</mi>
  <mo>;</mo>
  <mi>&#x03D5;<!-- ϕ --></mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <msup>
    <mi>&#x03D5;<!-- ϕ --></mi>
    <mi>x</mi>
  </msup>
  <mo stretchy="false">(</mo>
  <mn>1</mn>
  <mo>&#x2212;<!-- − --></mo>
  <mi>&#x03D5;<!-- ϕ --></mi>
  <msup>
    <mo stretchy="false">)</mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo>&#x2212;<!-- − --></mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
</math>
<p>it’s easy to see that this equals \(ϕ\) or \(x=1\) and \(1-ϕ\) for \(x=0\). Plugging this in to our maximum likelihood optimization problem we have</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <munder>
    <mo lspace="0" rspace="0" movablelimits="true">maximize</mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mi>&#x03D5;<!-- ϕ --></mi>
    </mrow>
  </munder>
  <mo>&#x2061;<!-- ⁡ --></mo>
  <munderover>
    <mo>&#x2211;<!-- ∑ --></mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>m</mi>
  </munderover>
  <mrow>
    <mo>(</mo>
    <mrow>
      <msup>
        <mi>x</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">(</mo>
          <mi>i</mi>
          <mo stretchy="false">)</mo>
        </mrow>
      </msup>
      <mi>log</mi>
      <mo>&#x2061;<!-- ⁡ --></mo>
      <mi>&#x03D5;<!-- ϕ --></mi>
      <mo>+</mo>
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo>&#x2212;<!-- − --></mo>
      <msup>
        <mi>x</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">(</mo>
          <mi>i</mi>
          <mo stretchy="false">)</mo>
        </mrow>
      </msup>
      <mo stretchy="false">)</mo>
      <mi>log</mi>
      <mo>&#x2061;<!-- ⁡ --></mo>
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo>&#x2212;<!-- − --></mo>
      <mi>&#x03D5;<!-- ϕ --></mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <mo>)</mo>
  </mrow>
</math>

<p>In order to maximize this equation, let’s take the derivative and set it equal to 0 (though we won’t show it, it turns out this function just a single maximum point, which thus must have derivative zero, and so we can find it in this manner). Via some basic calculus we have</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable columnalign="right left" rowspacing="3pt" columnspacing="0em" displaystyle="true">
    <mtr>
      <mtd>
        <mfrac>
          <mi>d</mi>
          <mrow>
            <mi>d</mi>
            <mi>&#x03D5;<!-- ϕ --></mi>
          </mrow>
        </mfrac>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msup>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msup>
            <mi>log</mi>
            <mo>&#x2061;<!-- ⁡ --></mo>
            <mi>&#x03D5;<!-- ϕ --></mi>
            <mo>+</mo>
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo>&#x2212;<!-- − --></mo>
            <msup>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msup>
            <mo stretchy="false">)</mo>
            <mi>log</mi>
            <mo>&#x2061;<!-- ⁡ --></mo>
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo>&#x2212;<!-- − --></mo>
            <mi>&#x03D5;<!-- ϕ --></mi>
            <mo stretchy="false">)</mo>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mtd>
      <mtd>
        <mi></mi>
        <mo>=</mo>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <mfrac>
          <mi>d</mi>
          <mrow>
            <mi>d</mi>
            <mi>&#x03D5;<!-- ϕ --></mi>
          </mrow>
        </mfrac>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msup>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msup>
            <mi>log</mi>
            <mo>&#x2061;<!-- ⁡ --></mo>
            <mi>&#x03D5;<!-- ϕ --></mi>
            <mo>+</mo>
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo>&#x2212;<!-- − --></mo>
            <msup>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msup>
            <mo stretchy="false">)</mo>
            <mi>log</mi>
            <mo>&#x2061;<!-- ⁡ --></mo>
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo>&#x2212;<!-- − --></mo>
            <mi>&#x03D5;<!-- ϕ --></mi>
            <mo stretchy="false">)</mo>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd />
      <mtd>
        <mi></mi>
        <mo>=</mo>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mfrac>
              <msup>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>i</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mi>&#x03D5;<!-- ϕ --></mi>
            </mfrac>
            <mo>&#x2212;<!-- − --></mo>
            <mfrac>
              <mrow>
                <mn>1</mn>
                <mo>&#x2212;<!-- − --></mo>
                <msup>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mo stretchy="false">(</mo>
                    <mi>i</mi>
                    <mo stretchy="false">)</mo>
                  </mrow>
                </msup>
              </mrow>
              <mrow>
                <mn>1</mn>
                <mo>&#x2212;<!-- − --></mo>
                <mi>&#x03D5;<!-- ϕ --></mi>
              </mrow>
            </mfrac>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>

<p>Setting this term equal to zero we have</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable columnalign="right left" rowspacing="3pt" columnspacing="0em" displaystyle="true">
    <mtr>
      <mtd />
      <mtd>
        <mi></mi>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mfrac>
              <msup>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>i</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mi>&#x03D5;<!-- ϕ --></mi>
            </mfrac>
            <mo>&#x2212;<!-- − --></mo>
            <mfrac>
              <mrow>
                <mn>1</mn>
                <mo>&#x2212;<!-- − --></mo>
                <msup>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mo stretchy="false">(</mo>
                    <mi>i</mi>
                    <mo stretchy="false">)</mo>
                  </mrow>
                </msup>
              </mrow>
              <mrow>
                <mn>1</mn>
                <mo>&#x2212;<!-- − --></mo>
                <mi>&#x03D5;<!-- ϕ --></mi>
              </mrow>
            </mfrac>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <mn>0</mn>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mo stretchy="false">&#x27F9;<!-- ⟹ --></mo>
        <mspace width="thickmathspace" />
        <mspace width="thickmathspace" />
      </mtd>
      <mtd>
        <mfrac>
          <mrow>
            <munderover>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>m</mi>
            </munderover>
            <msup>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msup>
          </mrow>
          <mi>&#x03D5;<!-- ϕ --></mi>
        </mfrac>
        <mo>&#x2212;<!-- − --></mo>
        <mfrac>
          <mrow>
            <munderover>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>m</mi>
            </munderover>
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo>&#x2212;<!-- − --></mo>
            <msup>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msup>
            <mo stretchy="false">)</mo>
          </mrow>
          <mrow>
            <mn>1</mn>
            <mo>&#x2212;<!-- − --></mo>
            <mi>&#x03D5;<!-- ϕ --></mi>
          </mrow>
        </mfrac>
        <mo>=</mo>
        <mn>0</mn>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mo stretchy="false">&#x27F9;<!-- ⟹ --></mo>
        <mspace width="thickmathspace" />
        <mspace width="thickmathspace" />
      </mtd>
      <mtd>
        <mi></mi>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <mi>&#x03D5;<!-- ϕ --></mi>
        <mo stretchy="false">)</mo>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <msup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo>&#x2212;<!-- − --></mo>
        <mi>&#x03D5;<!-- ϕ --></mi>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <msup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mn>0</mn>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mo stretchy="false">&#x27F9;<!-- ⟹ --></mo>
        <mspace width="thickmathspace" />
        <mspace width="thickmathspace" />
      </mtd>
      <mtd>
        <mi></mi>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <msup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo>=</mo>
        <mi>&#x03D5;<!-- ϕ --></mi>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo>+</mo>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <msup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mo stretchy="false">&#x27F9;<!-- ⟹ --></mo>
        <mspace width="thickmathspace" />
        <mspace width="thickmathspace" />
      </mtd>
      <mtd>
        <mi></mi>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <msup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo>=</mo>
        <mi>&#x03D5;<!-- ϕ --></mi>
        <mi>m</mi>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mo stretchy="false">&#x27F9;<!-- ⟹ --></mo>
        <mspace width="thickmathspace" />
        <mspace width="thickmathspace" />
      </mtd>
      <mtd>
        <mi>&#x03D5;<!-- ϕ --></mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <munderover>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>m</mi>
            </munderover>
            <msup>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msup>
          </mrow>
          <mi>m</mi>
        </mfrac>
        <mo>.</mo>
      </mtd>
    </mtr>
  </mtable>
</math>
<p>And there we have it, the surprisingly long proof of the fact that if we want to pick \(ϕ\) to maximize the likelihood of the observed data, we need to choose it to be equal to the empirical proportion of the ones. Of course, the objections we had at the beginning of this section were also valid: and in fact this perhaps is not the best estimate of \(ϕ\) if we have very little data, or some prior information about what values \(ϕ\) should take. But it is the estimate of \(ϕ\) that maximizes the probability of the observed data, and if this is a bad estimate then it reflects more on the underlying problem with this procedure than with the proof above. Nonetheless, in the presence of a lot of data, there is actually good reason to use the maximum likelihood estimator, and it is extremely common to use in practice.</p>]]></content><author><name></name></author><category term="update" /><summary type="html"><![CDATA[Suppose that we have access to some data set as follow: \(x^{(1)},x^{(2)},x^{(3)},\cdots x^{(m)}\). Our main intresting question is: can we find a random variable \(X\) and distribution \(P\) such that \(p(X)\) can well-modeled our data by this distribution?. In general, this is a hard problem. If we consider the class of all possible distributions, there is no way to best fit to the data.]]></summary></entry><entry><title type="html">Why Using Sigmoid in NN</title><link href="http://localhost:4000/update/2023/01/31/why-we-use-sigmoid-function-in-NN.html" rel="alternate" type="text/html" title="Why Using Sigmoid in NN" /><published>2023-01-31T05:31:29-05:00</published><updated>2023-01-31T05:31:29-05:00</updated><id>http://localhost:4000/update/2023/01/31/why-we%20use-sigmoid-function-in-NN</id><content type="html" xml:base="http://localhost:4000/update/2023/01/31/why-we-use-sigmoid-function-in-NN.html"><![CDATA[<!--- 

<style>
r { color: Red }
o { color: Orange }
g { color: Green }
</style>

# TODOs:

- <r>TODO:</r> Important thing to do
- <o>TODO:</o> Less important thing to do
- <g>DONE:</g> Breath deeply and improve karma
- 
<span style="color:blue">some *This is Blue italic.* text</span>
This is an HTML comment in Markdown 
$\color{red}{your-text-here}$
-->

<h2 id="model"><strong>Model</strong></h2>

<p>Given a <strong>classification problem</strong>, one of the more straightforward models is the <strong>logistic regression</strong>. But, instead of simply <em>presenting</em> it and using it right away, I am going to <strong>build up to it</strong>. The rationale behind this approach is twofold: First, it will make clear why this algorithm is called logistic <em>regression</em> if it is used for classification; second, you’ll get a <strong>clear understanding of what a <em>logit</em> is</strong>.</p>

<p>Well, since it is called logistic <strong>regression</strong>, I would say that <strong>linear regression</strong> is a good starting point. What would a linear regression model with two features look like?</p>

\[\Huge y=b+w_1x_1+w_2x_2+ϵ\]

<p><em>A linear regression model with two features</em></p>

<p>There is one obvious <strong>problem</strong> with the model above: Our <strong>labels (<em>y</em>)</strong> are <strong>discrete</strong>; that is, they are either <strong>zero</strong> or <strong>one</strong>; no other value is allowed. We need to <strong>change the model slightly</strong> to adapt it to our purposes.</p>

<p><em>What if we assign the <strong>positive</strong> outputs to <strong>one</strong> and the <strong>negative</strong></em> <em>outputs to <strong>zero</strong>?</em></p>

<p>Makes sense, right? We’re already calling them <strong>positive</strong> and <strong>negative</strong> classes anyway; why not put their names to good use? Our model would look like this:</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.004.jpeg" alt="" /></p>

<h2 id="logits"><strong>Logits</strong></h2>

<p>\(\color{red}{\text{Equation above  Mapping a linear regression model to discrete labels.}}\)
To make our lives easier, let’s give the right-hand side of the equation above a name: <strong>logit (<em>z</em>)</strong>.</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.005.jpeg" alt="" /></p>

<h2 id="computing-logits"><em>Computing</em> <strong>logits</strong></h2>

<p>The equation above is strikingly similar to the original <strong>linear regression model</strong>, but we’re calling the resulting value <strong><em>z</em></strong>, or <strong>logit</strong>, instead of <strong><em>y</em></strong>, or <strong>label</strong>.</p>

<p><strong>Does it mean a **logit</strong> is the same as <strong>linear regression</strong>?**</p>

<p>Not quite—there is one <strong>fundamental difference</strong> between them: There is <strong>no error term (<em>epsilon</em>)</strong> in Equation above.
If there is no error term, where does the <strong>uncertainty</strong> come from? I am glad you asked :smiley: That’s the role of the <strong>probability</strong>: Instead of assigning a data point to a <strong>discrete label (zero or one)</strong>, we’ll compute the <strong>probability of a data point’s belonging to the positive class</strong>.</p>

<h2 id="probabilities"><strong>Probabilities</strong></h2>

<p>If a data point has a <strong>logit</strong> that equals <strong>zero</strong>, it is exactly at the decision boundary since it is neither positive nor negative. For the sake of completeness, we assigned it to the <strong>positive class</strong>, but this assignment has <strong>maximum uncertainty</strong>, right? So, the corresponding <strong>probability needs to be 0.5</strong> (50%), since it could go either way.</p>

<p>Following this reasoning, we would like to have <strong>large <em>positive</em> logit values</strong> assigned to <strong><em>higher</em> probabilities</strong> (of being in the positive class) and <strong>large <em>negative</em> logit values</strong> assigned to <strong><em>lower probabilities</em></strong> (of being in the positive class).</p>

<p>For <em>really large</em> positive and negative <strong>logit values (<em>z</em>)</strong>, we would like to have:</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.007.jpeg" alt="" /></p>

<h2 id="probabilities-assigned-to-different-logit-values-z"><em>Probabilities assigned to different logit values (z)</em></h2>

<p>We still need to figure out a <strong>function</strong> that maps <strong>logit values</strong> into <strong>probabilities</strong>. We’ll get there soon enough, but first, we need to talk about…</p>

<p><strong>Odds Ratio</strong></p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.006.png" alt="" /><em>What are the odds?!</em></p>

<p>This is a colloquial expression meaning something very unlikely has happened. But <strong>odds</strong> do not have to refer to an unlikely event or a slim chance. The odds of getting <strong>heads</strong> in a (fair) coin flip are 1 to 1 since there is a 50% chance of success and a 50% chance of failure.</p>

<p>Let’s imagine we are betting on the winner of the World Cup final. There are two countries: <strong>A</strong> and <strong>B</strong>. Country <strong>A</strong> is the <strong>favorite</strong>: It has a 75% chance of winning. So, Country <strong>B</strong> has only a 25% chance of winning. If you bet on Country <strong>A</strong>, your chances of winning—that is, your <strong>odds (in favor)</strong>—are <strong>3 to 1</strong> (75 to 25). If you decide to test your luck and bet on Country <strong>B</strong>, your chances of winning—that is, your <strong>odds (in favor)</strong>—are <strong>1 to 3</strong> (25 to 75), or <strong>0.33 to 1</strong>.</p>

<p>The <strong>odds ratio</strong> is given by the <strong>ratio</strong> between the <strong>probability of success</strong> (<em>p</em>) and the</p>

<p><strong>probability of failure</strong> (<em>q</em>):</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.008.jpeg" alt="" /></p>

<h2 id="odds-ratio"><em>Odds ratio</em></h2>

<p>In code, our odds_ratio() function looks like this:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">odds_ratio</span><span class="p">(</span><span class="n">prob</span><span class="p">):</span>

<span class="k">return</span> <span class="n">prob</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">prob</span><span class="p">)</span>

<span class="n">p</span> <span class="o">=</span> <span class="p">.</span><span class="mi">75</span>

<span class="n">q</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span>

<span class="n">odds_ratio</span><span class="p">(</span><span class="n">p</span><span class="p">),</span> <span class="n">odds_ratio</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

<span class="o">*</span><span class="n">Output</span><span class="o">*</span>

<span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.3333333333333333</span><span class="p">)</span>
</code></pre></div></div>

<p>We can also <strong>plot</strong> the resulting <strong>odds ratios</strong> for probabilities ranging from 1% to 99%. The <em>red dots</em> correspond to the probabilities of 25% (<em>q</em>), 50%, and 75% (<em>p</em>).</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.009.png" alt="" /></p>

<p><strong>Odds ratio</strong></p>

<p>Clearly, the odds ratios (left plot) are <strong>not symmetrical</strong>. But, in a <strong>log scale</strong> (right plot), <strong>they are</strong>. This serves us very well since we’re looking for a <strong>symmetrical function</strong> that maps <strong>logit values</strong> into <strong>probabilities</strong>.</p>

<p>Why does it <strong>need</strong> to be <strong>symmetrical</strong>?</p>

<p>If the function <strong>weren’t</strong> symmetrical, different choices for the <strong>positive class</strong> would produce models that were <strong>not</strong> equivalent. But, using a symmetrical function, we could train <strong>two equivalent models</strong> using the <strong>same dataset</strong>, just flipping the classes:</p>

<ul>
  <li><strong>Blue Model</strong> (the positive class (<em>y=1</em>) corresponds to <strong>blue</strong> points)
    <ul>
      <li>Data Point #1: <strong>P(<em>y=1</em>) = P(blue) = .83</strong> (which is the same as <strong>P(red) = .17</strong>)</li>
    </ul>
  </li>
  <li><strong>Red Model</strong> (the positive class (<em>y=1</em>) corresponds to <strong>red</strong> points)
    <ul>
      <li>Data Point #1: <strong>P(<em>y=1</em>) = P(red) = .17</strong> (which is the same as <strong>P(blue) = .83</strong>)</li>
    </ul>
  </li>
</ul>

<p>##Log Odds Ratio</p>

<p>By taking the <strong>logarithm</strong> of the <strong>odds ratio</strong>, the function is not only <strong>symmetrical</strong>, but also maps <strong>probabilities</strong> into <strong>real numbers</strong>, instead of only the positive ones:</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.010.jpeg" alt="" /></p>

<p><strong>Log odds ratio</strong></p>

<p>In code, our log_odds_ratio() function looks like this:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_odds_ratio</span><span class="p">(</span><span class="n">prob</span><span class="p">):</span>

<span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">odds</span>\<span class="n">_ratio</span><span class="p">(</span><span class="n">prob</span><span class="p">))</span>

<span class="err">$$</span><span class="n">p</span> <span class="o">=</span> <span class="p">.</span><span class="mi">75</span><span class="err">$$</span>

<span class="err">$$</span><span class="n">q</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="err">$$</span>

<span class="err">$$</span>\<span class="n">log_odds_ratio</span><span class="p">(</span><span class="n">p</span><span class="p">),</span> <span class="n">log_odds_ratio</span><span class="p">(</span><span class="n">q</span><span class="p">)</span><span class="err">$$</span>


<span class="o">*</span><span class="n">Output</span><span class="o">*</span>

<span class="p">(</span><span class="mf">1.0986122886681098</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0986122886681098</span><span class="p">)</span>
</code></pre></div></div>

<p>As expected, <strong>probabilities that add up to 100%</strong> (like 75% and 25%) correspond to</p>

<p><strong>log odds ratios</strong> that are the <strong>same in absolute value</strong>. Let’s plot it:</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.009.png" alt="" /></p>

<h2 id="log-odds-ratio-and-probability">Log odds ratio and probability</h2>

<p>On the left, <strong>each probability maps into a log odds ratio</strong>. The <em>red dots</em> correspond to probabilities of 25%, 50%, and 75%, the same as before.</p>

<p>If we <strong>flip</strong> the horizontal and vertical axes (right plot), we are <strong>inverting the function</strong>, thus mapping <strong>each log odds ratio into a probability</strong>. That’s the function we were looking for!</p>

<p>Does its shape look familiar? Wait for it…</p>

<p><strong>From Logits to Probabilities</strong></p>

<p>In the previous section, we were trying to <strong>map logit values into probabilities</strong>, and we’ve just found out, graphically, a function that <strong>maps log odds ratios into probabilities</strong>.</p>

<p>Clearly, our <strong>logits are log odds ratios</strong> :-) Sure, drawing conclusions like this is not very scientific, but the purpose of this exercise is to illustrate how the results of a regression, represented by the <strong>logits (z)</strong>, get to be mapped into probabilities.</p>

<p>So, here’s what we arrived at:</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.011.jpeg" alt="" /></p>

<p><strong>Equation - Regression, logits, and log odds ratios</strong></p>

<p>Let’s work this equation out a bit, inverting, rearranging, and simplifying some terms to <strong>isolate <em>p</em></strong>:</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.012.jpeg" alt="" /></p>

<p><em>Equation - From logits (z) to probabilities (p)</em></p>

<p>Does it look familiar? That’s a <strong>sigmoid function</strong>! It is the <strong>inverse of the log odds ratio</strong>.</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.013.png" alt="" /></p>

<p><strong>Equation - Sigmoid function</strong></p>]]></content><author><name></name></author><category term="update" /><summary type="html"><![CDATA[&lt;!—]]></summary></entry></feed>