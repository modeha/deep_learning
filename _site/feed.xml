<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-01-24T12:59:47-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Deep Learning</title><subtitle>Mohsen Dehghani</subtitle><entry><title type="html">Recurrent Neural Network Models</title><link href="http://localhost:4000/update/2024/01/23/Forecasting-Alghorithems.html" rel="alternate" type="text/html" title="Recurrent Neural Network Models" /><published>2024-01-23T19:31:29-05:00</published><updated>2024-01-23T19:31:29-05:00</updated><id>http://localhost:4000/update/2024/01/23/Forecasting-Alghorithems</id><content type="html" xml:base="http://localhost:4000/update/2024/01/23/Forecasting-Alghorithems.html"><![CDATA[<h3 id="recurrent-neural-network-models-for-forecasting">Recurrent Neural Network Models For Forecasting</h3>

<p>LSTM achieves this by learning the weights for internal gates that control the recurrent connections within each node. Although developed for sequence data, LSTMs have not proven effective on time series forecasting problems where the output is a function of recent observations, e.g. an autoregressive type forecasting problem, such as the car sales dataset.</p>

<p>In this section, we will explore three variations on the LSTM model for univariate time series forecasting:</p>
<ul>
  <li>Vanilla LSTM: The LSTM network as-is.</li>
  <li>CNN-LSTM: A CNN network that learns input features and an LSTM that interprets them.</li>
  <li>ConvLSTM: A combination of CNNs and LSTMs where the LSTM units read input data using the convolutional process of a CNN.</li>
</ul>]]></content><author><name></name></author><category term="Update" /><summary type="html"><![CDATA[Recurrent Neural Network Models For Forecasting]]></summary></entry><entry><title type="html">Maximum Likelihood Estimator</title><link href="http://localhost:4000/update/2023/02/03/Maximum-Likelihood-Estimator.html" rel="alternate" type="text/html" title="Maximum Likelihood Estimator" /><published>2023-02-03T05:31:29-05:00</published><updated>2023-02-03T05:31:29-05:00</updated><id>http://localhost:4000/update/2023/02/03/Maximum-Likelihood-Estimator</id><content type="html" xml:base="http://localhost:4000/update/2023/02/03/Maximum-Likelihood-Estimator.html"><![CDATA[<p>Suppose that we have access to some data set as follow:
\(x^{(1)},x^{(2)},x^{(3)},\cdots x^{(m)}\).
Our main intresting question is:
can we find a random variable \(X\) and distribution \(P\) such that \(p(X)\) can  well-modeled our data by this distribution?. In general, this is a hard problem. If we consider the class of all possible distributions, there is no way to best fit to the data.</p>

<p>Instead, the common strategy is to choose our distribution from a certain parameterized family of distribution \(p(X;θ)\), parameterized by \(θ\), and have our goal be to find the parameters \(θ\) that fit the data best. Even here there are multiple different approaches that are possible, but at the very least this gives us a more concrete problem that lets us better attack the underlying problem.</p>

<p>In this set of notes, we’ll first answer this estimation problem by appeal to the maximum likelihood estimation (MLE) procedure.</p>
<h3 id="maximum-likelihood-estimation">Maximum likelihood estimation</h3>

<p>Given some parameterized distribution \(p(X;θ)\), and an collection of (independent) samples \(x^{(1)},x^{(2)},x^{(3)},\cdots x^{(m)}.\)
We can compute the probability of observing this set of samples under the distribution, which is simply given by</p>

<p>\(p(x^{(1)},x^{(2)},x^{(3)},\cdots x^{(m)};θ)=∏_{i=1}^mp(x^{(i)};θ)\)
We suppose here samples are all assumed to be independent.The basic idea of maximum likelihood estimation, is that we want to pick parameters \(θ\)that maximize the probaiblity of the observed data; in other words, we want to choose \(θ\) to solve the optimization problem</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <munder>
    <mo lspace="0" rspace="0" movablelimits="true">maximize</mo>
    <mi>&#x03B8;<!-- θ --></mi>
  </munder>
  <mspace width="thickmathspace" />
  <munderover>
    <mo>&#x220F;<!-- ∏ --></mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>m</mi>
  </munderover>
  <mi>p</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mi>x</mi>
    <mrow class="MJX-TeXAtom-ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>;</mo>
  <mi>&#x03B8;<!-- θ --></mi>
  <mo stretchy="false">)</mo>
</math>

<p>or equivalently (because maximizing a function is equivalent to maximizing the log of that function, and we can scale this function arbitrarily).</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <munder>
    <mo lspace="0" rspace="0" movablelimits="true">maximize</mo>
    <mi>&#x03B8;<!-- θ --></mi>
  </munder>
  <mspace width="thickmathspace" />
  <mfrac>
    <mn>1</mn>
    <mi>m</mi>
  </mfrac>
  <munderover>
    <mo>&#x2211;<!-- ∑ --></mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>m</mi>
  </munderover>
  <mi>log</mi>
  <mo>&#x2061;<!-- ⁡ --></mo>
  <mi>p</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mi>x</mi>
    <mrow class="MJX-TeXAtom-ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>;</mo>
  <mi>&#x03B8;<!-- θ --></mi>
  <mo stretchy="false">)</mo>
</math>
<p>The term we are maximizing above is so common that it usually has it’s own name: the log-likelihood of the data, written as</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>&#x2113;<!-- ℓ --></mi>
  <mo stretchy="false">(</mo>
  <mi>&#x03B8;<!-- θ --></mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mfrac>
    <mn>1</mn>
    <mi>m</mi>
  </mfrac>
  <munderover>
    <mo>&#x2211;<!-- ∑ --></mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>m</mi>
  </munderover>
  <mi>log</mi>
  <mo>&#x2061;<!-- ⁡ --></mo>
  <mi>p</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mi>x</mi>
    <mrow class="MJX-TeXAtom-ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>;</mo>
  <mi>&#x03B8;<!-- θ --></mi>
  <mo stretchy="false">)</mo>
</math>
<p>where we explicitly write \(ℓ\) as a function of \(θ\) because we want to emphasize the fact this likelihood depends on the parameters.</p>

<p>This procedure may seem “obvious” when stated like this (of course we want to find the parameters that make the data as likely as possible), but there are actually a number of other estimators that are equally valid or reasonable in many situations. We’ll consider some of these when we discuss hypothesis testing and then later probabilistic modeling, but for now, maximum likelihood estimation will serve as a nice principle for how we fit parameters of distributions to data.</p>

<h3 id="example-bernoulli-distribution">Example: Bernoulli distribution</h3>
<p>Let’s take a simple example as an illustration of this point for the Bernoulli distribution. Recall that a Bernoulli distribution, 
\(p(X;ϕ)\) is a simple binary distribution over random variables taking values in \({0,1}\), parameterized by \(ϕ\), which is just the probability of the random variable being equal to one. Now suppose we have some data \(x^{(1)},x^{(2)},x^{(3)},\cdots x^{(m)}\) with 
\(x^{(i)}∈({0,1})\); what would be a good estimate of the Bernoulli parameter \(ϕ\)? 
For example, maybe we flipped a coin 100 times and 30 of these times it came up heads; what would be a good estimate for the probability that this coin comes up heads?</p>

<p>The “obvious” answer here is that we just estimate \(ϕ\) to be the proportion of 1’s in the data</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>&#x03D5;<!-- ϕ --></mi>
  <mo>=</mo>
  <mfrac>
    <mstyle displaystyle="false" scriptlevel="0">
      <mtext># 1's</mtext>
    </mstyle>
    <mstyle displaystyle="false" scriptlevel="0">
      <mtext># Total</mtext>
    </mstyle>
  </mfrac>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <munderover>
        <mo>&#x2211;<!-- ∑ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi>i</mi>
          <mo>=</mo>
          <mn>1</mn>
        </mrow>
        <mi>m</mi>
      </munderover>
      <msup>
        <mi>x</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">(</mo>
          <mi>i</mi>
          <mo stretchy="false">)</mo>
        </mrow>
      </msup>
    </mrow>
    <mi>m</mi>
  </mfrac>
  <mo>.</mo>
</math>
<p>But why is this the case? If we flip the coin just once, for example, would we expect that we should estimate \(ϕ\) to be either zero or one? Maybe some other estimators exist that can better handle our expectation that the coin “should” be unbiased, i.e., have \(ϕ=1/2\).</p>

<p>While this is certainly true, in fact that maximum likelihood estimate of \(ϕ\) is just the equation above, the number of ones divided by the total number. So this gives some rationale that at least under the principles of maximum likelihood esimation, we should believe that this is a good estimate. However, showing that this is in fact the maximum likelihood estimator is a little more involved that you might expect. Let’s go through the derivation to see how this work.</p>

<p>First, recall that our objective is to choose \(ϕ\) maximize the likelihood, or equivalently the log likelihood of the data, of the observed data \(x^{(1)},x^{(2)},x^{(3)},\cdots x^{(m)}\). This can be written as the optimization problem.</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <munder>
    <mo lspace="0" rspace="0" movablelimits="true">maximize</mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mi>&#x03D5;<!-- ϕ --></mi>
    </mrow>
  </munder>
  <mo>&#x2061;<!-- ⁡ --></mo>
  <munderover>
    <mo>&#x2211;<!-- ∑ --></mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>m</mi>
  </munderover>
  <mi>log</mi>
  <mo>&#x2061;<!-- ⁡ --></mo>
  <mi>p</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mi>x</mi>
    <mrow class="MJX-TeXAtom-ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>;</mo>
  <mi>&#x03D5;<!-- ϕ --></mi>
  <mo stretchy="false">)</mo>
  <mo>.</mo>
</math>

<p>Recall that the probability under a Bernoulli distribution is just \(p(X=1;ϕ)=ϕ\), and \(p(X=0;ϕ)1−ϕ\), , which we can write compactly as</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>p</mi>
  <mo stretchy="false">(</mo>
  <mi>X</mi>
  <mo>=</mo>
  <mi>x</mi>
  <mo>;</mo>
  <mi>&#x03D5;<!-- ϕ --></mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <msup>
    <mi>&#x03D5;<!-- ϕ --></mi>
    <mi>x</mi>
  </msup>
  <mo stretchy="false">(</mo>
  <mn>1</mn>
  <mo>&#x2212;<!-- − --></mo>
  <mi>&#x03D5;<!-- ϕ --></mi>
  <msup>
    <mo stretchy="false">)</mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo>&#x2212;<!-- − --></mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
</math>
<p>it’s easy to see that this equals \(ϕ\) or \(x=1\) and \(1-ϕ\) for \(x=0\). Plugging this in to our maximum likelihood optimization problem we have</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <munder>
    <mo lspace="0" rspace="0" movablelimits="true">maximize</mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mi>&#x03D5;<!-- ϕ --></mi>
    </mrow>
  </munder>
  <mo>&#x2061;<!-- ⁡ --></mo>
  <munderover>
    <mo>&#x2211;<!-- ∑ --></mo>
    <mrow class="MJX-TeXAtom-ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>m</mi>
  </munderover>
  <mrow>
    <mo>(</mo>
    <mrow>
      <msup>
        <mi>x</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">(</mo>
          <mi>i</mi>
          <mo stretchy="false">)</mo>
        </mrow>
      </msup>
      <mi>log</mi>
      <mo>&#x2061;<!-- ⁡ --></mo>
      <mi>&#x03D5;<!-- ϕ --></mi>
      <mo>+</mo>
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo>&#x2212;<!-- − --></mo>
      <msup>
        <mi>x</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">(</mo>
          <mi>i</mi>
          <mo stretchy="false">)</mo>
        </mrow>
      </msup>
      <mo stretchy="false">)</mo>
      <mi>log</mi>
      <mo>&#x2061;<!-- ⁡ --></mo>
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo>&#x2212;<!-- − --></mo>
      <mi>&#x03D5;<!-- ϕ --></mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <mo>)</mo>
  </mrow>
</math>

<p>In order to maximize this equation, let’s take the derivative and set it equal to 0 (though we won’t show it, it turns out this function just a single maximum point, which thus must have derivative zero, and so we can find it in this manner). Via some basic calculus we have</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable columnalign="right left" rowspacing="3pt" columnspacing="0em" displaystyle="true">
    <mtr>
      <mtd>
        <mfrac>
          <mi>d</mi>
          <mrow>
            <mi>d</mi>
            <mi>&#x03D5;<!-- ϕ --></mi>
          </mrow>
        </mfrac>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msup>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msup>
            <mi>log</mi>
            <mo>&#x2061;<!-- ⁡ --></mo>
            <mi>&#x03D5;<!-- ϕ --></mi>
            <mo>+</mo>
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo>&#x2212;<!-- − --></mo>
            <msup>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msup>
            <mo stretchy="false">)</mo>
            <mi>log</mi>
            <mo>&#x2061;<!-- ⁡ --></mo>
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo>&#x2212;<!-- − --></mo>
            <mi>&#x03D5;<!-- ϕ --></mi>
            <mo stretchy="false">)</mo>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mtd>
      <mtd>
        <mi></mi>
        <mo>=</mo>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <mfrac>
          <mi>d</mi>
          <mrow>
            <mi>d</mi>
            <mi>&#x03D5;<!-- ϕ --></mi>
          </mrow>
        </mfrac>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msup>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msup>
            <mi>log</mi>
            <mo>&#x2061;<!-- ⁡ --></mo>
            <mi>&#x03D5;<!-- ϕ --></mi>
            <mo>+</mo>
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo>&#x2212;<!-- − --></mo>
            <msup>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msup>
            <mo stretchy="false">)</mo>
            <mi>log</mi>
            <mo>&#x2061;<!-- ⁡ --></mo>
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo>&#x2212;<!-- − --></mo>
            <mi>&#x03D5;<!-- ϕ --></mi>
            <mo stretchy="false">)</mo>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd />
      <mtd>
        <mi></mi>
        <mo>=</mo>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mfrac>
              <msup>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>i</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mi>&#x03D5;<!-- ϕ --></mi>
            </mfrac>
            <mo>&#x2212;<!-- − --></mo>
            <mfrac>
              <mrow>
                <mn>1</mn>
                <mo>&#x2212;<!-- − --></mo>
                <msup>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mo stretchy="false">(</mo>
                    <mi>i</mi>
                    <mo stretchy="false">)</mo>
                  </mrow>
                </msup>
              </mrow>
              <mrow>
                <mn>1</mn>
                <mo>&#x2212;<!-- − --></mo>
                <mi>&#x03D5;<!-- ϕ --></mi>
              </mrow>
            </mfrac>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>

<p>Setting this term equal to zero we have</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable columnalign="right left" rowspacing="3pt" columnspacing="0em" displaystyle="true">
    <mtr>
      <mtd />
      <mtd>
        <mi></mi>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mfrac>
              <msup>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>i</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mi>&#x03D5;<!-- ϕ --></mi>
            </mfrac>
            <mo>&#x2212;<!-- − --></mo>
            <mfrac>
              <mrow>
                <mn>1</mn>
                <mo>&#x2212;<!-- − --></mo>
                <msup>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mo stretchy="false">(</mo>
                    <mi>i</mi>
                    <mo stretchy="false">)</mo>
                  </mrow>
                </msup>
              </mrow>
              <mrow>
                <mn>1</mn>
                <mo>&#x2212;<!-- − --></mo>
                <mi>&#x03D5;<!-- ϕ --></mi>
              </mrow>
            </mfrac>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <mn>0</mn>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mo stretchy="false">&#x27F9;<!-- ⟹ --></mo>
        <mspace width="thickmathspace" />
        <mspace width="thickmathspace" />
      </mtd>
      <mtd>
        <mfrac>
          <mrow>
            <munderover>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>m</mi>
            </munderover>
            <msup>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msup>
          </mrow>
          <mi>&#x03D5;<!-- ϕ --></mi>
        </mfrac>
        <mo>&#x2212;<!-- − --></mo>
        <mfrac>
          <mrow>
            <munderover>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>m</mi>
            </munderover>
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo>&#x2212;<!-- − --></mo>
            <msup>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msup>
            <mo stretchy="false">)</mo>
          </mrow>
          <mrow>
            <mn>1</mn>
            <mo>&#x2212;<!-- − --></mo>
            <mi>&#x03D5;<!-- ϕ --></mi>
          </mrow>
        </mfrac>
        <mo>=</mo>
        <mn>0</mn>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mo stretchy="false">&#x27F9;<!-- ⟹ --></mo>
        <mspace width="thickmathspace" />
        <mspace width="thickmathspace" />
      </mtd>
      <mtd>
        <mi></mi>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <mi>&#x03D5;<!-- ϕ --></mi>
        <mo stretchy="false">)</mo>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <msup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo>&#x2212;<!-- − --></mo>
        <mi>&#x03D5;<!-- ϕ --></mi>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <msup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mn>0</mn>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mo stretchy="false">&#x27F9;<!-- ⟹ --></mo>
        <mspace width="thickmathspace" />
        <mspace width="thickmathspace" />
      </mtd>
      <mtd>
        <mi></mi>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <msup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo>=</mo>
        <mi>&#x03D5;<!-- ϕ --></mi>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo>+</mo>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <msup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mo stretchy="false">&#x27F9;<!-- ⟹ --></mo>
        <mspace width="thickmathspace" />
        <mspace width="thickmathspace" />
      </mtd>
      <mtd>
        <mi></mi>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>m</mi>
        </munderover>
        <msup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo>=</mo>
        <mi>&#x03D5;<!-- ϕ --></mi>
        <mi>m</mi>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mo stretchy="false">&#x27F9;<!-- ⟹ --></mo>
        <mspace width="thickmathspace" />
        <mspace width="thickmathspace" />
      </mtd>
      <mtd>
        <mi>&#x03D5;<!-- ϕ --></mi>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <munderover>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>m</mi>
            </munderover>
            <msup>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msup>
          </mrow>
          <mi>m</mi>
        </mfrac>
        <mo>.</mo>
      </mtd>
    </mtr>
  </mtable>
</math>
<p>And there we have it, the surprisingly long proof of the fact that if we want to pick \(ϕ\) to maximize the likelihood of the observed data, we need to choose it to be equal to the empirical proportion of the ones. Of course, the objections we had at the beginning of this section were also valid: and in fact this perhaps is not the best estimate of \(ϕ\) if we have very little data, or some prior information about what values \(ϕ\) should take. But it is the estimate of \(ϕ\) that maximizes the probability of the observed data, and if this is a bad estimate then it reflects more on the underlying problem with this procedure than with the proof above. Nonetheless, in the presence of a lot of data, there is actually good reason to use the maximum likelihood estimator, and it is extremely common to use in practice.</p>]]></content><author><name></name></author><category term="update" /><summary type="html"><![CDATA[Suppose that we have access to some data set as follow: \(x^{(1)},x^{(2)},x^{(3)},\cdots x^{(m)}\). Our main intresting question is: can we find a random variable \(X\) and distribution \(P\) such that \(p(X)\) can well-modeled our data by this distribution?. In general, this is a hard problem. If we consider the class of all possible distributions, there is no way to best fit to the data.]]></summary></entry><entry><title type="html">Why Using Sigmoid in NN</title><link href="http://localhost:4000/update/2023/01/31/why-we-use-sigmoid-function-in-NN.html" rel="alternate" type="text/html" title="Why Using Sigmoid in NN" /><published>2023-01-31T05:31:29-05:00</published><updated>2023-01-31T05:31:29-05:00</updated><id>http://localhost:4000/update/2023/01/31/why-we%20use-sigmoid-function-in-NN</id><content type="html" xml:base="http://localhost:4000/update/2023/01/31/why-we-use-sigmoid-function-in-NN.html"><![CDATA[<!--- 

<style>
r { color: Red }
o { color: Orange }
g { color: Green }
</style>

# TODOs:

- <r>TODO:</r> Important thing to do
- <o>TODO:</o> Less important thing to do
- <g>DONE:</g> Breath deeply and improve karma
- 
<span style="color:blue">some *This is Blue italic.* text</span>
This is an HTML comment in Markdown 
$\color{red}{your-text-here}$
-->

<h2 id="model"><strong>Model</strong></h2>

<p>Given a <strong>classification problem</strong>, one of the more straightforward models is the <strong>logistic regression</strong>. But, instead of simply <em>presenting</em> it and using it right away, I am going to <strong>build up to it</strong>. The rationale behind this approach is twofold: First, it will make clear why this algorithm is called logistic <em>regression</em> if it is used for classification; second, you’ll get a <strong>clear understanding of what a <em>logit</em> is</strong>.</p>

<p>Well, since it is called logistic <strong>regression</strong>, I would say that <strong>linear regression</strong> is a good starting point. What would a linear regression model with two features look like?</p>

\[\Huge y=b+w_1x_1+w_2x_2+ϵ\]

<p><em>A linear regression model with two features</em></p>

<p>There is one obvious <strong>problem</strong> with the model above: Our <strong>labels (<em>y</em>)</strong> are <strong>discrete</strong>; that is, they are either <strong>zero</strong> or <strong>one</strong>; no other value is allowed. We need to <strong>change the model slightly</strong> to adapt it to our purposes.</p>

<p><em>What if we assign the <strong>positive</strong> outputs to <strong>one</strong> and the <strong>negative</strong></em> <em>outputs to <strong>zero</strong>?</em></p>

<p>Makes sense, right? We’re already calling them <strong>positive</strong> and <strong>negative</strong> classes anyway; why not put their names to good use? Our model would look like this:</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.004.jpeg" alt="" /></p>

<h2 id="logits"><strong>Logits</strong></h2>

<p>\(\color{red}{\text{Equation above  Mapping a linear regression model to discrete labels.}}\)
To make our lives easier, let’s give the right-hand side of the equation above a name: <strong>logit (<em>z</em>)</strong>.</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.005.jpeg" alt="" /></p>

<h2 id="computing-logits"><em>Computing</em> <strong>logits</strong></h2>

<p>The equation above is strikingly similar to the original <strong>linear regression model</strong>, but we’re calling the resulting value <strong><em>z</em></strong>, or <strong>logit</strong>, instead of <strong><em>y</em></strong>, or <strong>label</strong>.</p>

<p><strong>Does it mean a **logit</strong> is the same as <strong>linear regression</strong>?**</p>

<p>Not quite—there is one <strong>fundamental difference</strong> between them: There is <strong>no error term (<em>epsilon</em>)</strong> in Equation above.
If there is no error term, where does the <strong>uncertainty</strong> come from? I am glad you asked :smiley: That’s the role of the <strong>probability</strong>: Instead of assigning a data point to a <strong>discrete label (zero or one)</strong>, we’ll compute the <strong>probability of a data point’s belonging to the positive class</strong>.</p>

<h2 id="probabilities"><strong>Probabilities</strong></h2>

<p>If a data point has a <strong>logit</strong> that equals <strong>zero</strong>, it is exactly at the decision boundary since it is neither positive nor negative. For the sake of completeness, we assigned it to the <strong>positive class</strong>, but this assignment has <strong>maximum uncertainty</strong>, right? So, the corresponding <strong>probability needs to be 0.5</strong> (50%), since it could go either way.</p>

<p>Following this reasoning, we would like to have <strong>large <em>positive</em> logit values</strong> assigned to <strong><em>higher</em> probabilities</strong> (of being in the positive class) and <strong>large <em>negative</em> logit values</strong> assigned to <strong><em>lower probabilities</em></strong> (of being in the positive class).</p>

<p>For <em>really large</em> positive and negative <strong>logit values (<em>z</em>)</strong>, we would like to have:</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.007.jpeg" alt="" /></p>

<h2 id="probabilities-assigned-to-different-logit-values-z"><em>Probabilities assigned to different logit values (z)</em></h2>

<p>We still need to figure out a <strong>function</strong> that maps <strong>logit values</strong> into <strong>probabilities</strong>. We’ll get there soon enough, but first, we need to talk about…</p>

<p><strong>Odds Ratio</strong></p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.006.png" alt="" /><em>What are the odds?!</em></p>

<p>This is a colloquial expression meaning something very unlikely has happened. But <strong>odds</strong> do not have to refer to an unlikely event or a slim chance. The odds of getting <strong>heads</strong> in a (fair) coin flip are 1 to 1 since there is a 50% chance of success and a 50% chance of failure.</p>

<p>Let’s imagine we are betting on the winner of the World Cup final. There are two countries: <strong>A</strong> and <strong>B</strong>. Country <strong>A</strong> is the <strong>favorite</strong>: It has a 75% chance of winning. So, Country <strong>B</strong> has only a 25% chance of winning. If you bet on Country <strong>A</strong>, your chances of winning—that is, your <strong>odds (in favor)</strong>—are <strong>3 to 1</strong> (75 to 25). If you decide to test your luck and bet on Country <strong>B</strong>, your chances of winning—that is, your <strong>odds (in favor)</strong>—are <strong>1 to 3</strong> (25 to 75), or <strong>0.33 to 1</strong>.</p>

<p>The <strong>odds ratio</strong> is given by the <strong>ratio</strong> between the <strong>probability of success</strong> (<em>p</em>) and the</p>

<p><strong>probability of failure</strong> (<em>q</em>):</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.008.jpeg" alt="" /></p>

<h2 id="odds-ratio"><em>Odds ratio</em></h2>

<p>In code, our odds_ratio() function looks like this:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">odds_ratio</span><span class="p">(</span><span class="n">prob</span><span class="p">):</span>

<span class="k">return</span> <span class="n">prob</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">prob</span><span class="p">)</span>

<span class="n">p</span> <span class="o">=</span> <span class="p">.</span><span class="mi">75</span>

<span class="n">q</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span>

<span class="n">odds_ratio</span><span class="p">(</span><span class="n">p</span><span class="p">),</span> <span class="n">odds_ratio</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

<span class="o">*</span><span class="n">Output</span><span class="o">*</span>

<span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.3333333333333333</span><span class="p">)</span>
</code></pre></div></div>

<p>We can also <strong>plot</strong> the resulting <strong>odds ratios</strong> for probabilities ranging from 1% to 99%. The <em>red dots</em> correspond to the probabilities of 25% (<em>q</em>), 50%, and 75% (<em>p</em>).</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.009.png" alt="" /></p>

<p><strong>Odds ratio</strong></p>

<p>Clearly, the odds ratios (left plot) are <strong>not symmetrical</strong>. But, in a <strong>log scale</strong> (right plot), <strong>they are</strong>. This serves us very well since we’re looking for a <strong>symmetrical function</strong> that maps <strong>logit values</strong> into <strong>probabilities</strong>.</p>

<p>Why does it <strong>need</strong> to be <strong>symmetrical</strong>?</p>

<p>If the function <strong>weren’t</strong> symmetrical, different choices for the <strong>positive class</strong> would produce models that were <strong>not</strong> equivalent. But, using a symmetrical function, we could train <strong>two equivalent models</strong> using the <strong>same dataset</strong>, just flipping the classes:</p>

<ul>
  <li><strong>Blue Model</strong> (the positive class (<em>y=1</em>) corresponds to <strong>blue</strong> points)
    <ul>
      <li>Data Point #1: <strong>P(<em>y=1</em>) = P(blue) = .83</strong> (which is the same as <strong>P(red) = .17</strong>)</li>
    </ul>
  </li>
  <li><strong>Red Model</strong> (the positive class (<em>y=1</em>) corresponds to <strong>red</strong> points)
    <ul>
      <li>Data Point #1: <strong>P(<em>y=1</em>) = P(red) = .17</strong> (which is the same as <strong>P(blue) = .83</strong>)</li>
    </ul>
  </li>
</ul>

<p>##Log Odds Ratio</p>

<p>By taking the <strong>logarithm</strong> of the <strong>odds ratio</strong>, the function is not only <strong>symmetrical</strong>, but also maps <strong>probabilities</strong> into <strong>real numbers</strong>, instead of only the positive ones:</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.010.jpeg" alt="" /></p>

<p><strong>Log odds ratio</strong></p>

<p>In code, our log_odds_ratio() function looks like this:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_odds_ratio</span><span class="p">(</span><span class="n">prob</span><span class="p">):</span>

<span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">odds</span>\<span class="n">_ratio</span><span class="p">(</span><span class="n">prob</span><span class="p">))</span>

<span class="err">$$</span><span class="n">p</span> <span class="o">=</span> <span class="p">.</span><span class="mi">75</span><span class="err">$$</span>

<span class="err">$$</span><span class="n">q</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="err">$$</span>

<span class="err">$$</span>\<span class="n">log_odds_ratio</span><span class="p">(</span><span class="n">p</span><span class="p">),</span> <span class="n">log_odds_ratio</span><span class="p">(</span><span class="n">q</span><span class="p">)</span><span class="err">$$</span>


<span class="o">*</span><span class="n">Output</span><span class="o">*</span>

<span class="p">(</span><span class="mf">1.0986122886681098</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0986122886681098</span><span class="p">)</span>
</code></pre></div></div>

<p>As expected, <strong>probabilities that add up to 100%</strong> (like 75% and 25%) correspond to</p>

<p><strong>log odds ratios</strong> that are the <strong>same in absolute value</strong>. Let’s plot it:</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.009.png" alt="" /></p>

<h2 id="log-odds-ratio-and-probability">Log odds ratio and probability</h2>

<p>On the left, <strong>each probability maps into a log odds ratio</strong>. The <em>red dots</em> correspond to probabilities of 25%, 50%, and 75%, the same as before.</p>

<p>If we <strong>flip</strong> the horizontal and vertical axes (right plot), we are <strong>inverting the function</strong>, thus mapping <strong>each log odds ratio into a probability</strong>. That’s the function we were looking for!</p>

<p>Does its shape look familiar? Wait for it…</p>

<p><strong>From Logits to Probabilities</strong></p>

<p>In the previous section, we were trying to <strong>map logit values into probabilities</strong>, and we’ve just found out, graphically, a function that <strong>maps log odds ratios into probabilities</strong>.</p>

<p>Clearly, our <strong>logits are log odds ratios</strong> :-) Sure, drawing conclusions like this is not very scientific, but the purpose of this exercise is to illustrate how the results of a regression, represented by the <strong>logits (z)</strong>, get to be mapped into probabilities.</p>

<p>So, here’s what we arrived at:</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.011.jpeg" alt="" /></p>

<p><strong>Equation - Regression, logits, and log odds ratios</strong></p>

<p>Let’s work this equation out a bit, inverting, rearranging, and simplifying some terms to <strong>isolate <em>p</em></strong>:</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.012.jpeg" alt="" /></p>

<p><em>Equation - From logits (z) to probabilities (p)</em></p>

<p>Does it look familiar? That’s a <strong>sigmoid function</strong>! It is the <strong>inverse of the log odds ratio</strong>.</p>

<p><img src="/assets/sigmoid/Aspose.Words.f06965dc-5caa-4654-b599-bd950b862427.013.png" alt="" /></p>

<p><strong>Equation - Sigmoid function</strong></p>]]></content><author><name></name></author><category term="update" /><summary type="html"><![CDATA[Model Given a classification problem, one of the more straightforward models is the logistic regression. But, instead of simply presenting it and using it right away, I am going to build up to it. The rationale behind this approach is twofold: First, it will make clear why this algorithm is called logistic regression if it is used for classification; second, you’ll get a clear understanding of what a logit is. Well, since it is called logistic regression, I would say that linear regression is a good starting point. What would a linear regression model with two features look like? \[\Huge y=b+w_1x_1+w_2x_2+ϵ\] A linear regression model with two features There is one obvious problem with the model above: Our labels (y) are discrete; that is, they are either zero or one; no other value is allowed. We need to change the model slightly to adapt it to our purposes. What if we assign the positive outputs to one and the negative outputs to zero? Makes sense, right? We’re already calling them positive and negative classes anyway; why not put their names to good use? Our model would look like this: Logits \(\color{red}{\text{Equation above Mapping a linear regression model to discrete labels.}}\) To make our lives easier, let’s give the right-hand side of the equation above a name: logit (z). Computing logits The equation above is strikingly similar to the original linear regression model, but we’re calling the resulting value z, or logit, instead of y, or label. Does it mean a **logit is the same as linear regression?** Not quite—there is one fundamental difference between them: There is no error term (epsilon) in Equation above. If there is no error term, where does the uncertainty come from? I am glad you asked :smiley: That’s the role of the probability: Instead of assigning a data point to a discrete label (zero or one), we’ll compute the probability of a data point’s belonging to the positive class. Probabilities If a data point has a logit that equals zero, it is exactly at the decision boundary since it is neither positive nor negative. For the sake of completeness, we assigned it to the positive class, but this assignment has maximum uncertainty, right? So, the corresponding probability needs to be 0.5 (50%), since it could go either way. Following this reasoning, we would like to have large positive logit values assigned to higher probabilities (of being in the positive class) and large negative logit values assigned to lower probabilities (of being in the positive class). For really large positive and negative logit values (z), we would like to have: Probabilities assigned to different logit values (z) We still need to figure out a function that maps logit values into probabilities. We’ll get there soon enough, but first, we need to talk about… Odds Ratio What are the odds?! This is a colloquial expression meaning something very unlikely has happened. But odds do not have to refer to an unlikely event or a slim chance. The odds of getting heads in a (fair) coin flip are 1 to 1 since there is a 50% chance of success and a 50% chance of failure. Let’s imagine we are betting on the winner of the World Cup final. There are two countries: A and B. Country A is the favorite: It has a 75% chance of winning. So, Country B has only a 25% chance of winning. If you bet on Country A, your chances of winning—that is, your odds (in favor)—are 3 to 1 (75 to 25). If you decide to test your luck and bet on Country B, your chances of winning—that is, your odds (in favor)—are 1 to 3 (25 to 75), or 0.33 to 1. The odds ratio is given by the ratio between the probability of success (p) and the probability of failure (q): Odds ratio In code, our odds_ratio() function looks like this: def odds_ratio(prob): return prob / (1 - prob) p = .75 q = 1 - p odds_ratio(p), odds_ratio(q) *Output* (3.0, 0.3333333333333333) We can also plot the resulting odds ratios for probabilities ranging from 1% to 99%. The red dots correspond to the probabilities of 25% (q), 50%, and 75% (p). Odds ratio Clearly, the odds ratios (left plot) are not symmetrical. But, in a log scale (right plot), they are. This serves us very well since we’re looking for a symmetrical function that maps logit values into probabilities. Why does it need to be symmetrical? If the function weren’t symmetrical, different choices for the positive class would produce models that were not equivalent. But, using a symmetrical function, we could train two equivalent models using the same dataset, just flipping the classes: Blue Model (the positive class (y=1) corresponds to blue points) Data Point #1: P(y=1) = P(blue) = .83 (which is the same as P(red) = .17) Red Model (the positive class (y=1) corresponds to red points) Data Point #1: P(y=1) = P(red) = .17 (which is the same as P(blue) = .83) ##Log Odds Ratio By taking the logarithm of the odds ratio, the function is not only symmetrical, but also maps probabilities into real numbers, instead of only the positive ones: Log odds ratio In code, our log_odds_ratio() function looks like this: def log_odds_ratio(prob): return np.log(odds\_ratio(prob)) $$p = .75$$ $$q = 1 - p$$ $$\log_odds_ratio(p), log_odds_ratio(q)$$ *Output* (1.0986122886681098, -1.0986122886681098) As expected, probabilities that add up to 100% (like 75% and 25%) correspond to log odds ratios that are the same in absolute value. Let’s plot it: Log odds ratio and probability On the left, each probability maps into a log odds ratio. The red dots correspond to probabilities of 25%, 50%, and 75%, the same as before. If we flip the horizontal and vertical axes (right plot), we are inverting the function, thus mapping each log odds ratio into a probability. That’s the function we were looking for! Does its shape look familiar? Wait for it… From Logits to Probabilities In the previous section, we were trying to map logit values into probabilities, and we’ve just found out, graphically, a function that maps log odds ratios into probabilities. Clearly, our logits are log odds ratios :-) Sure, drawing conclusions like this is not very scientific, but the purpose of this exercise is to illustrate how the results of a regression, represented by the logits (z), get to be mapped into probabilities. So, here’s what we arrived at: Equation - Regression, logits, and log odds ratios Let’s work this equation out a bit, inverting, rearranging, and simplifying some terms to isolate p: Equation - From logits (z) to probabilities (p) Does it look familiar? That’s a sigmoid function! It is the inverse of the log odds ratio. Equation - Sigmoid function]]></summary></entry><entry><title type="html">Autograd Encodes Execution</title><link href="http://localhost:4000/update/2023/01/31/Autograd-Encodes.html" rel="alternate" type="text/html" title="Autograd Encodes Execution" /><published>2023-01-31T05:31:29-05:00</published><updated>2023-01-31T05:31:29-05:00</updated><id>http://localhost:4000/update/2023/01/31/Autograd-Encodes</id><content type="html" xml:base="http://localhost:4000/update/2023/01/31/Autograd-Encodes.html"><![CDATA[<h2 id="how-autograd-encodes-execution-work">How autograd encodes execution work?</h2>
<iframe src="/assets/PyTorch.html" onload="javascript:(function(o){o.style.height=o.contentWindow.document.body.scrollHeight+&quot;px&quot;;}(this));" style="height:500px;width:100%;border:none;overflow:hidden;">
 </iframe>]]></content><author><name></name></author><category term="update" /><summary type="html"><![CDATA[How autograd encodes execution work?]]></summary></entry><entry><title type="html">Plotly</title><link href="http://localhost:4000/update/2023/01/29/plotly.html" rel="alternate" type="text/html" title="Plotly" /><published>2023-01-29T18:20:23-05:00</published><updated>2023-01-29T18:20:23-05:00</updated><id>http://localhost:4000/update/2023/01/29/plotly</id><content type="html" xml:base="http://localhost:4000/update/2023/01/29/plotly.html"><![CDATA[<h1 id="intractive-plot-with-plotly">Intractive plot with plotly</h1>

<p>Here is an example of an animated scatter plot creating using Plotly Express. Note that you should always fix the x_range and y_range to ensure that your data remains visible throughout the animation.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">gapminder</span><span class="p">()</span>
<span class="n">fig</span><span class="o">=</span><span class="n">px</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"gdpPercap"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"lifeExp"</span><span class="p">,</span> <span class="n">animation_frame</span><span class="o">=</span><span class="s">"year"</span><span class="p">,</span> <span class="n">animation_group</span><span class="o">=</span><span class="s">"country"</span><span class="p">,</span>
           <span class="n">size</span><span class="o">=</span><span class="s">"pop"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"continent"</span><span class="p">,</span> <span class="n">hover_name</span><span class="o">=</span><span class="s">"country"</span><span class="p">,</span>
           <span class="n">log_x</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">size_max</span><span class="o">=</span><span class="mi">55</span><span class="p">,</span> <span class="n">range_x</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">100000</span><span class="p">],</span> <span class="n">range_y</span><span class="o">=</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span><span class="mi">90</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">write_html</span><span class="p">(</span><span class="s">"plotly.html"</span><span class="p">)</span>
</code></pre></div></div>

<iframe src="/assets/plotly.html" onload="javascript:(function(o){o.style.height=o.contentWindow.document.body.scrollHeight+&quot;px&quot;;}(this));" style="height:500px;width:100%;border:none;overflow:hidden;">
 </iframe>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">gapminder</span><span class="p">()</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="n">bar</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"continent"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"pop"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"continent"</span><span class="p">,</span>
  <span class="n">animation_frame</span><span class="o">=</span><span class="s">"year"</span><span class="p">,</span> <span class="n">animation_group</span><span class="o">=</span><span class="s">"country"</span><span class="p">,</span> <span class="n">range_y</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">4000000000</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">write_html</span><span class="p">(</span><span class="s">"plotly2.html"</span><span class="p">)</span>
</code></pre></div></div>

<iframe src="/assets/plotly2.html" onload="javascript:(function(o){o.style.height=o.contentWindow.document.body.scrollHeight+&quot;px&quot;;}(this));" style="height:500px;width:100%;border:none;overflow:hidden;">
 </iframe>

<h2 id="moving-point-on-a-curve">Moving Point on a Curve</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="n">go</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Generate curve data
</span><span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">t</span> <span class="o">+</span> <span class="n">t</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">t</span> <span class="o">-</span> <span class="n">t</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">xm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.5</span>
<span class="n">xM</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.5</span>
<span class="n">ym</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.5</span>
<span class="n">yM</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.5</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">s</span> <span class="o">+</span> <span class="n">s</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">yy</span> <span class="o">=</span> <span class="n">s</span> <span class="o">-</span> <span class="n">s</span> <span class="o">**</span> <span class="mi">2</span>


<span class="c1"># Create figure
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="p">.</span><span class="n">Figure</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">go</span><span class="p">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                     <span class="n">mode</span><span class="o">=</span><span class="s">"lines"</span><span class="p">,</span>
                     <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"blue"</span><span class="p">)),</span>
          <span class="n">go</span><span class="p">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                     <span class="n">mode</span><span class="o">=</span><span class="s">"lines"</span><span class="p">,</span>
                     <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"blue"</span><span class="p">))],</span>
    <span class="n">layout</span><span class="o">=</span><span class="n">go</span><span class="p">.</span><span class="n">Layout</span><span class="p">(</span>
        <span class="n">xaxis</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="nb">range</span><span class="o">=</span><span class="p">[</span><span class="n">xm</span><span class="p">,</span> <span class="n">xM</span><span class="p">],</span> <span class="n">autorange</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">zeroline</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
        <span class="n">yaxis</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="nb">range</span><span class="o">=</span><span class="p">[</span><span class="n">ym</span><span class="p">,</span> <span class="n">yM</span><span class="p">],</span> <span class="n">autorange</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">zeroline</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
        <span class="n">title_text</span><span class="o">=</span><span class="s">"Kinematic Generation of a Planar Curve"</span><span class="p">,</span> <span class="n">hovermode</span><span class="o">=</span><span class="s">"closest"</span><span class="p">,</span>
        <span class="n">updatemenus</span><span class="o">=</span><span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s">"buttons"</span><span class="p">,</span>
                          <span class="n">buttons</span><span class="o">=</span><span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s">"Play"</span><span class="p">,</span>
                                        <span class="n">method</span><span class="o">=</span><span class="s">"animate"</span><span class="p">,</span>
                                        <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">])])]),</span>
    <span class="n">frames</span><span class="o">=</span><span class="p">[</span><span class="n">go</span><span class="p">.</span><span class="n">Frame</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">go</span><span class="p">.</span><span class="n">Scatter</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">xx</span><span class="p">[</span><span class="n">k</span><span class="p">]],</span>
            <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">yy</span><span class="p">[</span><span class="n">k</span><span class="p">]],</span>
            <span class="n">mode</span><span class="o">=</span><span class="s">"markers"</span><span class="p">,</span>
            <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s">"red"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">))])</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span>
<span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">write_html</span><span class="p">(</span><span class="s">"plotly3.html"</span><span class="p">)</span>
</code></pre></div></div>

<iframe src="/assets/plotly3.html" onload="javascript:(function(o){o.style.height=o.contentWindow.document.body.scrollHeight+&quot;px&quot;;}(this));" style="height:500px;width:100%;border:none;overflow:hidden;">
 </iframe>

<h1 id="moving-frenet-frame-along-a-planar-curve">Moving Frenet Frame Along a Planar Curve</h1>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="n">go</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>


<span class="c1"># Generate curve data
</span><span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">t</span> <span class="o">+</span> <span class="n">t</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">t</span> <span class="o">-</span> <span class="n">t</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">xm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.5</span>
<span class="n">xM</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.5</span>
<span class="n">ym</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.5</span>
<span class="n">yM</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.5</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">s</span> <span class="o">+</span> <span class="n">s</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">yy</span> <span class="o">=</span> <span class="n">s</span> <span class="o">-</span> <span class="n">s</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">vx</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">s</span>
<span class="n">vy</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">s</span>  <span class="c1"># v=(vx, vy) is the velocity
</span><span class="n">speed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">vx</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">vy</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ux</span> <span class="o">=</span> <span class="n">vx</span> <span class="o">/</span> <span class="n">speed</span>  <span class="c1"># (ux, uy) unit tangent vector, (-uy, ux) unit normal vector
</span><span class="n">uy</span> <span class="o">=</span> <span class="n">vy</span> <span class="o">/</span> <span class="n">speed</span>

<span class="n">xend</span> <span class="o">=</span> <span class="n">xx</span> <span class="o">+</span> <span class="n">ux</span>  <span class="c1"># end coordinates for the unit tangent vector at (xx, yy)
</span><span class="n">yend</span> <span class="o">=</span> <span class="n">yy</span> <span class="o">+</span> <span class="n">uy</span>

<span class="n">xnoe</span> <span class="o">=</span> <span class="n">xx</span> <span class="o">-</span> <span class="n">uy</span>  <span class="c1"># end coordinates for the unit normal vector at (xx,yy)
</span><span class="n">ynoe</span> <span class="o">=</span> <span class="n">yy</span> <span class="o">+</span> <span class="n">ux</span>


<span class="c1"># Create figure
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="p">.</span><span class="n">Figure</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">go</span><span class="p">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                     <span class="n">name</span><span class="o">=</span><span class="s">"frame"</span><span class="p">,</span>
                     <span class="n">mode</span><span class="o">=</span><span class="s">"lines"</span><span class="p">,</span>
                     <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"blue"</span><span class="p">)),</span>
          <span class="n">go</span><span class="p">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                     <span class="n">name</span><span class="o">=</span><span class="s">"curve"</span><span class="p">,</span>
                     <span class="n">mode</span><span class="o">=</span><span class="s">"lines"</span><span class="p">,</span>
                     <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"blue"</span><span class="p">))</span>
          <span class="p">],</span>
    <span class="n">layout</span><span class="o">=</span><span class="n">go</span><span class="p">.</span><span class="n">Layout</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span>
                     <span class="n">xaxis</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="nb">range</span><span class="o">=</span><span class="p">[</span><span class="n">xm</span><span class="p">,</span> <span class="n">xM</span><span class="p">],</span> <span class="n">autorange</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">zeroline</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
                     <span class="n">yaxis</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="nb">range</span><span class="o">=</span><span class="p">[</span><span class="n">ym</span><span class="p">,</span> <span class="n">yM</span><span class="p">],</span> <span class="n">autorange</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">zeroline</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
                     <span class="n">title</span><span class="o">=</span><span class="s">"Moving Frenet Frame Along a Planar Curve"</span><span class="p">,</span>
                     <span class="n">hovermode</span><span class="o">=</span><span class="s">"closest"</span><span class="p">,</span>
                     <span class="n">updatemenus</span><span class="o">=</span><span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s">"buttons"</span><span class="p">,</span>
                                       <span class="n">buttons</span><span class="o">=</span><span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s">"Play"</span><span class="p">,</span>
                                                     <span class="n">method</span><span class="o">=</span><span class="s">"animate"</span><span class="p">,</span>
                                                     <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">])])]),</span>

    <span class="n">frames</span><span class="o">=</span><span class="p">[</span><span class="n">go</span><span class="p">.</span><span class="n">Frame</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">go</span><span class="p">.</span><span class="n">Scatter</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">xx</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">xend</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="bp">None</span><span class="p">,</span> <span class="n">xx</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">xnoe</span><span class="p">[</span><span class="n">k</span><span class="p">]],</span>
            <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">yy</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">yend</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="bp">None</span><span class="p">,</span> <span class="n">yy</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">ynoe</span><span class="p">[</span><span class="n">k</span><span class="p">]],</span>
            <span class="n">mode</span><span class="o">=</span><span class="s">"lines"</span><span class="p">,</span>
            <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s">"red"</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
        <span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span>
<span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">write_html</span><span class="p">(</span><span class="s">"plotly4.html"</span><span class="p">)</span>
</code></pre></div></div>
<iframe src="/assets/plotly4.html" onload="javascript:(function(o){o.style.height=o.contentWindow.document.body.scrollHeight+&quot;px&quot;;}(this));" style="height:500px;width:100%;border:none;overflow:hidden;">
 </iframe>]]></content><author><name></name></author><category term="update" /><summary type="html"><![CDATA[Intractive plot with plotly]]></summary></entry><entry><title type="html">Deep Learning</title><link href="http://localhost:4000/update/2023/01/27/Deep-Learning.html" rel="alternate" type="text/html" title="Deep Learning" /><published>2023-01-27T18:20:23-05:00</published><updated>2023-01-27T18:20:23-05:00</updated><id>http://localhost:4000/update/2023/01/27/Deep-Learning</id><content type="html" xml:base="http://localhost:4000/update/2023/01/27/Deep-Learning.html"><![CDATA[<h1 id="deep-learning-with-pytorch-step-by-step">Deep Learning with PyTorch Step-by-Step</h1>

<h1 id="optimizer-for-nn">Optimizer for NN:</h1>

<p>What if we had a whole lot of parameters using the computed gradients?
We need to use one of PyTorch’s optimizers, like SGD, RMSprop, or Adam.
There are many optimizers: SGD is the most basic of them, and Adam is one of the most popular.
Different optimizers use different mechanics for updating the parameters, but they all achieve 
the same goal through, literally, different paths.The following animation shows a loss surface,the paths traversed by some optimizers to achieve the minimum (represented by a star).</p>

<p>Remember, the choice of mini-batch size influences the path of gradient descent, and so does the choice of an optimizer.</p>

<p align="left">
<img src="/assets/figures/opt2.gif" />
</p>
<p>Most modern neural networks are trained using maximum likelihood. This means that the cost function is simply the negative log-likelihood, equivalently described as the cross-entropy between the training data and the model distribution. This cost function is given by
 \(J(θ) = \log p_{model}(y | x)\)
The specific form of the cost function changes from model to model, depending on the specific form of \(\log p_{model}\) The expansion of the above equation typically yields some terms that do not depend on the model parameters and may be discarded. 
For example, if \(p_{model}(y | x) = N(y; f(x; θ), I)\),then we recover the mean squared error cost,
\(J(θ) = 1\)</p>

<p>In this blog, we will:</p>

<p>• define a simple linear regression model</p>

<p>• walk through every step of gradient descent: initializing parameters,
performing a forward pass, computing errors and loss, computing gradients, and updating parameters</p>

<p>• understand gradients using equations, code, and geometry</p>

<p>• understand the difference between batch, mini-batch, and stochastic gradient descent</p>

<p>• visualize the effects on the loss of using different learning rates</p>

<p>• understand the importance of standardizing / scaling features</p>

<iframe src="/assets/Chapter00.html" onload="javascript:(function(o){o.style.height=o.contentWindow.document.body.scrollHeight+&quot;px&quot;;}(this));" style="height:100px;width:90%;border:none;overflow:hidden;">
 </iframe>]]></content><author><name></name></author><category term="update" /><summary type="html"><![CDATA[Deep Learning with PyTorch Step-by-Step]]></summary></entry><entry><title type="html">Welcome to Jekyll!</title><link href="http://localhost:4000/jekyll/2023/01/26/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2023-01-26T05:31:29-05:00</published><updated>2023-01-26T05:31:29-05:00</updated><id>http://localhost:4000/jekyll/2023/01/26/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/jekyll/2023/01/26/welcome-to-jekyll.html"><![CDATA[<h2 id="how-to-use-mathjax">How to use MathJax</h2>

<ul>
  <li>
    <p>In the <code class="highlighter-rouge">_config.yml</code>, add the following lines:</p>

    <p>markdown: kramdown</p>
  </li>
  <li>
    <p>On the top of the posts that you’d like to add mathjax support, add the following line:</p>

    <p>usemathjax: true</p>
  </li>
  <li>
    <p>Add these lines to <code class="highlighter-rouge">_includes/head.html</code>:
<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script></p>
  </li>
</ul>

<p>You’ll find this post in your <code class="highlighter-rouge">_posts</code> directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run <code class="highlighter-rouge">jekyll serve</code>, which launches a web server and auto-regenerates your site when a file is updated.</p>

<h2 id="some-information-about-jekyll">Some information about Jekyll</h2>

<p>Jekyll requires blog post files to be named according to the following format:</p>

<p><code class="highlighter-rouge">YEAR-MONTH-DAY-title.MARKUP</code></p>

<p>Where <code class="highlighter-rouge">YEAR</code> is a four-digit number, <code class="highlighter-rouge">MONTH</code> and <code class="highlighter-rouge">DAY</code> are both two-digit numbers, and <code class="highlighter-rouge">MARKUP</code> is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.</p>

<p>Jekyll also offers powerful support for code snippets:</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">print_hi</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="nb">puts</span> <span class="s2">"Hi, </span><span class="si">#{</span><span class="nb">name</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>
<span class="n">print_hi</span><span class="p">(</span><span class="s1">'Tom'</span><span class="p">)</span>
<span class="c1"># =&gt; prints 'Hi, Tom' to STDOUT.</span></code></pre></figure>

<p>Check out the <a href="https://jekyllrb.com/docs/home">Jekyll docs</a> for more info on how to get the most out of Jekyll. File all bugs/feature requests at <a href="https://github.com/jekyll/jekyll">Jekyll’s GitHub repo</a>. If you have questions, you can ask them on <a href="https://talk.jekyllrb.com/">Jekyll Talk</a>.</p>

<p>this is it!</p>]]></content><author><name></name></author><category term="Jekyll" /><summary type="html"><![CDATA[How to use MathJax]]></summary></entry><entry><title type="html">Milestone2</title><link href="http://localhost:4000/update/2022/11/19/Milestone2.html" rel="alternate" type="text/html" title="Milestone2" /><published>2022-11-19T05:31:29-05:00</published><updated>2022-11-19T05:31:29-05:00</updated><id>http://localhost:4000/update/2022/11/19/Milestone2</id><content type="html" xml:base="http://localhost:4000/update/2022/11/19/Milestone2.html"><![CDATA[<h3 id="an-api-for-the-national-hockey-league-nhl">An API for The National Hockey League (NHL)</h3>

<h1 id="table-of-contents">Table of contents</h1>
<ol>
  <li><a href="#Experiment-Tracking">Experiment Tracking</a></li>
  <li><a href="#Feature-Engineering-I">Feature Engineering I</a></li>
  <li><a href="#Baseline-Models">Baseline Models</a></li>
  <li><a href="#Feature-Engineering-II">Feature Engineering II</a></li>
  <li><a href="#Advanced-Models">Advanced Models</a></li>
  <li><a href="#Give-it-your-best-shot">Give Best Shot</a></li>
  <li><a href="#Evaluate-on-test-set">Evaluate on Test Set</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ol>

<h2 id="experiment-tracking-">Experiment Tracking <a name="Experiment-Tracking"></a></h2>

<p>All the following experiments were carefully tracked using Comet ML. This is an important step that we took to make our experiments reproducible.</p>

<h2 id="feature-engineering-i-">Feature Engineering I <a name="Feature-Engineering-I"></a></h2>

<p align="center">
<img src="/assets/figures/figure_1_Shot_Counts_by_Distance.png" />
</p>

<p align="center">
<img src="/assets/figures/figure_2_Shot_Counts_by_Angle.png" />
</p>
<p align="center">
<img src="/assets/figures/figure_3_jointplot_Distance_Angle.png" width="600" height="600" />
</p>

<p align="center">
<img src="/assets/figures/figure_4_Goal_Rate_as_a_function_of_Distance.png" />
</p>

<h2 id="goal-rate-as-a-function-of-distance">Goal rate as a function of distance</h2>

<p>First, there is a relationship between the distance and the probability of a goal, such as the probability of a goal is inversely proportional to distance when the distance is less than approximately 60 ft.</p>

<p>Second, there is some (stochastic) probability of a goal with shots taken at approximately more than 60 feet which can be in some cases higher than for goals taken at about 60 feet.</p>
<p align="center">
<img src="/assets/figures/figure_5_Goal_Rate_as_a_function_of_Angle.png" />
</p>

<h2 id="goal-rate-as-a-function-of-angle">Goal Rate as a function of Angle</h2>

<p>First, there is slightly more chance that a goal results in a goal when it is taken from in front of the goal, but only when the shot is taken at less than 90 degrees.</p>

<p>Second, even though shots taken from in front of the goal (i.e. at an angle close to zero) are the most common, these are not the most successful in achieving a goal. The likelihood of a goal increases as a proportion to the angle, specifically when the angle is greater than approximately 90 degrees.</p>

<p>Third and very importantly, the distribution is symmetric only in its middle part. There is a very clear asymmetry when comparing the extremities of the distribution. Specifically, shots taken at very high angles (approximately more than 150 degrees) are more likely to achieve a goal when taken from the side which corresponds to the right-hand side of the goalie. We need to bear in mind that most goalies are right-handed and hold their hockey sticks on the left side as all other players, and therefore their right side is relatively unprotected. This might explain why shots taken from the far-right-hand side of the goalie are much more likely to result in a goal.</p>

<p align="center">
<img src="/assets/figures/figure_6_Empty_net_or_not_as_a_function_of_Distance.png" />
</p>
<p>The distribution is coherent with domain-specific knowledge (i.e. that “it is incredibly rare to score a non-empty net goal on the opposing team from within your defensive zone”). Indeed, a few successful goals (in total, 795 goals for the complete seasons involved) were shot from more than 89 feet. Based on this, it seems that the features are correct, and we cannot find any events that have incorrect features.</p>

<p>Note: Here, we indicate the approximate defense zone by a red shade, from 89 feet from the goal up to 180 feet. This is an approximation because the distance is measured from the goal and thus is not always calculated perpendicular to the central line.</p>

<h2 id="baseline-models-">Baseline Models <a name="Baseline-Models"></a></h2>
<p>Links to Various Logistic Models on Comet ML platform.</p>
<ul>
  <li><a href="https://www.comet.com/data-science-workspace/model-registry/linearmodel-angle">Logistic	model trained on angle</a></li>
  <li><a href="https://www.comet.com/data-science-workspace/model-registry/linearmodel-distance">logistic model trained on distance</a></li>
  <li><a href="https://www.comet.com/data-science-workspace/model-registry/linearmodel-angle-distance">logistic model trained on distance and angle</a></li>
</ul>

<p>The accuracy is 90% correctly predicted as compared total. However, accuracy is not the right metric to use in this setting because there is a high imbalance between goals and missed shots, in the order of 1:10. Indeed, we can see that from the classification report its clear that all the test data (validation data) has been predicted to be 0s (missed shots) as the output. Therefore, even though this model achieved 90% accuracy, it was not successful in predicting any goal. This is why accuracy is not the correct metric, and other metrics like ROC AUC should be used.</p>
<p align="center">
<img src="/assets/figures/Receiver_Operating_Characteristic_ROC_curves_and_the_AUC_metric.svg" />
</p>
<p>The model based on both distance and angle (<code class="highlighter-rouge">distance_angle</code>) and that based on distance alone (<code class="highlighter-rouge">distance</code>) are the two best models when considering the ROC AUC. Indeed, their area under the curve is the largest one (0.67964). In comparison, the two other models (<code class="highlighter-rouge">random_base_line</code> and the <code class="highlighter-rouge">angle</code> ones) are not distinguishable from random results, i.e. their ROC curve follows closely the bisector line.</p>
<p align="center">
<img src="/assets/figures/Goal_Rate.svg" />
</p>
<p>As expected, the highest percentiles observations (the observations that have the highest predicted probability of being a goal) have the highest proportion of goals; however only for the <code class="highlighter-rouge">distance_angle</code> and <code class="highlighter-rouge">distance_models</code>. More specifically, the proportion of goals raises to approximately 20% for the highest-percentile observations, while it gets close to 5% for the lowest-percentile observations. For the other two models, the proportion of goals is the same in all percentiles and is close to 10% which is the overall proportion of goals in any random sample from the dataset.</p>

<p align="center">
<img src="/assets/figures/Cumulative_of_goals.svg" />
</p>
<p>There is a non-linear relationship between the model probability percentile and the cumulative proportion of goals, but only for the <code class="highlighter-rouge">distance</code> and <code class="highlighter-rouge">distance_angle</code> curves. Specifically, for these two curves, the more the model probability percentile increases, the more the proportion increases, but the slope decreases. In comparison, the two other curves, show a close-to-linear relationship. 
This difference in slope between models reflects the same finding as the previous graph, i.e. the highest percentiles observations have the highest proportion of goals only for the <code class="highlighter-rouge">distance_angle</code> and <code class="highlighter-rouge">distance_models</code>. In other words, these two models were successful in learning to predict goals.</p>
<p align="center">
<img src="/assets/figures/Calibration_Display.svg" />
</p>
<p>From the Calibration plot, we see that fraction of positive cases is always the same whatever the mean predicted probability is, for every four models tested. In all cases, the fraction of positive cases is equal to 1:10 which is the (approximate) overall ratio of goals as compared to all shots in the complete sample.</p>

<p>This could induce us to think that all four models performed the same and that none of these models succeeded in predicting goals from the data. However, we need to take note that the calibration plot - like the accuracy metric - is not the most adequate way to evaluate a model when the predicted event is rare (in our case, 1:10). From the previous plots, we showed that the <code class="highlighter-rouge">distance_angle</code> and <code class="highlighter-rouge">distance</code> models show some success in predicting goals, which is not apparent from the calibration plot. This observation highlights the importance of choosing the correct metric and correct plots when evaluating a model.</p>

<h2 id="feature-engineering-ii-">Feature Engineering II <a name="Feature-Engineering-II"></a></h2>
<p>Here is a list and description of all features that we engineered:</p>

<p>•	<code class="highlighter-rouge">distance</code>: distance (feet) between the event and the center of the net at which the shot is aimed.</p>

<p>•	<code class="highlighter-rouge">angle</code>: angle (degrees) between the center line and the event. One can imagine this angle by putting himself in the position of the goalie: if the shot is taken from right in front, the angle is equal to zero.</p>

<p>•	<code class="highlighter-rouge">empty_net</code>: shot was taken against a net without a goalie.</p>

<p>•	<code class="highlighter-rouge">game_period</code>: period number during the game.</p>

<p>•	<code class="highlighter-rouge">distance_from_last_event</code>: distance (feet) between the last event and the present event.</p>

<p>•	<code class="highlighter-rouge">rebound</code>: whether this shot follows another shot.</p>

<p>•<code class="highlighter-rouge">change_in_shot_angle</code>: difference (degrees) in angle between the present shot and the previous event, if this event was also a shot.</p>

<p>•	<code class="highlighter-rouge">speed</code>: average speed (feet by second) of the puck between the last and the present shot.</p>

<p>•<code class="highlighter-rouge">x_coordinate</code>: coordinate (feet) taken from the center of the ice.</p>

<p>•<code class="highlighter-rouge">y_coordinate</code>: coordinate (feet) taken from the center of the ice.</p>

<p>•<code class="highlighter-rouge">game_seconds</code>: time (seconds) as measured from the beginning of the game.</p>

<p>•<code class="highlighter-rouge">shot_type</code>: type of shot, one-hot encoded into 8 indicator variables, i.e. Backhand, Deflected, Slap Shot, Snap Shot, Tip-In, Wrap-around, Wrist Shot, and NA.</p>

<p>•<code class="highlighter-rouge">last_event_type</code>: type of the last event, one-hot encoded into 9 indicator variables, i.e. Blocket shot, Faceoff, Giveaway, Goal, Hit, Missed shot, Penalty, Shot, Takeaway.</p>

<h2 id="link-to-the-filtered-data-frame">Link to the filtered data frame</h2>
<p><a href="https://www.comet.com/data-science-workspace/feature-engineering/4c0f01b7384943c982a3df97199ca521?assetId=abcb65b8de4e4d4cb1d5158971681f77&amp;assetPath=dataframes&amp;experiment-tab=assets">filtered data frame</a></p>

<h2 id="advanced-models-">Advanced Models <a name="Advanced-Models"></a></h2>
<h3 id="best-xgboost-model">Best XGBoost Model</h3>

<ul>
  <li><a href="https://www.comet.com/data-science-workspace/model-registry/xgboost-feature-selection-class-weights">XGBoost model</a></li>
</ul>

<h3 id="hyperparameter-tuning-and-model-comparison">Hyperparameter Tuning and Model Comparison</h3>
<p>In each experiment, we try to find the best possible values of hyperparameters utilizing a grid search. We split the train set into stratified 3-fold cross-validation and take mean performance to decide the best performing set of hyperparameters. As explained before, we have a class imbalance in our data, which is better captured by the <code class="highlighter-rouge">F1 Score</code> metric.</p>

<p>To compare models based on their performance, we used the same train-validation split for all models. This guarantees that the performance metrics are obtained on the same dataset, therefore we can compare the model’s performance. Specifically train-validation split was an 80%-20% split obtained with random splitting, and we used the same for all models.</p>

<h3 id="handling-class-imbalance">Handling Class Imbalance</h3>
<p>We tried 2 methods to handle the huge class imbalance present in the data:</p>
<ol>
  <li>Class weighting the objective function:</li>
</ol>

<p>In this method, due to the nature of the algorithm, we employ gradient updates for parameter optimization. While aggregating the gradients over the samples, we apply weights to them based on the number of occurrences in training sets. We scale the gradients for the majority and minority classes to achieve balance.</p>
<ol>
  <li>ADASYN algorithm:</li>
</ol>

<p>In this method, we oversample the minority class so that we have enough samples to learn from. The new samples are generated synthetically.</p>

<p>We compare the results from both this method for all 3 XGBoost configurations and found that we don’t find much difference in performance metrics(F1 Score). The results reported are from utilizing class weighting.</p>
<h3 id="xgboost-classifier-trains-on-distance-and-angle">XGBoost classifier trains on distance and angle</h3>
<p>First, we create XGBoost baseline to compare the logistic baseline, with just ‘distance’ and ‘angle’ as the features. To tackle the class imbalance problem, the Following are the hyper-parameters for grid search:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  {"learning_rate": [0.1, 0.3], "max_depth": [2, 4, 6], "n_estimators": [4, 6, 8, 10], "objective": ["binary:logistic"], "reg_alpha": [0.1, 1.0]}
</code></pre></div></div>
<p>We get the following hyper-parameters for the best estimator:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 6, 'objective': 'binary:logistic', 'reg_alpha': 1.0}
</code></pre></div></div>
<p><a href="https://www.comet.com/data-science-workspace/advanced-models/b2a2c89e56f34414aaadfe6a719c0887?experiment-tab=chart&amp;showOutliers=true&amp;smoothing=0&amp;transformY=smoothing&amp;xAxis=step">Link to the model experiment</a></p>

<p>Here, we compare the performance of XGBoost based on features distance and angle, to that of a simple logistic regression trained using the same features. The ROC AUC=0.66892 obtained for the logistic regression was, while that obtained with XGBoost is ROC AUC= 0.708. We consider that there is no significant difference here between the performance of these two models. This is likely explained by the fact that they were trained on only two features, therefore the XGBoost does not have much advantage over the logistic model here.</p>

<p align="center">
<img src="/assets/figures/xgboost01.svg" />
</p>
<p align="center">
<img src="/assets/figures/xgboost02.svg" />
</p>
<p align="center">
<img src="/assets/figures/xgboost03.svg" />
</p>
<p align="center">
<img src="/assets/figures/xgboost04.svg" />
</p>

<h2 id="xgboost-classifier-trains-on-all-features">XGBoost classifier trains on all features</h2>

<p>Next, we use all the features as defined in the section feature engineering 2. We use the same configuration of training/validation split and grid search with cross-validation as the XGBoost baseline. As explained above, the <code class="highlighter-rouge">F1 Score</code> is used as a metric for selecting the best performing model. Following are the hyperparameters for hyperparameter tuning:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{"learning_rate": [0.3], "max_depth": [4, 6, 8], "n_estimators": [25, 45, 70, 100], "objective": ["binary:logistic"], "reg_alpha": [1.0], "reg_lambda": [1.0]}
</code></pre></div></div>
<p>We get the following configuration for the best estimator:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 45, 'objective': 'binary:logistic', 'reg_alpha': 1.0, 'reg_lambda': 1.0}
</code></pre></div></div>

<p><a href="https://www.comet.com/data-science-workspace/advanced-models/77f0015ccf7f45afba799b51603448fb?experiment-tab=chart&amp;showOutliers=true&amp;smoothing=0&amp;transformY=smoothing&amp;xAxis=step">Link to the model experiment</a></p>

<h2 id="performance-of-the-model">Performance of the model</h2>
<p>We notice a small improvement in the performance of the model when using all features (ROC AUC = 0.757) as compared to using only distance and angle (ROC AUC = 0.708).</p>
<p align="center">
<img src="/assets/figures/gb01.svg" />
</p>
<p align="center">
<img src="/assets/figures/gb02.svg" />
</p>
<p align="center">
<img src="/assets/figures/gb03.svg" />
</p>
<p align="center">
<img src="/assets/figures/gb04.svg" />
</p>

<h2 id="feature-selection">Feature Selection</h2>

<p>We used a range of methods of feature selection that cover all types: filter, wrapper, and embedded methods. Here, we discuss and compare the results of each method. For each method, we keep the 5 highest-scoring features; and at the end, we will compute the intersection and union between all sets of obtained features.</p>

<p>Before starting feature selection, we notice that some features are correlated. Specifically, the angle is correlated with y_coordinate (r=0.84) and change_in_shot_angle, as well as rebound, are correlated with last_event_type_SHOT (because these variables were defined based on one another). This means that there is probably some redundant information in these variables that needs to be parsed using feature selection.</p>

<p align="center">
<img src="/assets/figures/Picture1.png" />
</p>

<h2 id="filter-methods">Filter Methods</h2>
<p>We first used a variance threshold of 80%, thus removing all features that are either one or zero in more than 80% of the sample.</p>
<p align="center">
<img src="/assets/figures/fig01.jpeg" />
</p>
<p>Then, we decided to keep only the 5 features with the highest variance, to stay coherent with the general method for all feature selection methods. The 5 top-ranked features were: <code class="highlighter-rouge">['distance', 'angle', 'distance_from_last_event', 'x_coordinate', 'game_seconds']</code></p>

<p>Then, we also used univariate feature selection while using 6 different criteria. That is, we used all the methods available in scikit-learn that can be used for a classification task and that accept both positive and negative inputs:</p>

<p>•	ANOVA F-value between label/feature for classification tasks.</p>

<p>•	Mutual information for a discrete target.</p>

<p>•	Percentile of the highest scores.</p>

<p>•	False positive rate test.</p>

<p>•	Estimated false discovery rate.</p>

<p>•	Family-wise error rate.</p>
<p align="center">
<img src="/assets/figures/fig02.jpeg" />
</p>
<p>The 5 top-ranked features (on average among methods) were:<code class="highlighter-rouge"> ['distance', 'empty_net', 'game_period', 'y_coordinate', 'shot_type_Wrist Shot']</code></p>

<h2 id="wrapper-methods">Wrapper methods</h2>
<p>We used both forward search and backward search using logistic regression, setting the number of features to select equal to 5. For these methods, we scaled all features because this was needed for the logistic regression to converge.</p>

<p>Forward search resulted in these features: 
<code class="highlighter-rouge">['distance', 'angle', 'empty_net', 'game_period', 'distance_from_last_event']</code></p>

<p>Backward search: the model does not converge.</p>

<h2 id="embedded-methods">Embedded methods</h2>
<p>We evaluated the coefficients from l2-penalized logistic regression, as well as that of Linear Support Vector Classification. Note that we are interested in the magnitude, therefore in the absolute value of the coefficient. For these methods, we scaled all features because this was needed for the logistic regression to converge.</p>
<p align="center">
<img src="/assets/figures/fig03.jpeg" />
</p>
<p>For logistic regression, the 5 top-ranked features were: [<code class="highlighter-rouge">distance</code>, <code class="highlighter-rouge">empty_net</code>, <code class="highlighter-rouge">distance_from_last_event</code>, <code class="highlighter-rouge">speed</code>, <code class="highlighter-rouge">shot_type_Wrap-around</code>]
For linear SVC, the 5 top-ranked features were: [<code class="highlighter-rouge">distance</code>, <code class="highlighter-rouge">empty_net</code>, <code class="highlighter-rouge">distance_from_last_event</code>, <code class="highlighter-rouge">speed</code>, <code class="highlighter-rouge">shot_type_Tip-In</code>]</p>

<h2 id="shap-features">SHAP Features</h2>
<p>We also use SHAP algorithm’s tree explainer which operates on adding a particular feature to a subset of features and assigns importance based on its contribution. We use all features XGBoost model to generate the Shapley values and in turn, get the feature importance. The following plot describes this:</p>
<p align="center">
<img src="/assets/figures/shap_values.png" />
</p>

<h2 id="summary-and-consensus-among-feature-selection-methods">Summary and consensus among feature selection methods</h2>

<p>In summary, we used 2 filters (including 6 different univariate methods) plus wrapper two more embedded and SHAP for a total of six different methods. From these, we compute the intersection and union of sets of 5 top-ranked features from each method. We obtain:</p>

<p>Intersection: [<code class="highlighter-rouge">distance</code>]</p>

<p>Union: [<code class="highlighter-rouge">angle</code>, <code class="highlighter-rouge">distance_from_last_event</code>, <code class="highlighter-rouge">empty_net</code>, <code class="highlighter-rouge">shot_type_Wrap-around</code>, <code class="highlighter-rouge">y_coordinate</code>, <code class="highlighter-rouge">speed</code>, <code class="highlighter-rouge">distance</code>, <code class="highlighter-rouge">x_coordinate</code>, <code class="highlighter-rouge">game_period</code>, <code class="highlighter-rouge">shot_type_Tip-In</code>, <code class="highlighter-rouge">shot_type_Wrist Shot</code>, <code class="highlighter-rouge">game_seconds</code>]</p>

<h2 id="xgboost-train-on-selected-features">XGBoost train on selected features</h2>
<p><a href="https://www.comet.com/data-science-workspace/advanced-models/8824cd33f6aa4574886508b514cc6724?experiment-tab=chart&amp;showOutliers=true&amp;smoothing=0&amp;transformY=smoothing&amp;xAxis=step">Link to the model experiment</a></p>

<p>We utilize the following hyper-parameter configuration for grid search:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{"learning_rate": [0.1, 0.4], "max_depth": [4, 6, 8], "n_estimators": [25, 35, 50, 70, 100], "objective": ["binary:logistic"], "reg_alpha": [1.0], "reg_lambda": [1.0]}
</code></pre></div></div>

<p>Performance of the model
It is important to notice that the feature selection is achieved in improving the model performance. While the model with all features had a ROC AUC = 0.708, the model with fewer but rigorously selected features had a ROC AUC = 0.75. This reflects the fact that sometimes adding more features mostly adds more noise. This is an illustration of why feature selection is important.</p>
<p align="center">
<img src="/assets/figures/xgb01.svg" />
</p>
<p align="center">
<img src="/assets/figures/xgb02.svg" />
</p>
<p align="center">
<img src="/assets/figures/xgb03.svg" />
</p>
<p align="center">
<img src="/assets/figures/xgb04.svg" />
</p>
<h2 id="give-best-shot-">Give Best Shot <a name="Give-it-your-best-shot"></a></h2>
<p>We trained and evaluated a variety of different models. Considering that our dataset was unbalanced (goals represent 1/10 events), models were trained such as to obtain the best performance as measured by ROC-AUC.</p>

<ol>
  <li>K-nearest-neighbours</li>
  <li>Random Forests</li>
  <li>Decision trees</li>
  <li>Neural Networks - which resulted in our Best Model</li>
</ol>

<p>We also used a variety of other approaches including stratified train-test split and k-fold cross-validation.</p>

<p>At this point, we also went back to the feature selection section, and we improved the variety of our feature selection algorithm. We improved the general pipeline of our feature selection (see the relevant section) such as to include a variety of filter, embedded and wrapper methods, to achieve a consensus among these methods. We noted that different feature selection methods resulted in different sets of features, and we reasoned that these sets might have importance for different reasons. We thus used the union of all sets as the final set of features, to include all relevant information. Models were thus trained with the final set of features.</p>
<p align="center">
<img src="/assets/figures/md04.svg" />
</p>
<p align="center">
<img src="/assets/figures/md03.svg" />
</p>
<p align="center">
<img src="/assets/figures/md01.svg" />
</p>
<p align="center">
<img src="/assets/figures/md02.svg" />
</p>

<p># k-nearest-neighbours
We trained a kNN using Scikit-learn. We used Euclidean distance as a distance metric. We searched for the best hyperparameter i.e., the one that maximized the area under the ROC curve (ROC AUC), by iteratively training KNN while using different values of k = 1, 2, 3, …, 50.</p>

<p>kNN resulted in high accuracy and a satisfying ROC AUC (AUC=0.71)). However, when we evaluated the confusion matrix, we noticed that there were very few events predicted to be goals: the model predicted most events to be missed shots, and therefore is not an adequate model for this data and setting where we aim to predict goals. We explain this observation by the imbalance of our dataset, in which Euclidean distance could not establish a space where a majority of points would be goals: in nearly all cases, the majority of points would still be missed shots, and therefore kNN nearly always predicted shots to be missed shots.</p>

<h1 id="random-forests">Random Forests</h1>
<p>We trained a Random Forest model using Scikit-learn. After some trials, we finally used the following hyperparameters: number of trees as 1,000 and Gini impurity as the criterium for the split.</p>

<p>We got a ROC AUC = 0.72. However, like the kNN, when we evaluated the confusion matrix, we noticed that there were very few events predicted to be goals: the model predicted most events to be missed shots, and therefore is not an adequate model for this data and setting where we aim to predict goals.</p>

<h1 id="decision-trees">Decision Trees</h1>
<p>We trained a Decision Tree model using Scikit-learn. We selected hyperparameters using a randomized search.</p>

<p>This model performed the worst among those tested here, as we got a ROC AUC = 0.65.</p>

<h1 id="neural-networks-are-our-best-model">Neural Networks are our Best Model</h1>
<p>Link of the model in Comet ML platform.</p>
<ul>
  <li><a href="https://www.comet.com/data-science-workspace/model-registry/neural-network-model">Neural Network Model</a></li>
</ul>

<p>We trained Neural Networks using PyTorch. After exploring a variety of loss functions and optimizers, we finally selected AdamW as an optimizer, and Cross Entropy as a loss function because it optimized model performance. The model was then trained iteratively using the settings available in PyTorch. Importantly, the main turnaround for increasing accuracy was introducing class weights in my loss function which will penalize more on the class which has a low no of data points hence giving more importance.</p>

<p>The Neural Networks obtained the best ROC-AUC ex-aequo with the Random Forest (i.e. we consider AUC=0.72 to be not significantly different from that of Random Forest). However, we consider the Neural Networks to be our best model because the examination of the confusion matrix reveals a much better classification than for Random Forest.</p>

<h2 id="evaluate-on-test-set-">Evaluate on Test Set <a name="Evaluate-on-test-set"></a></h2>

<h2 id="regular-season">Regular season:</h2>

<p>For the Regular Season, the model’s performed similarly in the final evaluation set as they did in the training set. The best model based on ROC-AUC was again the Neural Networks. All curves as well as ROC-AUC metrics are close to those observed during training for the same models. This confirms that our models were not over-fitted during training, as there don’t seem to be issues such as leakage or improper splitting that could have resulted in over-fitting.</p>
<p align="center">
<img src="/assets/figures/md05.svg" />
</p>
<p align="center">
<img src="/assets/figures/md06.svg" />
</p>
<p align="center">
<img src="/assets/figures/md07.svg" />
</p>
<p align="center">
<img src="/assets/figures/md08.svg" />
</p>

<h2 id="playoffs">Playoffs:</h2>

<p>For the Playoffs, there was a very interesting discrepancy between model performance in the training set and that observed here in the final evaluation set. Specifically, performance is lower in the final evaluation set than in the training test and for all models. This is likely explained by the fact that most observations in the training set were from the Regular Season: Playoffs represent only 6.4% (20280/315725) of all observations. Goals made during Playoffs are likely predicted by different parameters than those made during Regular Season, because of the context and rules. Therefore, the models were trained mostly to predict goals during the Regular Season and thus performed poorly when evaluated only during the Playoff season. Possible solutions to this problem would be to train specific models only for the Playoff season, or to better account for the effect of the season on how the season affects the predictive power of other variables.</p>

<p>Finally, we note that in this case, XGBoost performs better than other models, with a ROC-AUC=0.61. This might be explained by the specificity of this model, i.e. its ensemble methodology and its ability to learn multiple weak models to build a strong robust model which is flexible in many scenarios.</p>
<p align="center">
<img src="/assets/figures/md09.svg" />
</p>
<p align="center">
<img src="/assets/figures/md10.svg" />
</p>
<p align="center">
<img src="/assets/figures/md11.svg" />
</p>
<p align="center">
<img src="/assets/figures/md12.svg" />
</p>

<h2 id="conclusion-">Conclusion <a name="conclusion"></a></h2>

<p>The present project is a good example of how sports data can be obtained from a publicly available API and made into a data format that can be used for advanced, interactive visualizations, as well as a variety of Machine Learning models.</p>

<p>This project involved predicted a relatively rare (10%) event - goals. It illustrates how feature engineering, feature selection, and finally hyperparameter tuning and model selection can achieve a good model generalizability to the evaluation set. The fact that the generalizability was poor to the part of the evaluation set that represented the playoffs illustrates how it is important for the training set to be representative of the evaluation set in all aspects. This also illustrates how some models might have better generalizability in such a context (i.e. XGBoost).</p>

<p>Some limitations of the present project involve the fact that we used a simple train-test split. The method that we used therefore did not permit us to assign a confidence interval to the performance metrics, such as to compare whether the performance of the models was significantly different from one another.</p>]]></content><author><name></name></author><category term="update" /><summary type="html"><![CDATA[An API for The National Hockey League (NHL)]]></summary></entry><entry><title type="html">Milestone1</title><link href="http://localhost:4000/update/2022/10/19/Milestone1.html" rel="alternate" type="text/html" title="Milestone1" /><published>2022-10-19T06:31:29-04:00</published><updated>2022-10-19T06:31:29-04:00</updated><id>http://localhost:4000/update/2022/10/19/Milestone1</id><content type="html" xml:base="http://localhost:4000/update/2022/10/19/Milestone1.html"><![CDATA[<h3 id="an-api-for-the-national-hockey-league-nhl">An API for The National Hockey League (NHL)</h3>
<ul>
  <li><a href="#introduction">Introduction</a>
    <ul>
      <li><a href="#motivation">Motivation</a></li>
    </ul>
  </li>
  <li><a href="#installation">Installation</a>
    <ul>
      <li><a href="#setup-environment">Setup Environment</a></li>
      <li><a href="#install-dependencies">Install Dependencies</a></li>
    </ul>
  </li>
  <li><a href="#usage">Usage</a>
    <ul>
      <li><a href="#download-data">Download Data</a></li>
      <li><a href="#run-interactive-debugging-tool">Run Interactive Debugging Tool</a></li>
      <li><a href="#create-tidy-data-for-visualisation">Create Tidy Data for Visualisation</a></li>
      <li><a href="#run-simple-visualisation">Run Simple Visualisation</a></li>
      <li><a href="#run-advance-visualisation">Run Advance Visualisation</a></li>
    </ul>
  </li>
  <li><a href="#project-structure">Project Structure</a></li>
  <li><a href="#data-apis">Data APIs</a></li>
  <li><a href="#data-insights">Data Insights</a>
    <ul>
      <li><a href="#data-extractions">Data Extractions</a></li>
      <li><a href="#interactive-debugging-tool">Interactive Debugging Tool</a></li>
      <li><a href="#simple-visualisations">Simple Visualisations</a></li>
      <li><a href="#advanced-visualisations">Advanced Visualisations</a></li>
      <li><a href="#discuss-the-interpretation-of-the-figures">Discuss the interpretation of the figures</a></li>
    </ul>
  </li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#authors">Authors</a></li>
</ul>

<!--te-->

<h1 id="introduction">Introduction</h1>

<p>The National Hockey League (NHL) is a professional ice hockey league in North America. It comprises a total of 32 teams, of which 7 in Canada <a href="https://en.wikipedia.org/wiki/National_Hockey_League">EPL</a>, including the Montreal Canadien. Each year, the Stanley Cup playoffs selects the best team, which is awarded the Stanley Cup for the season. For example, The Montreal Canadien won the Stanley Cup 24 for 24 seasons, the last time in 1992-1993.</p>

<p>The NHL makes publically available an API that features statistics including meta-data on each season, season standings, player statistics by season, and play-by-play data. This last format of data is the most thorough and it features important information for all events during each game, such as the players involved, location coordinates on the ice, and the type of event. The NHL API is a valuable source of fine-grained sports data that can be used in a number of tasks such as finding the features that predict goals, or those that predict players salaries.</p>

<h2 id="motivation">Motivation</h2>

<p>The purpose of this project is to provide a Python API for accessing NHL data, specifically, all the play-by-play informations. The reader will learn here how to download the NHL data for a given year, how to first visualize it, and then how to format it into a tidy data frame. This tidy data format will then be used for producing simple, as well as more advanced, interactive visualizations. At term, this data could also be used for a number of purpuses including machine learning, or other tasks at the reader’s will.</p>

<h1 id="installation">Installation</h1>

<h2 id="setup-environment">Setup Environment</h2>

<ul>
  <li>Git clone the <a href="https://github.com/amandalmia14/hockey-primer-1">repository</a></li>
  <li>Make sure Python is installed on the system</li>
  <li>Create a virtual environment / conda environment</li>
</ul>

<h2 id="install-dependencies">Install Dependencies</h2>

<ul>
  <li>Activate the environment and run <code class="highlighter-rouge">pip install -r requirement.txt</code> this will download all the dependencies related to
this project.</li>
</ul>

<h1 id="usage">Usage</h1>

<h2 id="download-data">Download Data</h2>
<ul>
  <li>The data for the NHL games are exposed out in the form of various APIs, the details of the APIs can be found over
<a href="https://gitlab.com/dword4/nhlapi">here</a></li>
  <li>Run the python script which resides at <code class="highlighter-rouge">modules/dataextraction/data_extraction.py</code>, this script will fetch the data 
of the seasons starting from 2016 to 2020.</li>
  <li>This will create a folder in your directory for the season which you want to download and two json files will be 
appeared along with some other files which will be used later part of the project.
    <ul>
      <li><code class="highlighter-rouge">YYYY_regular_season.json</code></li>
      <li><code class="highlighter-rouge">YYYY_playoffs.json</code></li>
    </ul>

    <p><br />
<img src="/assets/figures/data_download.png" width="200" />
<br /></p>
  </li>
</ul>

<h2 id="run-interactive-debugging-tool">Run Interactive Debugging Tool</h2>
<ul>
  <li>Run the <code class="highlighter-rouge">jupyter notebook</code> locally inside the project folder</li>
  <li>Navigate to the <code class="highlighter-rouge">notebook</code> folder</li>
  <li>Run <code class="highlighter-rouge">3_interactive_debugging_tool.ipynb</code> file</li>
</ul>

<h2 id="create-tidy-data-for-visualisation">Create Tidy Data for Visualisation</h2>
<ul>
  <li>Run the python script which resides at <code class="highlighter-rouge">modules/dataretrival/data_retrival.py</code>, this script will creates the tidy data 
and save the data into a pickle file for all the seasons starting from 2016 to 2020.</li>
</ul>

<h2 id="run-simple-visualisation">Run Simple Visualisation</h2>
<ul>
  <li>Run the <code class="highlighter-rouge">jupyter notebook</code> locally inside the project folder (Incase if jupyter notebook isn’t running)</li>
  <li>Navigate to the <code class="highlighter-rouge">notebook</code> folder</li>
  <li>Run <code class="highlighter-rouge">4_simple_visualizations.ipynb</code> file</li>
</ul>

<h2 id="run-advance-visualisation">Run Advance Visualisation</h2>
<ul>
  <li>Run the <code class="highlighter-rouge">jupyter notebook</code> locally inside the project folder (Incase if jupyter notebook isn’t running)</li>
  <li>Navigate to the <code class="highlighter-rouge">notebook</code> folder</li>
  <li>Run <code class="highlighter-rouge">7_interactive_figure.ipynb</code> file</li>
</ul>

<h1 id="project-structure">Project Structure</h1>

<p>As seen in the above image, the project is divided into various parts,</p>

<ul>
  <li><code class="highlighter-rouge">data</code> - It contains all the NHL tournament data season wise, in each season we have two json files of regular season
games and playoffs.</li>
  <li><code class="highlighter-rouge">figures</code> - It contains all the data insights which we captured in this project.</li>
  <li><code class="highlighter-rouge">modules</code> - For every action which we are performing in this project, are captured as modules, like data
extractions, data retrieval (data parsing)</li>
  <li><code class="highlighter-rouge">notebooks</code> - For all kinds of visualisations, insights of the data can be accessed through the notebooks.</li>
  <li><code class="highlighter-rouge">constants.py</code> - As the name suggests, all the common functions and variables reside in this file.</li>
</ul>

<h1 id="data-apis">Data APIs</h1>

<p>This project uses two APIs which were provided by the NHL :</p>

<ul>
  <li><code class="highlighter-rouge">GET_ALL_MATCHES_FOR_A_GIVEN_SEASON = "https://statsapi.web.nhl.com/api/v1/schedule?season=XXXX"</code>
    <ul>
      <li>This API fetch all the matches metadata for a given input season, using this API we are getting the map of
Matches ID and the type of Match it is like <code class="highlighter-rouge">regular season or playoffs</code></li>
    </ul>
  </li>
  <li><code class="highlighter-rouge">GET_ALL_DATA_FOR_A_GIVEN_MATCH = "https://statsapi.web.nhl.com/api/v1/game/XXXXXXXXXX/feed/live/"</code>
    <ul>
      <li>This API fetches all the data in a granular form for a given match id, where we gather the insights subsequently
in
the following tasks.</li>
    </ul>
  </li>
  <li>In order to download a particular data for a season, update the file <code class="highlighter-rouge">modules\dataextraction\data_extraction.py</code> with
the <code class="highlighter-rouge">year</code> variable (one can put multiple seasons to download as well)</li>
  <li>Once the update is done, run <code class="highlighter-rouge">data_extraction.py</code> it will download the data and place it under a folder with the
season
year with two json files, with regular season games and playoffs respectively.</li>
</ul>

<h1 id="data-insights">Data Insights</h1>

<h2 id="data-extractions">Data Extractions</h2>

<p>The data available by the NHL API needs to be parsed and formatted in order to make more advanced data usages possible. In this regard, we select the relevant data out from the nested dictionnaries from the json file, and and we format a single tabular structure, i.e. 
 a dataframe. Below is a glimpse of the tidy dataframe which will be used in further analyses.</p>

<details>
<summary>Tidy Data</summary>
<img src="/assets/figures/df.png" />
</details>

<h3>How to get the number of players in each team</h3>
<p>The first step would be to format a new tidy dataframe which would includes all types of events (not only the shots and goals, such as in the dataframe featured above), with events as rows and including datetime, eventType, periodType, penaltySeverity, penaltyMinutes, and team, as columns. The events would to be sorted in order of their occurrence in time during the game (datetime).</p>

<p>We would then create an empty (np.nan) column for the number of players on ice, and program a loop to iterate over all event, while concatenating a list of player counts for each time, n_1 and n_2. At the beginning of the loop, and at the beginning of each period
(each time the period of the event is not the same as the previous event), we re-initiate the parameters: n_1 = 6 (number of players in first team, including the goalie), n_2 = 6 (number of players in second team, including the goalie).</p>

<p>Eight parameters would be set: penalty_player_A_team_1=None, end_time_of_penalty_for_player_A_team_1 = Datetime penalty_player_B_team_1 = None, and end_time_of_penalty_for_player_B_team_1=Datetime (as there can be a maximum of 2 players in penalty at the same time);and the four equivalent parameters for team 2.</p>

<p>Then, as the loop iterater over all events, each time the eventTypeId == “PENALTY”, if “penaltySeverity”: “Minor” or “DoubleMinor”, the number of player in the team involved in the penalty (Team of the player that is penalized) would be substracted 1, the penalty_player would be set to the name the penalized player, and end_time_of_penalty parameter would be set to DateTime + penaltyMinutes. For subsequent events, as long as the penalty_player is not None, the datetime of the event would be compared to end_time_of_penalty, untill datetime &gt; end_time_of_penalty and then the number of player for that team would be added +1, as the player is back on ice.</p>

<p>Note that for other types of penalty (e.g. misconduct), the number of player on the ice would not be updated as an immediate player replacement is allowed.</p>

<h3>Engineering additionnal features </h3>
<p>We would be interested in studying the impact of tackling and hitting on the chance of goals, both (1) at team-level (2 variables), (2) player-level (4 variables), and (3) total through the game (4 variables). Indeed, tackling and hitting has become an important part of hockey, often discussed by commentators, and highly represented in the data under “eventTypeId”: “HIT”.</p>

<p>(1) We would first extract, for each shot event, variables at team-level that corresponds to the time (in minutes) between the shot and the last time a player of the team on which the shot was taken was hit. This would be done by iterating through all events in chronological order, initiating the time at as NaN at the beginning of each period, and updating the time at each time a hit happens, for each team. This would result in variables: time_since_last_hit_team_1 and time_since_last_hit_team_2.</p>

<p>(2) Additionally, during the same iteration process, we would update four boolean variables with player-level information to note whether the hitter and the hittee from the last hit event were among the player involved in the shot (shooter, goalie or assist). This would result in variables: hitter_involved_team_1, hittee_involved_team_1, hitter_involved_team_2, hittee_involved_team_2.</p>

<p>(3) Finally, to study the relationship between goals and the total number of hits in a game, we would extract 4 variables, during the same iteration process as above. These variables would be initiated at 0 at the beginning of the game, and updated at each hit event for each team and type of player involved (hitter or hittee). This would result in variables: n_hitter_team_1, n_hittee_team_1, n_hitter_team_2, n_hittee_team_2.</p>

<h2 id="interactive-debugging-tool">Interactive Debugging Tool</h2>

<details>
<summary>Event locations for the season 2020</summary>
     <h4>Insights</h4>
     <img src="/assets/figures/idt.png" />
</details>

<h2 id="simple-visualisations">Simple Visualisations</h2>

<details>
<summary>Goals and missed shots, by shot type for season 2016-2017</summary>
     <h4>Insights</h4>
     The most dangerous types of shots for this 2016-2017 season are “deflected” (19.8% of success) followed by 
     “tip-in” shots (17.9% of success). By “most dangerous”, we mean that these shots are the ones that end up the most 
     frequently by a successful goal, as opposed to being missed. However, these are among the less frequent ones: 
     there were only 806 “deflected” and 3,267 “tip-in” shots this season. On the contrary, the most common type of 
     shots was by far the “wrist shot”, with a total of 38,056 shots of that type for this season.
     <br />
     <br />
     We chose to illustrate this question with a barplot while overlaying the count of goals in blue overtop the total 
     count of shots in orange (thus, total of both goals and other, missed shots), by type of shot. Even though there 
     is a large difference between the most common and less common types of shots, we chose to plot the absolute numbers
     and to keep the scale linear, because these are the most intuitive for the reader to understand the scale 
     (the great number of goals involved in a same season) and not to be confused with too many percentages on the same 
     figure. We chose to add annotations on top of bars for the percentage of goals over all shots, because 
     these proportions could not be visually abstracted simply from the figure, and this was an intuitive way to 
     illustrate them.
     <img src="/assets/figures/figure_1_goals_by_shot_type_2016.png" />
</details>

<details>
<summary>Proportion of goal by distance for seasons 2018, 2019, 2020</summary>
     <h4>Insights</h4>
     The proportion of goals over all shots increases overall exponentially as the distance diminishes, with a maximum 
     proportion of goals &gt;25% when goals are shot at less than 5 feet from the goal. We also note a small, local maximum
     at 75 to 80 feet. This distribution did not change significantly for seasons 2018-19 to 2020-21. This local 
     maximum could suggest that there is another variable (e.g. shot type or other) that could underlie this 
     distribution. 
     <br />
     <img src="/assets/figures/figure_2_goal_by_distance2020.png" />
</details>

<details>
<summary>Proportion of goal by distance and shot type</summary>
     <h4>Insights</h4>
     We chose this figure after having considered and visualized different types of figures. First, we visualized 
     violin plots of the distribution of goals and missed shots; however, these did not intuitively represent the 
     chance (proportion) of goals over all shots per se, and the result was dependent on some assumption on the kernel 
     size. We also experimented computing a logistic regression to predict goals from the distance category, which 
     worked fine.
     <br />
     Finally, we chose to come back to the most simple and intuitive method, which is to bin the distance into 
     categories, and plot the proportion of goals for each bin. We chose to divide the distance into equal bins (as 
     opposed to percentiles or other kind of distribution), in order to be able to draw direct conclusion about the 
     relationship of goals to the absolute value of distance by visualizing the figure. Overall, the most dangerous \
     type of shot is the “tip-in” shot taken at a distance of less than 5 feet, followed closely by “back-hand” 
     shots: more than 40% of these shots result in a goal. The relationship found in the previous questions, i.e. that 
     the probability of a goal augments exponentially as the distance decreases, holds true overall for most types of 
     shots. However, the “deflected” and “tip-in” shots have a second maximum between around 30 and 60 feet.
     <br />
     Importantly, the “back-hand” shot has a second maximum at about 80 feet, and the slap-shot has a second maximum at 
     more than 90 feet. This could explain the small local maximum at that distance that we observed in the global 
     distribution of all shots at the previous figure.
     <br />
     Finally, the curves are somewhat irregular, and adding more data (e.g. averaging through a few years) could add 
     more smoothness in the results. Note that to have more smoothed curves and remove outliers which made interpretations difficult, we did not plot the 
     points for which we had less than 10 total observations for that type of shot and at that distance in that season. 
     <br />
     <img src="/assets/figures/figure_3_goals_by_distance_and_shot_type2017.png" />
</details>

<h2 id="advanced-visualisations">Advanced Visualisations</h2>
<p>Comparison of Colorado Avalanche avg shots between season 2016-2017 and 2020-2021<br />
  Here, we compute the average shot rate per hour for each team and for each location on the ice, rounded to a square foot, as compared to the league average shot rate per hour for that same location in years 2016 to 2020, and normalized by the latter. Formally, we compute: 
  ((team average shot rate per hour for location k) - (league average shot rate per hour for location k))/ (team average shot rate per hour for location k).
   We then display the result for each square foot on the ice, as a heatmap overlayed on a figure of the hockey rink, for the offensive zone. The heatmap was smoothed using a gaussian filter with sigma=5. We plot independent figures the years 2016-17, 2018-2019, 2019-20 and 2020-21, with option to choose the team to display.</p>

<p>Season 2016-2017</p>
<iframe src="/assets/2016.html" onload="javascript:(function(o){o.style.height=o.contentWindow.document.body.scrollHeight+&quot;px&quot;;}(this));" style="height:600px;width:200%;border:none;overflow:hidden;"></iframe>
<p>Season 2018-2019</p>
<iframe src="/assets/2018.html" onload="javascript:(function(o){o.style.height=o.contentWindow.document.body.scrollHeight+&quot;px&quot;;}(this));" style="height:600px;width:200%;border:none;overflow:hidden;"></iframe>
<p>Season 2019-2020</p>

<iframe src="/assets/2019.html" onload="javascript:(function(o){o.style.height=o.contentWindow.document.body.scrollHeight+&quot;px&quot;;}(this));" style="height:600px;width:200%;border:none;overflow:hidden;"></iframe>
<p>Season 2020-2021</p>

<iframe src="/assets/2020.html" onload="javascript:(function(o){o.style.height=o.contentWindow.document.body.scrollHeight+&quot;px&quot;;}(this));" style="height:600px;width:200%;border:none;overflow:hidden;"></iframe>

<h2 id="discuss-the-interpretation-of-the-figures">Discuss the interpretation of the figures</h2>

<p>These figures display the excess shot rate per hour by ice location for the team and year, normalized by the league average for all teams and years (2016 to 2020). The excess shot rate is displayed as a proportion of the league average; it is important to look as the scale from the side colorbar before comparing the teams, because we chose to re-adjust the colorbar scale for each team for a better visualization.</p>

<p>The visualization of these figures can help understand in which locations each team was more active in taking shoots as compared to the usual level of shot activity in that zone. This can reflect in part the strategy taken by the team during that season. This strategy could also be correlated with the success of that team during that season.</p>

<p>We consulted the complete league standings on the NHL webside (https://www.nhl.com/standings/2016/league). We note that the Colorado Avalanche was ranked as the last team in the league for the 2016-2017 season, with as few as 48 points total. On the contrary, the Colorado Avalanche was ranked as first of the league for the 2020-2021, with 82 points total and winning the Stanley Cup. This does not directly reflect in the shot maps, since the shot map displays all shots, not only goals. For the 2016-2017 season, the shot map shows that the Colorado Avalanche were doing many shots in excess (up to 120% of the league average for some locations on the ice), specifically in the neutral zone face-off spots (especially left side), and also from the boards behind the goalie. This suggests that shooting from these areas was not successful in achieving goals.  On the contrary, for the 2020-2021 year, the shot map of the Colorado Avalanche shows a shot rate per hour overall closer to the team average, except that an excess of shots were taken from the zone near the referee crease (left side on the plot). This suggests that shooting from that area might have been a good strategy to achieve goals. However, these hypotheses would need to be confirmed in further work that could draw for example shot maps only for the shots that were successful in achieving goals.</p>

<p>Here, we compare the Buffalo Sabres and the Tampa Bay Lightning, while considering that the latter ranked first (for total points) in 2018-2019 and won the Stanley Cup both for 2019-2020 and 2020-2021 seasons. As a comparison, the Buffalo Sabres has been struggling and ranked 27th, 25th and 31th (for total points) for those three years. We thus compare the shot maps of teams for these three years.</p>

<p>The patterns reflected by shot maps for these years shows a clear difference, when focusing on the locations from where the shots were taken. We note that while the Tampa Bay Lightning has taken an excess of shots of the attacking zone (2018-19, and 2019-2020 seasons), and also from the boards just behind the goalie (2019-2020 and 2020-2021 season). On the contrary, the Buffalo Sabres were taking a number of shots close to the league average, and the only areas in which they were taking excesses of shots were further from the goal, for example, they were taking shots from near the referee crease or from close to the ice center.</p>

<p>These comparisons suggest that some teams (here, the Tampa Bay Lightning) are more successful in winning when taking more shots from the attacking zone or from the boards behind the goalie, than other teams (here, the Buffalo Sabres) that took shots from further on the ice. Overall, it suggests that not only the excess of shots by itself, but the location of these shots, has an impact on the performance of the team.</p>

<p>However, this does not give the complete picture. First, all the comparisons done here were done visually, that is, we did not conduct statistical testing nor predictive modelling assessments. Second, we considered only the excess shot rate per hour stratified by all locations on the ice, not whether these shots actually resulted in goals. This does not give us information on the proportion of each shot that achieved goals in each location. For example, we see that the Buffalo Sabres were taking many shots from a furthest distance, and this is associated with the fact that this team did poorly in that season, but we did not directly assess the mechanism involved in the relationship between the shots in question and their unsuccessful results. Finally, it is possible that some locations of shooting do not directly result in goals, but have a broader impact on the game, by either making later goals possible for the team involved or making this team vulnerable to shots by the other team.  Overall, these limitations highlight the necessity to be very careful when making causality or mechanistic assumptions from solely the visualization of data.</p>

<h1 id="conclusion">Conclusion</h1>

<p>The present project is a good example of how sports data can be obtained from a publically available API, and made into a data format that can be used for advanced, interactive visualizations. Some limitations of the present work include that the conclusions drawn here from the data are based solely on data visualizations, and not yet on thorough predictive modelling. Further work could focus on using the formatted data for tasks such as feature selection and machine learning.</p>

<h1 id="authors">Authors</h1>

<p><strong>Vaibhav Jade:</strong> First year student of MSc. Computer Science at UdeM.</p>

<p><strong>Mohsen Dehghani:</strong> Master’s degree in Optimization 2010-2013 and student of DESS in Machin learning at MILA 2022-2023. I start a master’s degree in Machin learning at MILA 2022-2023 love to show how to apply theoretical mathematical knowledge to real-life problems by using computer languages such as Java or Python.</p>

<p><strong>Aman Dalmia:</strong> First year student of MSc. Computer Science at UdeM, have an interest in Information Retrieval and Natural Language Processing. <br />
  <em>“Don’t create a problem statement for an available solution, rather work towards a solution for a given problem”</em></p>

<p><strong>Raphaël Bourque:</strong> Graduated from Medicine, presently doing residency training in Psychiatry and at the Clinician Investigator Program, and studying at the MSc in Computationnal Medicine. My current research work is in Genetics (CHU Sainte-Justine), and I am very interested in the applications of data science and machine learning to evidence-based medical practice.</p>

<p><em>(Names are in ascending order)</em></p>]]></content><author><name></name></author><category term="Update" /><summary type="html"><![CDATA[An API for The National Hockey League (NHL) Introduction Motivation Installation Setup Environment Install Dependencies Usage Download Data Run Interactive Debugging Tool Create Tidy Data for Visualisation Run Simple Visualisation Run Advance Visualisation Project Structure Data APIs Data Insights Data Extractions Interactive Debugging Tool Simple Visualisations Advanced Visualisations Discuss the interpretation of the figures Conclusion Authors]]></summary></entry><entry><title type="html">Youtube</title><link href="http://localhost:4000/update/2022/10/18/youtube.html" rel="alternate" type="text/html" title="Youtube" /><published>2022-10-18T19:20:23-04:00</published><updated>2022-10-18T19:20:23-04:00</updated><id>http://localhost:4000/update/2022/10/18/youtube</id><content type="html" xml:base="http://localhost:4000/update/2022/10/18/youtube.html"><![CDATA[<h3 id="youtube-video-convert-to-mp3">Youtube Video convert to mp3</h3>

<p>Create a function that downloads the audio of the Youtube Video with a given ID and saves it in the folder given by path.
Download it as an mp3. If there is a problem downloading the file, handle the exception. If a file at <code class="highlighter-rouge">path</code> exists, 
the function should return without attempting to download it again.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">unicode_literals</span>
<span class="kn">import</span> <span class="nn">youtube_dl</span>
<span class="kn">import</span> <span class="nn">ffmpeg</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">os.path</span> <span class="kn">import</span> <span class="n">exists</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="c1">#TEST_DIR = os.path.dirname()
</span>
<span class="k">def</span> <span class="nf">download_audio</span><span class="p">(</span><span class="n">YTID</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="s">"""
    Create a function that downloads the audio of the Youtube Video with a given ID
    and saves it in the folder given by path. Download it as an mp3. If there is a 
    problem downloading the file, handle the exception. If a file at `path` exists, 
    the function should return without attempting to download it again.

    ** Use the library youtube_dl: https://github.com/ytdl-org/youtube-dl/ **
    Args:
      YTID: Contains the youtube ID, the corresponding youtube video can be found at
      'https://www.youtube.com/watch?v='+YTID
      path: The path to the file where the audio will be saved
    """</span>
    <span class="n">video_url</span> <span class="o">=</span> <span class="s">'https://www.youtube.com/watch?v='</span><span class="o">+</span><span class="n">YTID</span>
  
    <span class="c1">#from youtube_dl import YoutubeDL
</span>    <span class="c1">#ydl_opts = {
</span>        <span class="c1">#'ignoreerrors': True,
</span>        <span class="c1">#'simulate': True,
</span>    <span class="c1">#}   
</span>    <span class="c1">#with YoutubeDL(ydl_opts) as ydl:
</span>      
        <span class="c1">#info_dict = ydl.extract_info(video_url)
</span>    <span class="c1">#print(info_dict)
</span>    <span class="c1">#xx
</span>    <span class="c1">#video_info = youtube_dl.YoutubeDL(ydl_opts).extract_info(url = video_url,download=False)
</span>
  
    <span class="n">filename</span> <span class="o">=</span> <span class="n">YTID</span><span class="o">+</span><span class="s">".mp3"</span>
    <span class="n">options</span><span class="o">=</span><span class="p">{</span>
        <span class="s">'format'</span><span class="p">:</span><span class="s">'bestaudio/best'</span><span class="p">,</span>
        <span class="s">'keepvideo'</span><span class="p">:</span><span class="bp">False</span><span class="p">,</span>
        <span class="s">'outtmpl'</span><span class="p">:</span> <span class="n">path</span> <span class="p">,</span><span class="c1">#"outtmpl" : path+"$(id)s.$(ext)s" 
</span>        <span class="s">'ignoreerrors'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">video_info</span> <span class="o">=</span> <span class="n">youtube_dl</span><span class="p">.</span><span class="n">YoutubeDL</span><span class="p">(</span><span class="n">options</span><span class="p">).</span><span class="n">extract_info</span><span class="p">(</span><span class="n">url</span> <span class="o">=</span> <span class="n">video_url</span><span class="p">,</span><span class="n">download</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">video_info</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">)</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">youtube_dl</span><span class="p">.</span><span class="n">YoutubeDL</span><span class="p">(</span><span class="n">options</span><span class="p">)</span> <span class="k">as</span> <span class="n">ydl</span><span class="p">:</span>
            <span class="n">ydl</span><span class="p">.</span><span class="n">download</span><span class="p">([</span><span class="n">video_info</span><span class="p">[</span><span class="s">'webpage_url'</span><span class="p">]])</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Download complete... {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">filename</span><span class="p">))</span>  
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Oops!  That was no valid YTID.  Try again..."</span><span class="p">)</span>
    <span class="c1"># to dowload vido use :
</span>    <span class="c1">#ydl_opts = {}   
</span>    <span class="c1">#with youtube_dl.YoutubeDL(ydl_opts) as ydl:
</span>        <span class="c1">#ydl.download(['https://www.youtube.com/watch?v='+YTID])   
</span>  


<span class="k">def</span> <span class="nf">cut_audio</span><span class="p">(</span><span class="n">in_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">out_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">end</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="s">"""
    Create a function that cuts the audio from in_path to only include the segment 
    from start to end and saves it to out_path.

    ** Use the ffmpeg library: https://github.com/kkroening/ffmpeg-python
    Args:
      in_path: Path of the audio file to cut
      out_path: Path of file to save the cut audio
      start: Indicates the start of the sequence (in seconds)
      end: Indicates the end of the sequence (in seconds)
    """</span>
    <span class="c1"># TODO
</span>    <span class="k">try</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">ffmpeg</span><span class="p">.</span><span class="nb">input</span><span class="p">(</span><span class="n">in_path</span><span class="p">)</span>
        <span class="n">audio</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">audio</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="s">"atrim"</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">ffmpeg</span><span class="p">.</span><span class="n">output</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">out_path</span><span class="p">)</span>
        <span class="n">ffmpeg</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">capture_stdout</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">capture_stderr</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">except</span> <span class="nb">ValueError</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Oops!  That was no valid file.  Try again..."</span><span class="p">)</span>


</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">YTID</span> <span class="o">=</span><span class="s">'30PV4W3w_k4&amp;ab_channel=AbangYellowZIN'</span><span class="c1">#RFeU64gTvGQ'
</span><span class="n">l</span><span class="o">=</span><span class="p">[</span><span class="s">"dczdR4laGwc&amp;ab_channel=EnriqueIglesiasVEVO"</span><span class="p">,</span><span class="s">"gfZChizkEuI&amp;ab_channel=RapSamurai"</span><span class="p">]</span>
<span class="c1">#VqzpEw69Tze
</span><span class="n">filename</span> <span class="o">=</span> <span class="n">YTID</span><span class="o">+</span><span class="s">".mp3"</span>
<span class="n">download_audio</span><span class="p">(</span><span class="n">YTID</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
<span class="c1">#cut_audio(TEST_DIR+'/4X3upUSL54I.mp3', 'cut_'+'4X3upUSL54I.mp3', 0.0, 10.0)
</span></code></pre></div></div>]]></content><author><name></name></author><category term="Update" /><summary type="html"><![CDATA[Youtube Video convert to mp3]]></summary></entry></feed>