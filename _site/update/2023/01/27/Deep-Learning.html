<!DOCTYPE html>
<html lang="en"><head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Deep Learning" /></head>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<style>@import url(/public/css/syntax/monokai.css);</style>
  <title>Deep Learning</title>
  <!-- <link href="/public/css/bootstrap.min.css" rel="stylesheet"> -->

  <link href="/public/css/style.css" rel="stylesheet">
  <body>
  	<div class="container"> 
		<div class="sidebar">
			<div class="sidebar-item sidebar-header">
	<div class='sidebar-brand'>
		<a href="/about/">Deep Learning</a>
	</div>
	<p class="lead">A blog exploring deep learning, AI, and data science topics by Mohsen Dehghani.</p></div>

<div class="sidebar-item sidebar-nav">
	<ul class="nav">
      <li class="nav-title">Pages</li>
	  <li>
	  	<a class="nav-item" href="/">Articles</a>
	  </li>
	  
	  
	    
	  
	    
	      
	        <li>
	        	<a class="nav-item" href="/about/">
	            	About
	            </a>
	        </li>
	      
	    
	  
	    
	      
	    
	  
	    
	  
	    
	  
	    
	  
	</ul>
</div>

<div class="sidebar-item sidebar-nav">
  	<ul class="nav">
			<li class="nav-title">Categories</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#Update">
				<span class="name">Update</span>
				<span class="badge">13</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#Jekyll">
				<span class="name">Jekyll</span>
				<span class="badge">1</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#update">
				<span class="name">update</span>
				<span class="badge">6</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#math">
				<span class="name">math</span>
				<span class="badge">1</span>
	    	</a>
 		</li>
	    
	    <li>
	    	<a class="nav-item" href="/category/#data-science">
				<span class="name">data-science</span>
				<span class="badge">1</span>
	    	</a>
 		</li>
	    
	  </nav>
	</ul>
</div>

<div class="sidebar-item sidebar-footer">
	<p>Powered by <a href="https://github.com/jekyll/jekyll">Jekyll</a></p>
</div>
		</div>
		<div class="content">
			<article class="post">
	<header class="post-header">
		<div class="post-title"> 
			Deep Learning
		</div>
		<time class="post-date dt-published" datetime="2023-01-27T18:20:23-05:00" itemprop="datePublished">2023/01/27
		</time>		
	</header>

	<div class="post-content">
		<h1 id="deep-learning-with-pytorch-step-by-step">Deep Learning with PyTorch Step-by-Step</h1>

<h1 id="optimizer-for-nn">Optimizer for NN:</h1>

<p>What if we had a whole lot of parameters using the computed gradients?
We need to use one of PyTorch’s optimizers, like SGD, RMSprop, or Adam.
There are many optimizers: SGD is the most basic of them, and Adam is one of the most popular.
Different optimizers use different mechanics for updating the parameters, but they all achieve 
the same goal through, literally, different paths.The following animation shows a loss surface,the paths traversed by some optimizers to achieve the minimum (represented by a star).</p>

<p>Remember, the choice of mini-batch size influences the path of gradient descent, and so does the choice of an optimizer.</p>

<p align="left">
<img src="/assets/figures/opt2.gif" />
</p>
<p>Most modern neural networks are trained using maximum likelihood. This means that the cost function is simply the negative log-likelihood, equivalently described as the cross-entropy between the training data and the model distribution. This cost function is given by
 \(J(θ) = \log p_{model}(y | x)\)
The specific form of the cost function changes from model to model, depending on the specific form of \(\log p_{model}\) The expansion of the above equation typically yields some terms that do not depend on the model parameters and may be discarded. 
For example, if \(p_{model}(y | x) = N(y; f(x; θ), I)\),then we recover the mean squared error cost,
\(J(θ) = 1\)</p>

<p>In this blog, we will:</p>

<p>• define a simple linear regression model</p>

<p>• walk through every step of gradient descent: initializing parameters,
performing a forward pass, computing errors and loss, computing gradients, and updating parameters</p>

<p>• understand gradients using equations, code, and geometry</p>

<p>• understand the difference between batch, mini-batch, and stochastic gradient descent</p>

<p>• visualize the effects on the loss of using different learning rates</p>

<p>• understand the importance of standardizing / scaling features</p>

<iframe src="/assets/Chapter00.html" onload="javascript:(function(o){o.style.height=o.contentWindow.document.body.scrollHeight+&quot;px&quot;;}(this));" style="height:100px;width:90%;border:none;overflow:hidden;">
 </iframe>

	</div>
</article>
		</div>
	</div>
  </body>
</html>